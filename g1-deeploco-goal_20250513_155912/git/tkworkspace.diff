--- git status ---
rebase in progress; onto 121eb84
You are currently rebasing branch 'main' on '121eb84'.
  (all conflicts fixed: run "git rebase --continue")

Changes not staged for commit:
  (use "git add/rm <file>..." to update what will be committed)
  (use "git restore <file>..." to discard changes in working directory)
	deleted:    .vscode/launch.json
	deleted:    .vscode/settings.json
	deleted:    __init__.py
	deleted:    cuda_12.2.0_535.54.03_linux.run
	deleted:    my_deeploco.egg-info/PKG-INFO
	deleted:    my_deeploco.egg-info/SOURCES.txt
	deleted:    my_deeploco.egg-info/dependency_links.txt
	deleted:    my_deeploco.egg-info/entry_points.txt
	deleted:    my_deeploco.egg-info/requires.txt
	deleted:    my_deeploco.egg-info/top_level.txt
	deleted:    my_deeploco/__init__.py
	deleted:    my_deeploco/__pycache__/g1_env.cpython-310.pyc
	deleted:    my_deeploco/__pycache__/g1_env.cpython-38.pyc
	deleted:    my_deeploco/__pycache__/g1_env_new.cpython-310.pyc
	deleted:    my_deeploco/__pycache__/g1_gym_wrapper.cpython-310.pyc
	deleted:    my_deeploco/__pycache__/g1_nav_env.cpython-310.pyc
	deleted:    my_deeploco/__pycache__/g1_train.cpython-310.pyc
	deleted:    my_deeploco/g1_env.py
	deleted:    my_deeploco/g1_eval.py
	deleted:    my_deeploco/g1_gym_wrapper.py
	deleted:    my_deeploco/g1_nav_env.py
	deleted:    my_deeploco/g1_nav_eval.py
	deleted:    my_deeploco/g1_nav_train.py
	deleted:    my_deeploco/g1_train.py
	deleted:    my_deeploco/rsl_rl/__init__.py
	deleted:    my_deeploco/rsl_rl/__pycache__/__init__.cpython-310.pyc
	deleted:    my_deeploco/rsl_rl/algorithms/__init__.py
	deleted:    my_deeploco/rsl_rl/algorithms/__pycache__/__init__.cpython-310.pyc
	deleted:    my_deeploco/rsl_rl/algorithms/__pycache__/distillation.cpython-310.pyc
	deleted:    my_deeploco/rsl_rl/algorithms/__pycache__/ppo.cpython-310.pyc
	deleted:    my_deeploco/rsl_rl/algorithms/distillation.py
	deleted:    my_deeploco/rsl_rl/algorithms/ppo.py
	deleted:    my_deeploco/rsl_rl/env/__init__.py
	deleted:    my_deeploco/rsl_rl/env/__pycache__/__init__.cpython-310.pyc
	deleted:    my_deeploco/rsl_rl/env/__pycache__/vec_env.cpython-310.pyc
	deleted:    my_deeploco/rsl_rl/env/vec_env.py
	deleted:    my_deeploco/rsl_rl/modules/__init__.py
	deleted:    my_deeploco/rsl_rl/modules/__pycache__/__init__.cpython-310.pyc
	deleted:    my_deeploco/rsl_rl/modules/__pycache__/actor_critic.cpython-310.pyc
	deleted:    my_deeploco/rsl_rl/modules/__pycache__/actor_critic_recurrent.cpython-310.pyc
	deleted:    my_deeploco/rsl_rl/modules/__pycache__/normalizer.cpython-310.pyc
	deleted:    my_deeploco/rsl_rl/modules/__pycache__/rnd.cpython-310.pyc
	deleted:    my_deeploco/rsl_rl/modules/__pycache__/student_teacher.cpython-310.pyc
	deleted:    my_deeploco/rsl_rl/modules/__pycache__/student_teacher_recurrent.cpython-310.pyc
	deleted:    my_deeploco/rsl_rl/modules/actor_critic.py
	deleted:    my_deeploco/rsl_rl/modules/actor_critic_recurrent.py
	deleted:    my_deeploco/rsl_rl/modules/normalizer.py
	deleted:    my_deeploco/rsl_rl/modules/rnd.py
	deleted:    my_deeploco/rsl_rl/modules/student_teacher.py
	deleted:    my_deeploco/rsl_rl/modules/student_teacher_recurrent.py
	deleted:    my_deeploco/rsl_rl/networks/__init__.py
	deleted:    my_deeploco/rsl_rl/networks/__pycache__/__init__.cpython-310.pyc
	deleted:    my_deeploco/rsl_rl/networks/__pycache__/memory.cpython-310.pyc
	deleted:    my_deeploco/rsl_rl/networks/memory.py
	deleted:    my_deeploco/rsl_rl/runners/__init__.py
	deleted:    my_deeploco/rsl_rl/runners/__pycache__/__init__.cpython-310.pyc
	deleted:    my_deeploco/rsl_rl/runners/__pycache__/on_policy_runner.cpython-310.pyc
	deleted:    my_deeploco/rsl_rl/runners/on_policy_runner.py
	deleted:    my_deeploco/rsl_rl/storage/__init__.py
	deleted:    my_deeploco/rsl_rl/storage/__pycache__/__init__.cpython-310.pyc
	deleted:    my_deeploco/rsl_rl/storage/__pycache__/rollout_storage.cpython-310.pyc
	deleted:    my_deeploco/rsl_rl/storage/rollout_storage.py
	deleted:    my_deeploco/rsl_rl/utils/__init__.py
	deleted:    my_deeploco/rsl_rl/utils/__pycache__/__init__.cpython-310.pyc
	deleted:    my_deeploco/rsl_rl/utils/__pycache__/utils.cpython-310.pyc
	deleted:    my_deeploco/rsl_rl/utils/__pycache__/wandb_utils.cpython-310.pyc
	deleted:    my_deeploco/rsl_rl/utils/neptune_utils.py
	deleted:    my_deeploco/rsl_rl/utils/utils.py
	deleted:    my_deeploco/rsl_rl/utils/wandb_utils.py
	deleted:    pyproject.toml
	deleted:    rsl_rl
	deleted:    setup.py
	deleted:    test.py

Untracked files:
  (use "git add <file>..." to include in what will be committed)
	An Introduction to DeepReinforcement Learning.pdf
	Antrag_Wohnzeitverlaengerung.pdf
	ArchitectureDetail_corrected.svg
	Control for Rob 950702125 (S23)_20250313_1653.zip
	Cursor-0.47.9-x86_64.AppImage.zs-old
	Data-Efficient Hierarchical Reinforcement Learning.pdf
	DeepLoco_2017.pdf
	Deep_Reinforcement_Learning_A_Brief_Survey.pdf
	Ex5.pdf
	FeUdal Networks for Hierarchical Reinforcement Learning.pdf
	Genesis-main/
	Grade_Report_21042025_1542.pdf
	Hindsight Experience replay.pdf
	IsaacGym_Preview_4_Package.tar.gz
	IsaacLab-main/
	KunTao_Report.pdf
	Learning Bipedal Walking On Planned FootstepsFor Humanoid Robots.pdf
	Master_Thesis___Iterative_Algorithms_for_Quadrupedal_Locomotion-2.pdf
	My_deeploco/
	Policy Gradient Algorithms _ Lil'Log.pdf
	Resume_KunTao.pdf
	Untitled diagram-2025-03-27-222739.png
	cont_rich_midterm_2024-2.pdf
	cuda_11.8.0_520.61.05_linux.run
	exercise_4_notebook_json.ipynb
	fmr/
	g1-deeploco-goal_20250512_195536/
	geom.py
	ha-et-al-2025-learning-based-legged-locomotion-state-of-the-art-and-future-perspectives.pdf
	humanoid-gym-main/
	legged_gym-master/
	miniconda3/
	myenv/
	opus_rl-main/
	p058.pdf
	ppo.pdf
	rigid_entity.py
	rollout_storage.py
	rustdesk-1.3.8-x86_64.deb
	terrain.py
	tum-thesis-latex-master.zip
	tum-thesis-latex-master/
	unitree_rl_gym-main/

no changes added to commit (use "git add" and/or "git commit -a") 


--- git diff ---
diff --git a/.vscode/launch.json b/.vscode/launch.json
deleted file mode 100644
index f4ba53f..0000000
--- a/.vscode/launch.json
+++ /dev/null
@@ -1,16 +0,0 @@
-{
-    // Use IntelliSense to learn about possible attributes.
-    // Hover to view descriptions of existing attributes.
-    // For more information, visit: https://go.microsoft.com/fwlink/?linkid=830387
-    "version": "0.2.0",
-    "configurations": [
-        {
-            "name": "Python Debugger: g1_train.py",
-            "type": "debugpy",
-            "request": "launch",
-            "program": "${workspaceFolder}/my_deeploco/g1_train.py",
-            "console": "integratedTerminal",
-            "args": ["--path", "${workspaceFolder}/my_deeploco/log/g1-deeploco-walk"]
-        }
-    ]
-}
\ No newline at end of file
diff --git a/.vscode/settings.json b/.vscode/settings.json
deleted file mode 100644
index 013034f..0000000
--- a/.vscode/settings.json
+++ /dev/null
@@ -1,6 +0,0 @@
-{
-    "python.analysis.extraPaths": [
-        "/home/dodolab/.local/lib/python3.9/site-packages"
-    ],
-    "ros.distro": "noetic"
-}
\ No newline at end of file
diff --git a/__init__.py b/__init__.py
deleted file mode 100644
index e69de29..0000000
diff --git a/cuda_12.2.0_535.54.03_linux.run b/cuda_12.2.0_535.54.03_linux.run
deleted file mode 100644
index b36b572..0000000
Binary files a/cuda_12.2.0_535.54.03_linux.run and /dev/null differ
diff --git a/my_deeploco.egg-info/PKG-INFO b/my_deeploco.egg-info/PKG-INFO
deleted file mode 100644
index 4953c8d..0000000
--- a/my_deeploco.egg-info/PKG-INFO
+++ /dev/null
@@ -1,3 +0,0 @@
-Metadata-Version: 2.1
-Name: my_deeploco
-Version: 0.1
diff --git a/my_deeploco.egg-info/SOURCES.txt b/my_deeploco.egg-info/SOURCES.txt
deleted file mode 100644
index 41a0b9b..0000000
--- a/my_deeploco.egg-info/SOURCES.txt
+++ /dev/null
@@ -1,7 +0,0 @@
-pyproject.toml
-setup.py
-my_deeploco/__init__.py
-my_deeploco.egg-info/PKG-INFO
-my_deeploco.egg-info/SOURCES.txt
-my_deeploco.egg-info/dependency_links.txt
-my_deeploco.egg-info/top_level.txt
\ No newline at end of file
diff --git a/my_deeploco.egg-info/dependency_links.txt b/my_deeploco.egg-info/dependency_links.txt
deleted file mode 100644
index 8b13789..0000000
--- a/my_deeploco.egg-info/dependency_links.txt
+++ /dev/null
@@ -1 +0,0 @@
-
diff --git a/my_deeploco.egg-info/entry_points.txt b/my_deeploco.egg-info/entry_points.txt
deleted file mode 100644
index 7dc59a3..0000000
--- a/my_deeploco.egg-info/entry_points.txt
+++ /dev/null
@@ -1,3 +0,0 @@
-[console_scripts]
-g1-eval = my_deeploco.g1_eval:main
-g1-train = my_deeploco.g1_train:main
diff --git a/my_deeploco.egg-info/requires.txt b/my_deeploco.egg-info/requires.txt
deleted file mode 100644
index 8fb104e..0000000
--- a/my_deeploco.egg-info/requires.txt
+++ /dev/null
@@ -1,6 +0,0 @@
-torch>=1.10.0
-numpy>=1.21.0
-gym>=0.21.0
-
-[rsl_rl]
-rsl_rl@ git+https://github.com/leggedrobotics/rsl_rl.git#egg=rsl_rl
diff --git a/my_deeploco.egg-info/top_level.txt b/my_deeploco.egg-info/top_level.txt
deleted file mode 100644
index e77f1e3..0000000
--- a/my_deeploco.egg-info/top_level.txt
+++ /dev/null
@@ -1 +0,0 @@
-my_deeploco
diff --git a/my_deeploco/__init__.py b/my_deeploco/__init__.py
deleted file mode 100644
index e69de29..0000000
diff --git a/my_deeploco/__pycache__/g1_env.cpython-310.pyc b/my_deeploco/__pycache__/g1_env.cpython-310.pyc
deleted file mode 100644
index 857849c..0000000
Binary files a/my_deeploco/__pycache__/g1_env.cpython-310.pyc and /dev/null differ
diff --git a/my_deeploco/__pycache__/g1_env.cpython-38.pyc b/my_deeploco/__pycache__/g1_env.cpython-38.pyc
deleted file mode 100644
index accda75..0000000
Binary files a/my_deeploco/__pycache__/g1_env.cpython-38.pyc and /dev/null differ
diff --git a/my_deeploco/__pycache__/g1_env_new.cpython-310.pyc b/my_deeploco/__pycache__/g1_env_new.cpython-310.pyc
deleted file mode 100644
index 86acdc2..0000000
Binary files a/my_deeploco/__pycache__/g1_env_new.cpython-310.pyc and /dev/null differ
diff --git a/my_deeploco/__pycache__/g1_gym_wrapper.cpython-310.pyc b/my_deeploco/__pycache__/g1_gym_wrapper.cpython-310.pyc
deleted file mode 100644
index 65471d9..0000000
Binary files a/my_deeploco/__pycache__/g1_gym_wrapper.cpython-310.pyc and /dev/null differ
diff --git a/my_deeploco/__pycache__/g1_nav_env.cpython-310.pyc b/my_deeploco/__pycache__/g1_nav_env.cpython-310.pyc
deleted file mode 100644
index 7b3945b..0000000
Binary files a/my_deeploco/__pycache__/g1_nav_env.cpython-310.pyc and /dev/null differ
diff --git a/my_deeploco/__pycache__/g1_train.cpython-310.pyc b/my_deeploco/__pycache__/g1_train.cpython-310.pyc
deleted file mode 100644
index 39dbc65..0000000
Binary files a/my_deeploco/__pycache__/g1_train.cpython-310.pyc and /dev/null differ
diff --git a/my_deeploco/g1_env.py b/my_deeploco/g1_env.py
deleted file mode 100644
index d3d9f3c..0000000
--- a/my_deeploco/g1_env.py
+++ /dev/null
@@ -1,602 +0,0 @@
-import torch
-import math
-import numpy as np
-import genesis as gs
-from genesis.utils.geom import quat_to_xyz, xyz_to_quat, transform_by_quat, inv_quat, transform_quat_by_quat
-
-def gs_rand_float(lower, upper, shape, device):
-    return (upper - lower) * torch.rand(size=shape, device=device) + lower
-
-class G1DeeplocoEnv:
-    def __init__(self, num_envs, env_cfg, obs_cfg, reward_cfg, command_cfg, domain_rand_cfg, show_viewer=True, device="cuda"):
-        self.device = torch.device(device)
-        self.num_envs = num_envs
-        self.num_obs = obs_cfg["num_obs"]
-        self.num_privileged_obs = None
-        # Use env_cfg["num_actions"] if defined, otherwise infer from number of dof_names.
-        self.num_actions = env_cfg.get("num_actions", len(env_cfg["dof_names"]))
-        self.num_commands = command_cfg["num_commands"]
-
-        # Set simulation parameters 
-        self.simulate_action_latency = env_cfg.get("simulation_action_latency", False) #(Sim2Sim mode)
-        self.dt = 0.02  # 50Hz control frequency
-        self.max_episode_length = math.ceil(env_cfg["episode_length_s"] / self.dt)
-
-        self.env_cfg = env_cfg
-        self.obs_cfg = obs_cfg
-        self.reward_cfg = reward_cfg
-        self.command_cfg = command_cfg
-        self.domain_rand_cfg = domain_rand_cfg
-
-        self.obs_scales = obs_cfg["obs_scales"]
-        self.reward_scales = reward_cfg["reward_scales"]
-        
-        # Match the command_scale tensor to the actual commands used (3 instead of 4)
-        self.commands_scale = torch.tensor(
-            [self.obs_scales["lin_vel"], self.obs_scales["lin_vel"], self.obs_scales["ang_vel"]],
-            device=self.device, dtype=gs.tc_float,
-        )
-        
-        # Increase all reward scales for more significant per-timestep rewards
-        reward_multiplier = 2.0  # Reduced from 5.0 to create more balanced rewards
-        
-        # Multiply reward scales by dt and the multiplier
-        for key in self.reward_scales.keys():
-            self.reward_scales[key] *= self.dt * reward_multiplier
-
-        # Create scene 
-        self.scene = gs.Scene(
-            sim_options=gs.options.SimOptions(dt=self.dt, substeps=1),
-            viewer_options=gs.options.ViewerOptions(
-                max_FPS=int(0.5 / self.dt),
-                camera_pos=(2.0, 0.0, 2.5),
-                camera_lookat=(0.0, 0.0, 0.5),
-                camera_fov=40,
-            ),
-            vis_options=gs.options.VisOptions(n_rendered_envs=1),
-            rigid_options=gs.options.RigidOptions(
-                dt=self.dt,
-                constraint_solver=gs.constraint_solver.Newton,
-                enable_collision=True,
-                enable_self_collision=True,
-                enable_joint_limit=True,
-            ),
-            show_viewer=show_viewer,
-        )
-        # Add plane
-        self.plane = self.scene.add_entity(gs.morphs.URDF(file="urdf/plane/plane.urdf", fixed=True))
-        
-        # Add robot – note the URDF file has been changed for g1
-        self.base_init_pos = torch.tensor(env_cfg["base_init_pos"], device=self.device)
-        self.base_init_quat = torch.tensor(env_cfg["base_init_quat"], device=self.device)
-        self.inv_base_init_quat = inv_quat(self.base_init_quat)
-        self.robot = self.scene.add_entity(
-            gs.morphs.URDF(
-                file="/home/dodolab/tkworkspace/My_deeploco/my_deeploco/urdf/g1_12dof.urdf",
-                pos=self.base_init_pos.cpu().numpy(),
-                quat=self.base_init_quat.cpu().numpy(),
-            )
-        )
-        self.scene.build(n_envs=num_envs)
-
-        # Map joint names to motor indices and set default positions
-        self.motor_dofs = [self.robot.get_joint(name).dof_idx_local for name in env_cfg["dof_names"]]
-        self.default_dof_pos = torch.tensor(
-            [env_cfg["default_joint_angles"][name] for name in env_cfg["dof_names"]],
-            device=self.device, dtype=gs.tc_float
-        )
-
-        # Set generic PD gains for all joints
-        self.robot.set_dofs_kp([env_cfg["kp"]] * self.num_actions, self.motor_dofs)  # kp = 60.0 for all
-        self.robot.set_dofs_kv([env_cfg["kd"]] * self.num_actions, self.motor_dofs)  # kd = 1.5 for all
-
-        # Override kd for hips to increase damping - especially for hip roll which affects lateral stability
-        self.hip_indices = [0, 1, 2, 6, 7, 8]  # Hip pitch, roll, yaw for both legs
-        self.hip_roll_indices = [1, 7]  # Left and right hip roll indices
-        
-        # Increased damping for better stability
-        self.robot.set_dofs_kv([8.0] * len(self.hip_indices), self.hip_indices)  # Increased from 5.0
-        
-        # Higher stiffness and damping for hip roll joints to resist lateral movements
-        self.robot.set_dofs_kp([150.0] * len(self.hip_roll_indices), self.hip_roll_indices)  # Increased from 100.0
-        self.robot.set_dofs_kv([20.0] * len(self.hip_roll_indices), self.hip_roll_indices)   # Increased from 10.0
-        
-        # Increase kd for knees and ankles for better stability
-        self.knee_indices = [3, 9]  # Just knees
-        self.ankle_indices = [4, 5, 10, 11]  # Just ankles
-        
-        # Different gains for knees vs ankles - increased for stability
-        self.robot.set_dofs_kp([75.0] * len(self.knee_indices), self.knee_indices)  # Added explicit kp for knees
-        self.robot.set_dofs_kv([6.0] * len(self.knee_indices), self.knee_indices)  # Increased from 4.0
-        self.robot.set_dofs_kp([65.0] * len(self.ankle_indices), self.ankle_indices)  # Added explicit kp for ankles
-        self.robot.set_dofs_kv([10.0] * len(self.ankle_indices), self.ankle_indices)  # Increased from 6.0
-        
-        # Higher stiffness for ankle roll to resist unwanted inversion/eversion
-        self.ankle_roll_indices = [5, 11]  # Left and right ankle roll
-        self.robot.set_dofs_kp([85.0] * len(self.ankle_roll_indices), self.ankle_roll_indices)  # Increased from 75.0
-        self.robot.set_dofs_kv([15.0] * len(self.ankle_roll_indices), self.ankle_roll_indices)  # Increased from 12.0
-        
-        # Register reward functions
-        self.reward_functions = {}
-        self._register_reward_functions()
-        
-        # Initialize episode sums after reward functions and scales are set up
-        self.episode_sums = {key: torch.zeros(num_envs, device=self.device, dtype=gs.tc_float) 
-                            for key in self.reward_scales}
-
-        # Initialize buffers
-        self.base_lin_vel = torch.zeros((num_envs, 3), device=self.device, dtype=gs.tc_float)
-        self.base_ang_vel = torch.zeros((num_envs, 3), device=self.device, dtype=gs.tc_float)
-        self.projected_gravity = torch.zeros((num_envs, 3), device=self.device, dtype=gs.tc_float)
-        self.global_gravity = torch.tensor([0.0, 0.0, -1.0], device=self.device, dtype=gs.tc_float).repeat(num_envs, 1)
-        self.obs_buf = torch.zeros((num_envs, self.num_obs), device=self.device, dtype=gs.tc_float)
-        self.rew_buf = torch.zeros((num_envs,), device=self.device, dtype=gs.tc_float)
-        self.reset_buf = torch.ones((num_envs,), device=self.device, dtype=gs.tc_int)
-        self.episode_length_buf = torch.zeros((num_envs,), device=self.device, dtype=gs.tc_int)
-        self.commands = torch.zeros((num_envs, self.num_commands), device=self.device, dtype=gs.tc_float)
-        self.actions = torch.zeros((num_envs, self.num_actions), device=self.device, dtype=gs.tc_float)
-        self.last_actions = torch.zeros_like(self.actions)
-        self.dof_pos = torch.zeros_like(self.actions)
-        self.dof_vel = torch.zeros_like(self.actions)
-        self.base_pos = torch.zeros((num_envs, 3), device=self.device, dtype=gs.tc_float)
-        self.base_quat = torch.zeros((num_envs, 4), device=self.device, dtype=gs.tc_float)
-        self.extras = dict()
-
-        # Modified physics and foot contact parameters 
-        self.contact_forces = self.robot.get_links_net_contact_force()
-        self.left_foot_link = self.robot.get_link(name='left_ankle_roll_link')
-        self.right_foot_link = self.robot.get_link(name='right_ankle_roll_link')
-        self.left_foot_id_local = self.left_foot_link.idx_local
-        self.right_foot_id_local = self.right_foot_link.idx_local
-        self.feet_indices = [self.left_foot_id_local, self.right_foot_id_local]
-        self.feet_num = len(self.feet_indices)
-        self.links_vel = self.robot.get_links_vel()
-        self.feet_vel = self.links_vel[:, self.feet_indices, :]
-        self.links_pos = self.robot.get_links_pos()
-        self.feet_pos = self.links_pos[:, self.feet_indices, :]
-        self.links_quat = self.robot.get_links_quat()
-        self.feet_quat = self.links_quat[:, self.feet_indices, :]
-        self.feet_quat_euler = quat_to_xyz(self.feet_quat)
-        # Reference feet eulaers: [90, 0, 0], [-90, 0, 0]
-        self.feet_quat_euler_ref = torch.tensor([[90, 0, 0], [-90, 0, 0]], device=self.device, dtype=gs.tc_float)
-        self.feet_quat_euler_ref = self.feet_quat_euler_ref.repeat(self.num_envs, 1, 1)
-        
-        self.pelvis_link = self.robot.get_link(name='pelvis')
-        self.pelvis_mass = self.pelvis_link.get_mass()
-        self.pelvis_id_local = self.pelvis_link.idx_local
-        self.links_pos = self.robot.get_links_pos()  # Critical!
-        self.pelvis_pos = self.links_pos[:, self.pelvis_id_local, :]
-        
-        # Initialize phase variables
-        self.phase = torch.zeros(num_envs, device=self.device, dtype=gs.tc_float)
-        self.phase_left = self.phase.clone()
-        self.phase_right = self.phase.clone()
-        self.leg_phase = torch.zeros((num_envs, 2), device=self.device, dtype=gs.tc_float)
-        self.sin_phase = torch.zeros((num_envs, 1), device=self.device, dtype=gs.tc_float)
-        self.cos_phase = torch.zeros((num_envs, 1), device=self.device, dtype=gs.tc_float)
-        
-        self.original_links_mass = []
-        
-        termination_contact_names = env_cfg["terminate_after_contacts_on"]
-        self.termination_contact_indices = []
-        for name in termination_contact_names:
-            link = self.robot.get_link(name)
-            self.termination_contact_indices.append(link.idx_local)
-
-        # Add consecutive contact tracking
-        self.consecutive_contact_count = torch.zeros((num_envs, 2), device=self.device, dtype=gs.tc_float)
-
-        # Track feet air time for the feet_air_time reward
-        self.feet_air_time = torch.zeros((num_envs, 2), device=self.device, dtype=gs.tc_float)
-        self.last_contacts = torch.zeros((num_envs, 2), device=self.device, dtype=torch.bool)
-        self.last_dof_vel = torch.zeros_like(self.dof_vel)
-        
-    def _resample_commands(self, envs_idx):
-        # Use the new flat range keys from command_cfg
-        self.commands[envs_idx, 0] = gs_rand_float(*self.command_cfg["lin_vel_x_range"], (len(envs_idx),), self.device)
-        self.commands[envs_idx, 1] = gs_rand_float(*self.command_cfg["lin_vel_y_range"], (len(envs_idx),), self.device)
-        self.commands[envs_idx, 2] = gs_rand_float(*self.command_cfg["ang_vel_range"], (len(envs_idx),), self.device)
-
-    def step(self, actions):
-        self.actions = torch.clip(actions, -self.env_cfg["clip_actions"], self.env_cfg["clip_actions"])
-        exec_actions = self.last_actions if self.simulate_action_latency else self.actions
-        target_dof_pos = exec_actions * self.env_cfg["action_scale"] + self.default_dof_pos        
-        self.robot.control_dofs_position(target_dof_pos, self.motor_dofs)
-        self.scene.step()
-
-        # Update contact forces after the simulation step
-        self.contact_forces = self.robot.get_links_net_contact_force()
-        
-        # Update consecutive contact count
-        contact = self.contact_forces[:, self.feet_indices, 2] > 0.5
-        self.consecutive_contact_count = torch.where(
-            contact,
-            self.consecutive_contact_count + 1,
-            torch.zeros_like(self.consecutive_contact_count)
-        )
-
-        # Update buffer
-        self.episode_length_buf += 1
-        self.base_pos[:] = self.robot.get_pos()
-        self.links_vel = self.robot.get_links_vel()
-        self.feet_vel = self.links_vel[:, self.feet_indices, :]
-
-        # if it gets NaN values in self.robot.get_*()
-        if torch.isnan(self.base_pos).any():
-            nan_envs = torch.isnan(self.base_pos).any(dim=1).nonzero(as_tuple=False).flatten()
-            self.reset_idx(nan_envs)
-        self.base_quat[:] = self.robot.get_quat()
-        self.base_euler = quat_to_xyz(
-            transform_quat_by_quat(torch.ones_like(self.base_quat) * self.inv_base_init_quat, self.base_quat)
-        )
-
-        inv_base_quat = inv_quat(self.base_quat)
-        self.base_lin_vel[:] = transform_by_quat(self.robot.get_vel(), inv_base_quat)
-        self.base_ang_vel[:] = transform_by_quat(self.robot.get_ang(), inv_base_quat)
-        self.projected_gravity = transform_by_quat(self.global_gravity, inv_base_quat)
-        self.dof_pos[:] = self.robot.get_dofs_position(self.motor_dofs)
-        self.dof_vel[:] = self.robot.get_dofs_velocity(self.motor_dofs)
-
-        # Change period to match realistic canine gait
-        period = 1.0
-        self.phase = (self.episode_length_buf * self.dt) % period / period
-        self.left_stance = self.phase < 0.5
-        self.right_stance = self.phase >= 0.5
-        self.left_swing = ~self.left_stance
-        self.right_swing = ~self.right_stance
-        self.sin_phase = torch.sin(2 * np.pi * self.phase).unsqueeze(1)
-        self.cos_phase = torch.cos(2 * np.pi * self.phase).unsqueeze(1)
-        
-        # Resample commands
-        envs_idx = ((self.episode_length_buf % int(self.env_cfg["resampling_time_s"] / self.dt)) == 0).nonzero(as_tuple=False).flatten()
-        self._resample_commands(envs_idx)
-        
-        # Check termination and reset
-        self.links_pos = self.robot.get_links_pos()  # Critical!
-        self.feet_pos = self.links_pos[:, self.feet_indices, :]
-        self.pelvis_pos = self.links_pos[:, self.pelvis_id_local, :]
-        self.reset_buf = (self.episode_length_buf > self.max_episode_length) & (self.episode_length_buf > 100)
-        self.reset_buf |= (self.pelvis_pos[:, 2] < self.env_cfg["termination_if_pelvis_z_less_than"]) & (self.episode_length_buf > 100)
-        termination_contacts = torch.any(self.contact_forces[:, self.termination_contact_indices, 2] > 60.0, dim=1)
-        self.reset_buf |= termination_contacts 
-
-        time_out_idx = (self.episode_length_buf > self.max_episode_length).nonzero(as_tuple=False).flatten()
-        self.extras["time_outs"] = torch.zeros_like(self.reset_buf, device=self.device, dtype=gs.tc_float)
-        self.extras["time_outs"][time_out_idx] = 1.0
-        
-        self.reset_idx(self.reset_buf.nonzero(as_tuple=False).flatten())
-        
-        # Domain randomization with progressive curriculum
-        if torch.any((self.episode_length_buf % int(self.domain_rand_cfg.get('push_interval_s', 10.0)/self.dt)) == 0):
-            # Calculate training progress for progressive randomization
-            progress = 1.0  # Always use full randomization now
-            if self.domain_rand_cfg.get('randomize_friction', False):
-                self.randomize_friction(progress)
-            if self.domain_rand_cfg.get('randomize_mass', False):
-                self.randomize_mass(progress)
-            if self.domain_rand_cfg.get('push_robots', False):
-                self.push_robots(progress)
-        
-        # Compute reward with clipping
-        self.rew_buf[:] = 0.0
-        for name, reward_func in self.reward_functions.items():
-            if name in self.reward_scales:  # Only compute rewards with non-zero scales
-                raw_rew = reward_func()
-                
-                # Convert to tensor if it's a float (handles _reward_alive which returns 1.0)
-                if isinstance(raw_rew, float):
-                    raw_rew = torch.ones_like(self.rew_buf) * raw_rew
-                    
-                # Add clipping to prevent extreme values before scaling
-                # Increased clipping threshold to match reward multiplier
-                clipped_rew = torch.clamp(raw_rew, -25.0, 25.0)  
-                
-                rew = clipped_rew * self.reward_scales[name]
-                self.rew_buf += rew
-                self.episode_sums[name] += rew
-        
-        contact = (self.contact_forces[:, self.feet_indices, 2] > 1.0).float()  # Back to 1N
-        
-        # Compute relative foot positions
-        feet_pos_rel = self.feet_pos - self.base_pos.unsqueeze(1)
-
-        # Compute observations - simplify to match official implementation
-        self.obs_buf = torch.cat([
-            self.base_ang_vel * self.obs_scales["ang_vel"],
-            self.projected_gravity,
-            self.commands[:, :3] * self.commands_scale,  # Using first 3 values of commands (x, y, yaw)
-            (self.dof_pos - self.default_dof_pos) * self.obs_scales["dof_pos"],
-            self.dof_vel * self.obs_scales["dof_vel"],
-            self.actions,
-            self.sin_phase,
-            self.cos_phase
-        ], axis=-1)
-        
-        self.obs_buf = torch.clip(self.obs_buf, -self.env_cfg["clip_observations"], self.env_cfg["clip_observations"])
-        self.last_actions[:] = self.actions[:]
-
-        # Update self.extras
-        self.extras["observations"] = {
-            "critic": self.obs_buf,
-            "privileged": None
-        }
-
-        # Update last_dof_vel for acceleration calculation
-        self.last_dof_vel = self.dof_vel.clone()
-
-        # Update feet_quat and feet_quat_euler
-        self.feet_quat = self.links_quat[:, self.feet_indices, :]
-        self.feet_quat_euler = quat_to_xyz(self.feet_quat)
-
-        # Update leg_phase
-        self.leg_phase[:, 0] = self.phase
-        self.leg_phase[:, 1] = (self.phase + 0.5) % 1.0
-
-        return self.obs_buf, self.rew_buf, self.reset_buf, self.extras
-
-    def randomize_friction(self, progress=1.0):
-        # Calculate progress-based friction range
-        base_range = self.domain_rand_cfg['friction_range']
-        
-        # Start with narrow range, expand to full range as training progresses
-        # Use a more conservative range initially
-        center = 0.75  # Shifted toward higher friction (was 0.5 * (base_range[0] + base_range[1]))
-        half_width = 0.3 * (base_range[1] - base_range[0])  # Reduced variation (was 0.5)
-        
-        # Apply progressively wider range based on training progress but with slower ramp-up
-        progress_scaled = progress * 0.8  # Slow down the progression
-        actual_range = [
-            max(0.6, center - half_width * progress_scaled),  # Set minimum friction
-            min(1.1, center + half_width * progress_scaled)   # Set maximum friction
-        ]
-        
-        # Apply randomization with the calculated range
-        self.robot.set_friction_ratio(
-            friction_ratio = actual_range[0] + torch.rand(self.num_envs, self.robot.n_links, device=self.device) * 
-                            (actual_range[1] - actual_range[0]),
-            link_indices = np.arange(0, self.robot.n_links)
-        )
-
-    def randomize_mass(self, progress=1.0):
-        # Only randomize pelvis mass for simplicity, ignore progress
-        added_mass_range = self.domain_rand_cfg['added_mass_range']
-        added_mass = float(torch.rand(1).item() * (added_mass_range[1] - added_mass_range[0]) + added_mass_range[0])
-        new_mass = max(self.pelvis_mass + added_mass, 0.1)
-        self.pelvis_link.set_mass(new_mass)
-
-    def push_robots(self, progress=1.0):
-        env_ids = torch.arange(self.num_envs, device=self.device)
-        push_env_ids = env_ids[self.episode_length_buf[env_ids] % int(self.domain_rand_cfg['push_interval_s']/self.dt) == 0]
-        if len(push_env_ids) == 0:
-            return
-        
-        # Scale push intensity by progress - much more gradually
-        max_vel_xy = self.domain_rand_cfg['max_push_vel_xy'] * progress * 0.6  # Reduced by 40%
-        max_vel_rp = self.domain_rand_cfg['max_push_vel_rp'] * progress * 0.5   # Reduced by 50%
-        
-        # Reduce number of pushes early in training - more drastically
-        push_probability = min(1.0, 0.2 + 0.8 * progress * progress)  # Square term for slower ramp-up
-        
-        # Randomly select which environments to push based on progress
-        push_mask = torch.rand(len(push_env_ids), device=self.device) < push_probability
-        if not torch.any(push_mask):
-            return
-        
-        # Only push selected environments
-        push_env_ids = push_env_ids[push_mask]
-        
-        # Make pushes smaller in magnitude and biased toward forward direction
-        # This helps the robot learn to recover from pushes in the intended direction
-        new_base_lin_vel = torch.zeros_like(self.base_lin_vel)
-        new_base_ang_vel = torch.zeros_like(self.base_ang_vel)
-        
-        # Generate push velocities with forward bias
-        x_vel = gs_rand_float(-max_vel_xy * 0.5, max_vel_xy, (len(push_env_ids),), self.device)  # Forward bias
-        y_vel = gs_rand_float(-max_vel_xy * 0.7, max_vel_xy * 0.7, (len(push_env_ids),), self.device)  # Reduced lateral
-        z_vel = gs_rand_float(0, max_vel_xy * 0.3, (len(push_env_ids),), self.device)  # Small upward only
-        
-        new_base_lin_vel[push_env_ids] = torch.stack([x_vel, y_vel, z_vel], dim=1)
-        
-        # Reduced angular disturbances, especially for roll (x) which affects lateral stability
-        roll_vel = gs_rand_float(-max_vel_rp * 0.4, max_vel_rp * 0.4, (len(push_env_ids),), self.device)  # Reduced roll
-        pitch_vel = gs_rand_float(-max_vel_rp * 0.7, max_vel_rp * 0.7, (len(push_env_ids),), self.device)  # Less reduction for pitch
-        yaw_vel = gs_rand_float(-max_vel_rp * 0.6, max_vel_rp * 0.6, (len(push_env_ids),), self.device)  # Moderate reduction for yaw
-        
-        new_base_ang_vel[push_env_ids] = torch.stack([roll_vel, pitch_vel, yaw_vel], dim=1)
-        
-        d_vel_xy = new_base_lin_vel - self.base_lin_vel[:, :3]
-        d_vel_rp = new_base_ang_vel - self.base_ang_vel[:, :3]
-        d_pos = d_vel_xy * self.dt
-        d_pos[:, [2]] = 0
-        current_pos = self.robot.get_pos()
-        new_pos = current_pos[push_env_ids] + d_pos[push_env_ids]
-        self.robot.set_pos(new_pos, zero_velocity=False, envs_idx=push_env_ids)
-        current_euler = self.base_euler
-        d_euler = d_vel_rp * self.dt
-        new_euler = current_euler[push_env_ids] + d_euler[push_env_ids]
-        new_quat = xyz_to_quat(new_euler)
-        self.robot.set_quat(new_quat, zero_velocity=False, envs_idx=push_env_ids)
-
-    def get_observations(self):
-        extras = {
-            "observations": {
-                "critic": self.obs_buf,
-                "privileged": None
-            }
-        }
-        return self.obs_buf, extras
-    
-    def get_privileged_observations(self):
-        return None
-
-    def reset_idx(self, envs_idx):
-        if len(envs_idx) == 0:
-            return
-            
-        # Reset joint positions and velocities
-        self.dof_pos[envs_idx] = self.default_dof_pos
-        self.dof_vel[envs_idx] = 0.0
-        self.robot.set_dofs_position(
-            position=self.dof_pos[envs_idx],
-            dofs_idx_local=self.motor_dofs,
-            zero_velocity=True,
-            envs_idx=envs_idx,
-        )
-        
-        # Reset base pose and velocities
-        self.base_pos[envs_idx] = self.base_init_pos
-        self.base_quat[envs_idx] = self.base_init_quat.reshape(1, -1)
-        self.base_euler = quat_to_xyz(
-            transform_quat_by_quat(torch.ones_like(self.base_quat) * self.inv_base_init_quat, self.base_quat)
-        )
-        self.robot.set_pos(self.base_pos[envs_idx], zero_velocity=True, envs_idx=envs_idx)
-        self.robot.set_quat(self.base_quat[envs_idx], zero_velocity=True, envs_idx=envs_idx)
-        self.base_lin_vel[envs_idx] = 0
-        self.base_ang_vel[envs_idx] = 0
-        self.robot.zero_all_dofs_velocity(envs_idx)
-        
-        # Reset action buffer
-        self.last_actions[envs_idx] = 0.0
-        
-        # Reset episode stats and track them
-        self.episode_length_buf[envs_idx] = 0
-        self.reset_buf[envs_idx] = True
-        self.extras["episode"] = {}
-        for key in self.episode_sums.keys():
-            self.extras["episode"]["rew_" + key] = (
-                torch.mean(self.episode_sums[key][envs_idx]).item() / self.env_cfg["episode_length_s"]
-            )
-            self.episode_sums[key][envs_idx] = 0.0
-            
-        # Reset phase
-        self.phase[envs_idx] = gs_rand_float(0.0, 0.2, (len(envs_idx),), self.device)  # Start early in phase
-        
-        # Resample commands
-        self._resample_commands(envs_idx)
-
-        # Reset consecutive contact tracking
-        self.consecutive_contact_count[envs_idx] = 0
-
-        # Reset feet air time
-        self.feet_air_time[envs_idx] = 0.0
-        self.last_contacts[envs_idx] = False
-        self.last_dof_vel[envs_idx] = 0.0
-
-        # Start with higher base height and less noise
-        self.base_pos[envs_idx, 2] = self.base_init_pos[2] + 0.03  # 0.12 -> 0.03
-
-    def reset(self):
-        self.reset_buf[:] = True
-        self.reset_idx(torch.arange(self.num_envs, device=self.device))
-        extras = {
-        "observations": {
-            "critic": self.obs_buf,
-            "privileged": None
-            }
-        }
-        return self.obs_buf, extras
-
-    # --------------------- Reward Functions ---------------------
-    def _reward_tracking_lin_vel(self):
-        lin_vel_error = torch.sum(torch.square(self.commands[:, :2] - self.base_lin_vel[:, :2]), dim=1)
-        return torch.exp(-lin_vel_error / self.reward_cfg["tracking_sigma"])
-
-    def _reward_tracking_ang_vel(self):
-        ang_vel_error = torch.square(self.commands[:, 2] - self.base_ang_vel[:, 2])
-        return torch.exp(-ang_vel_error / self.reward_cfg["tracking_sigma"])
-
-    def _reward_lin_vel_z(self):
-        return torch.square(self.base_lin_vel[:, 2])
-
-    def _reward_action_rate(self):
-        return torch.sum(torch.square(self.last_actions - self.actions), dim=1)
-
-    def _reward_base_height(self):
-        return torch.square(self.base_pos[:, 2] - self.reward_cfg["base_height_target"])
-
-    def _reward_alive(self):
-        return 1.0
-
-    def _reward_gait_contact(self):
-        res = torch.zeros(self.num_envs, dtype=torch.float, device=self.device)
-        for i in range(self.feet_num):
-            is_stance = self.leg_phase[:, i] < 0.55
-            contact = self.contact_forces[:, self.feet_indices[i], 2] > 1
-            res += ~(contact ^ is_stance)
-        return res
-
-    def _reward_gait_swing(self):
-        res = torch.zeros(self.num_envs, dtype=torch.float, device=self.device)
-        for i in range(self.feet_num):
-            is_swing = self.leg_phase[:, i] >= 0.55
-            contact = self.contact_forces[:, self.feet_indices[i], 2] > 1
-            res += ~(contact ^ is_swing)
-        return res
-
-    def _reward_contact_no_vel(self):
-        contact = torch.norm(self.contact_forces[:, self.feet_indices, :3], dim=2) > 1.
-        contact_feet_vel = self.feet_vel * contact.unsqueeze(-1)
-        penalize = torch.square(contact_feet_vel[:, :, :3])
-        return torch.sum(penalize, dim=(1, 2))
-
-    def _reward_feet_swing_height(self):
-        contact = torch.norm(self.contact_forces[:, self.feet_indices, :3], dim=2) > 0.5  # Lowered threshold
-        pos_error = torch.square(self.feet_pos[:, :, 2] - self.reward_cfg["feet_height_target"]) * ~contact
-        return torch.sum(pos_error, dim=(1))
-
-    def _reward_orientation(self):
-        return torch.sum(torch.square(self.projected_gravity[:, :2]), dim=1)
-
-    def _reward_ang_vel_xy(self):
-        return torch.sum(torch.square(self.base_ang_vel[:, :2]), dim=1)
-
-    def _reward_dof_vel(self):
-        return torch.sum(torch.square(self.dof_vel), dim=1)
-
-    def _reward_knee_angle(self):
-        return torch.sum(torch.square(self.dof_pos[:, self.knee_indices]), dim=1)
-
-    def _reward_feet_angle(self):
-        return torch.sum(torch.square(self.feet_quat_euler[:,:,2] - self.feet_quat_euler_ref[:,:,2]), dim=1)
-
-    def _register_reward_functions(self):
-        # Register only the reward functions we actually use
-        reward_function_names = [
-            "tracking_lin_vel",
-            "tracking_ang_vel",
-            "lin_vel_z",
-            "action_rate",
-            "base_height",
-            "alive",
-            "gait_contact",
-            "gait_swing",
-            "contact_no_vel",
-            "feet_swing_height",
-            "orientation",
-            "ang_vel_xy",
-            "dof_vel",
-            "knee_angle",
-            "feet_angle",
-        ]
-        
-        # First register all functions that exist
-        for name in reward_function_names:
-            if hasattr(self, f"_reward_{name}"):
-                self.reward_functions[name] = getattr(self, f"_reward_{name}")
-            else:
-                print(f"Warning: No reward function found for {name}")
-        
-        # Then ensure all needed scales exist
-        for key in list(self.reward_functions.keys()):
-            if key not in self.reward_scales:
-                print(f"Adding missing reward scale for {key}")
-                self.reward_scales[key] = 0.0  # Disable by default
-        
-        # And remove any functions for disabled rewards to save computation
-        for key in list(self.reward_functions.keys()):
-            if self.reward_scales.get(key, 0.0) == 0.0:
-                print(f"Disabling unused reward function: {key}")
-                del self.reward_functions[key]  # Actually remove disabled functions
-
-    def close(self):
-        print("Environment closed.")
\ No newline at end of file
diff --git a/my_deeploco/g1_eval.py b/my_deeploco/g1_eval.py
deleted file mode 100644
index fc4184b..0000000
--- a/my_deeploco/g1_eval.py
+++ /dev/null
@@ -1,263 +0,0 @@
-import argparse
-import os
-import pickle
-import torch
-from g1_env import G1DeeplocoEnv
-from rsl_rl.runners import OnPolicyRunner
-import genesis as gs
-import time
-import numpy as np
-from datetime import datetime
-
-def get_eval_train_cfg():
-    """Define a minimal train_cfg for evaluation."""
-    train_cfg_dict = {
-        "algorithm": {
-            "class_name": "PPO",  # Explicitly set to match training
-        },
-        "policy": {
-            "class_name": "ActorCritic",  # Match training policy
-            "activation": "elu",
-            "actor_hidden_dims": [512, 256, 128],
-            "critic_hidden_dims": [512, 256, 128],
-            "init_noise_std": 0.1,  # Lower noise for smoother evaluation
-            "noise_std_type": "log"  # Use log parameterization like in training
-        },
-        "num_steps_per_env": 128,  # Required by OnPolicyRunner
-        "save_interval": 50,       # Required by OnPolicyRunner
-        "empirical_normalization": False,  # Required by OnPolicyRunner
-    }
-    return train_cfg_dict
-
-def add_metrics_tracking(env):
-    """Add metrics tracking to the environment for better visualization."""
-    import types
-    
-    # Setup metrics buffers
-    env.metrics = {
-        "foot_clearance": [],
-        "step_length": [],
-        "base_height": [],
-        "forward_velocity": [],
-        "lateral_velocity": [],
-        "angular_velocity": [],
-        "alternating_contacts": [],
-        "feet_air_time": [],
-        "torso_roll": [],
-        "torso_pitch": []
-    }
-    
-    # Override step function to collect metrics
-    original_step = env.step
-    
-    def step_with_metrics(self, actions):
-        obs, rew, reset, extras = original_step(actions)
-        
-        # Collect metrics
-        if hasattr(self, 'feet_pos'):
-            # Get foot heights when not in contact
-            contact = self.contact_forces[:, self.feet_indices, 2] > 5.0
-            feet_height = self.feet_pos[:, :, 2]
-            non_contact_heights = torch.where(
-                ~contact,
-                feet_height,
-                torch.zeros_like(feet_height)
-            )
-            mean_clearance = torch.mean(non_contact_heights[non_contact_heights > 0]).item() if torch.any(non_contact_heights > 0) else 0
-            
-            # Step length
-            feet_pos_rel = self.feet_pos - self.base_pos.unsqueeze(1)
-            mean_step_length = torch.mean(torch.abs(feet_pos_rel[:, :, 0])).item()
-            
-            # Base height
-            mean_base_height = torch.mean(self.base_pos[:, 2]).item()
-            
-            # Velocities
-            mean_forward_vel = torch.mean(self.base_lin_vel[:, 0]).item()
-            mean_lateral_vel = torch.mean(self.base_lin_vel[:, 1]).item()
-            mean_angular_vel = torch.mean(self.base_ang_vel[:, 2]).item()
-            
-            # Alternating gait - measure if left/right feet are alternating
-            alternating = torch.logical_xor(contact[:, 0], contact[:, 1]).float()
-            mean_alternating = torch.mean(alternating).item()
-            
-            # Feet air time - average time feet spend in air
-            mean_air_time = torch.mean(self.feet_air_time).item()
-            
-            # Torso orientation
-            mean_roll = torch.mean(torch.abs(self.base_euler[:, 0])).item()  # Roll (lateral tilt)
-            mean_pitch = torch.mean(torch.abs(self.base_euler[:, 1])).item()  # Pitch (forward tilt)
-            
-            # Store metrics
-            self.metrics["foot_clearance"].append(mean_clearance)
-            self.metrics["step_length"].append(mean_step_length)
-            self.metrics["base_height"].append(mean_base_height)
-            self.metrics["forward_velocity"].append(mean_forward_vel)
-            self.metrics["lateral_velocity"].append(mean_lateral_vel)
-            self.metrics["angular_velocity"].append(mean_angular_vel)
-            self.metrics["alternating_contacts"].append(mean_alternating)
-            self.metrics["feet_air_time"].append(mean_air_time)
-            self.metrics["torso_roll"].append(mean_roll)
-            self.metrics["torso_pitch"].append(mean_pitch)
-            
-            # Print metrics occasionally
-            if len(self.metrics["foot_clearance"]) % 50 == 0:
-                print(f"\nMetrics (avg last 50 steps):")
-                print(f"  Velocity: forward={sum(self.metrics['forward_velocity'][-50:]) / 50:.3f} m/s, "
-                      f"lateral={sum(self.metrics['lateral_velocity'][-50:]) / 50:.3f} m/s, "
-                      f"angular={sum(self.metrics['angular_velocity'][-50:]) / 50:.3f} rad/s")
-                print(f"  Gait: clearance={sum(self.metrics['foot_clearance'][-50:]) / 50:.3f} m, "
-                      f"step length={sum(self.metrics['step_length'][-50:]) / 50:.3f} m, "
-                      f"air time={sum(self.metrics['feet_air_time'][-50:]) / 50:.3f} s, "
-                      f"alternating={sum(self.metrics['alternating_contacts'][-50:]) / 50:.3f}")
-                print(f"  Stability: height={sum(self.metrics['base_height'][-50:]) / 50:.3f} m, "
-                      f"roll={sum(self.metrics['torso_roll'][-50:]) / 50:.3f} rad, "
-                      f"pitch={sum(self.metrics['torso_pitch'][-50:]) / 50:.3f} rad")
-                
-        return obs, rew, reset, extras
-    
-    env.step = types.MethodType(step_with_metrics, env)
-    return env
-
-def main():
-    parser = argparse.ArgumentParser(description="G1 Deeploco Evaluation Script")
-    parser.add_argument("-e", "--exp_name", type=str, default="g1-deeploco-walk")
-    parser.add_argument("--ckpt", type=int, default=1000, help="Checkpoint iteration to load")
-    parser.add_argument("--cmd_x", type=float, default=0.3, help="Forward velocity command")
-    parser.add_argument("--cmd_y", type=float, default=0.0, help="Lateral velocity command")
-    parser.add_argument("--cmd_yaw", type=float, default=0.0, help="Yaw velocity command")
-    parser.add_argument("--record", action="store_true", help="Record video of evaluation")
-    parser.add_argument("--duration", type=int, default=60, help="Evaluation duration in seconds")
-    args = parser.parse_args()
-
-    gs.init()
-
-    # Load environment configs from cfgs.pkl, but not train_cfg
-    log_dir = f"/home/dodolab/tkworkspace/My_deeploco/my_deeploco/log/{args.exp_name}"
-    if not os.path.exists(f"{log_dir}/cfgs.pkl"):
-        raise FileNotFoundError(f"Configuration file {log_dir}/cfgs.pkl not found. Run training first.")
-    
-    env_cfg, obs_cfg, reward_cfg, command_cfg, train_saved_cfg, domain_rand_cfg = pickle.load(open(f"{log_dir}/cfgs.pkl", "rb"))
-    
-    # Set reward scales to zero for evaluation (instead of clearing them)
-    original_reward_scales = reward_cfg["reward_scales"].copy()
-    reward_cfg["reward_scales"] = {key: 0.0 for key in original_reward_scales.keys()}
-
-    # Set fixed commands for evaluation
-    command_cfg["curriculum"] = False
-    command_cfg["ranges"] = {
-        "lin_vel_x": [args.cmd_x, args.cmd_x],
-        "lin_vel_y": [args.cmd_y, args.cmd_y],
-        "ang_vel_yaw": [args.cmd_yaw, args.cmd_yaw],
-        "heading": [0.0, 0.0]
-    }
-
-    # Disable domain randomization for evaluation
-    domain_rand_cfg["randomize_friction"] = False
-    domain_rand_cfg["randomize_mass"] = False
-    domain_rand_cfg["push_robots"] = False
-
-    # Create environment with 1 env for evaluation
-    env = G1DeeplocoEnv(
-        num_envs=1,
-        env_cfg=env_cfg,
-        obs_cfg=obs_cfg,
-        reward_cfg=reward_cfg,
-        command_cfg=command_cfg,
-        domain_rand_cfg=domain_rand_cfg,
-        show_viewer=True,
-        device="cuda:0"  # Match training device
-    )
-    
-    # Add metrics tracking
-    env = add_metrics_tracking(env)
-
-    # Try to detect whether the model uses log_std or std parameter
-    # First, check the saved training config if it exists
-    if train_saved_cfg and "policy" in train_saved_cfg and "noise_std_type" in train_saved_cfg["policy"]:
-        noise_std_type = train_saved_cfg["policy"]["noise_std_type"]
-    else:
-        # Default to log for newer models
-        noise_std_type = "log"
-    
-    # Use a fresh train_cfg for evaluation
-    train_cfg = get_eval_train_cfg()
-    train_cfg["policy"]["noise_std_type"] = noise_std_type
-    
-    print(f"Using noise_std_type: {noise_std_type}")
-    print(f"Evaluating with commands: forward={args.cmd_x}, lateral={args.cmd_y}, yaw={args.cmd_yaw}")
-
-    # Initialize runner and load checkpoint
-    runner = OnPolicyRunner(env, train_cfg, log_dir, device="cuda:0")
-    resume_path = os.path.join(log_dir, f"model_{args.ckpt}.pt")
-    if not os.path.exists(resume_path):
-        raise FileNotFoundError(f"Checkpoint {resume_path} not found.")
-    
-    # Try loading the model, if it fails due to std/log_std mismatch, try the other type
-    try:
-        runner.load(resume_path)
-    except RuntimeError as e:
-        if "Missing key(s) in state_dict: \"std\"" in str(e):
-            print("Model uses log_std instead of std, updating config...")
-            train_cfg["policy"]["noise_std_type"] = "log"
-            runner = OnPolicyRunner(env, train_cfg, log_dir, device="cuda:0")
-            runner.load(resume_path)
-        elif "Missing key(s) in state_dict: \"log_std\"" in str(e):
-            print("Model uses std instead of log_std, updating config...")
-            train_cfg["policy"]["noise_std_type"] = "scalar"
-            runner = OnPolicyRunner(env, train_cfg, log_dir, device="cuda:0")
-            runner.load(resume_path)
-        else:
-            raise e
-            
-    policy = runner.get_inference_policy(device="cuda:0")
-
-    # Setup video recording if requested
-    if args.record:
-        record_dir = os.path.join(log_dir, "videos")
-        os.makedirs(record_dir, exist_ok=True)
-        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
-        video_path = os.path.join(record_dir, f"eval_x{args.cmd_x}_y{args.cmd_y}_yaw{args.cmd_yaw}_{timestamp}.mp4")
-        print(f"Recording video to {video_path}")
-        env.scene.toggle_video_recording(video_path)
-    
-    # Reset environment and run evaluation loop
-    obs, _ = env.reset()
-    start_time = time.time()
-    step_count = 0
-    
-    with torch.no_grad():
-        while True:
-            actions = policy(obs)
-            obs, rews, dones, infos = env.step(actions)
-            
-            step_count += 1
-            elapsed_time = time.time() - start_time
-            
-            # Print status every 5 seconds
-            if step_count % 250 == 0:
-                print(f"Evaluation progress: {elapsed_time:.1f}s / {args.duration}s")
-            
-            # Reset if done or if we've reached the evaluation duration
-            if dones.any() or elapsed_time >= args.duration:
-                if elapsed_time >= args.duration:
-                    print(f"\nEvaluation completed after {elapsed_time:.1f} seconds ({step_count} steps)")
-                    # Print summary statistics
-                    if hasattr(env, 'metrics') and len(env.metrics['forward_velocity']) > 0:
-                        print("\nFinal metrics summary:")
-                        metrics_len = min(step_count, len(env.metrics['forward_velocity']))
-                        print(f"  Avg forward velocity: {np.mean(env.metrics['forward_velocity'][-metrics_len:]):.3f} m/s")
-                        print(f"  Avg step length: {np.mean(env.metrics['step_length'][-metrics_len:]):.3f} m")
-                        print(f"  Avg foot clearance: {np.mean(env.metrics['foot_clearance'][-metrics_len:]):.3f} m")
-                        print(f"  Avg alternating gait: {np.mean(env.metrics['alternating_contacts'][-metrics_len:]):.3f}")
-                        print(f"  Avg torso roll: {np.mean(env.metrics['torso_roll'][-metrics_len:]):.3f} rad")
-                    
-                    if args.record:
-                        print("Finalizing video recording...")
-                        env.scene.toggle_video_recording(None)
-                    break
-                
-                obs, _ = env.reset()  # Reset if done
-
-if __name__ == "__main__":
-    main() 
\ No newline at end of file
diff --git a/my_deeploco/g1_gym_wrapper.py b/my_deeploco/g1_gym_wrapper.py
deleted file mode 100644
index 56d0eae..0000000
--- a/my_deeploco/g1_gym_wrapper.py
+++ /dev/null
@@ -1,87 +0,0 @@
-import gymnasium as gym
-from gymnasium import spaces
-import numpy as np
-import torch
-
-class G1DeeplocoGymWrapper(gym.Env):
-    def __init__(self, genesis_env):
-        """
-        wrap the genesis-based G1DeeplocoEnv to make it compatible with Gymnasium.
-
-        Args:
-            genesis_env(G1DeeplocoEnv): your existing Genesis-based environment.
-        """
-        super(G1DeeplocoGymWrapper, self).__init__()
-
-        self.genesis_env = genesis_env
-
-        # define observation and action spaces 
-        self. observation_space = spaces.Box(
-            low=-np.inf,
-            high=np.inf,
-            shape=(100,), # observation dimension
-            dtype=np.float32
-        )
-        
-        self.action_space = spaces.Box(
-            low=-1.0,
-            high=1.0,
-            shape=(self.genesis_env.num_actions,), # action dimension
-            dtype=np.float32
-        )
-
-    def reset(self, seed=None, options=None):
-        """
-        reset the environment and return the initial observation.
-
-        Args:
-            seed(int): Seed for random number generation.
-            options(dict): Addotional options for reset.
-
-        Returns:
-            obs(np.ndarray): Initial observation.
-            info(dict): Additional information.
-        """
-
-        # reset the genesis environment
-        obs_buf= self.genesis_env.reset()
-
-        # convert observation to numpy array
-        obs = obs_buf.cpu().numpy()
-
-        # return observation and info
-        return obs, {}
-    
-    def step(self, action):
-        """
-        take a step in the environment.
-
-        Args:
-            action (np.ndarray): Action to take.
-
-        Returns:
-            obs (np.ndarray): Next observation.
-            reward (float): Reward for the step.
-            done (bool): Whether the episode is done.
-            truncated (bool): Whether the episode was truncated.
-            info (dict): Additional information.
-        """
-
-        # convert action totensor if necessary
-        if not isinstance(action, torch.Tensor):
-            action = torch.tensor(action, device=self.genesis_env.device, dtype=torch.float32)
-
-        # perform the step in the genesis environment
-        obs_buf, rew_buf, reset_buf, extras = self.genesis_env.step(action)
-        obs = obs_buf.cpu().numpy()
-        rewards = rew_buf.cpu().numpy()
-        terminated = reset_buf.cpu().numpy()
-        truncated = np.zeros_like(terminated)
-        infos = extras
-
-        # return observation, reward, done, truncated, and info
-        return obs, rewards, terminated, truncated, infos
-    
-    def close(self):
-        """Clean up the environment."""
-        self.genesis_env.close()
\ No newline at end of file
diff --git a/my_deeploco/g1_nav_env.py b/my_deeploco/g1_nav_env.py
deleted file mode 100644
index bdb1f6c..0000000
--- a/my_deeploco/g1_nav_env.py
+++ /dev/null
@@ -1,751 +0,0 @@
-import torch
-import math
-import numpy as np
-import genesis as gs
-from genesis.utils.geom import quat_to_xyz, xyz_to_quat, transform_by_quat, inv_quat, transform_quat_by_quat
-
-def gs_rand_float(lower, upper, shape, device):
-    return (upper - lower) * torch.rand(size=shape, device=device) + lower
-
-class G1DeeplocoEnv:
-    def __init__(self, num_envs, env_cfg, obs_cfg, reward_cfg, command_cfg, domain_rand_cfg, show_viewer=True, device="cuda"):
-        self.device = torch.device(device)
-        self.num_envs = num_envs
-        # Update num_obs to include goal position (2) and footstep targets (4)
-        self.num_obs = obs_cfg["num_obs"]  # Existing obs + 2 (goal) + 4 (footsteps)
-        self.num_privileged_obs = None
-        self.num_actions = env_cfg.get("num_actions", len(env_cfg["dof_names"]))
-        self.num_commands = command_cfg["num_commands"]
-
-        self.simulate_action_latency = env_cfg.get("simulation_action_latency", False)
-        self.dt = 0.02  # 50Hz
-        self.max_episode_length = math.ceil(env_cfg["episode_length_s"] / self.dt)
-
-        self.env_cfg = env_cfg
-        self.obs_cfg = obs_cfg
-        self.reward_cfg = reward_cfg
-        self.command_cfg = command_cfg
-        self.domain_rand_cfg = domain_rand_cfg
-
-        self.obs_scales = obs_cfg["obs_scales"]
-        self.reward_scales = reward_cfg["reward_scales"]
-        self.commands_scale = torch.tensor(
-            [self.obs_scales["lin_vel"], self.obs_scales["lin_vel"], self.obs_scales["ang_vel"]],
-            device=self.device, dtype=gs.tc_float,
-        )
-        reward_multiplier = 2.0
-        for key in self.reward_scales.keys():
-            self.reward_scales[key] *= self.dt * reward_multiplier
-
-        # Scene setup (unchanged)
-        self.scene = gs.Scene(
-            sim_options=gs.options.SimOptions(dt=self.dt, substeps=1),
-            viewer_options=gs.options.ViewerOptions(
-                max_FPS=int(0.5 / self.dt),
-                camera_pos=(2.0, 0.0, 2.5),
-                camera_lookat=(0.0, 0.0, 0.5),
-                camera_fov=40,
-            ),
-            vis_options=gs.options.VisOptions(n_rendered_envs=1),
-            rigid_options=gs.options.RigidOptions(
-                dt=self.dt,
-                constraint_solver=gs.constraint_solver.Newton,
-                enable_collision=True,
-                enable_self_collision=True,
-                enable_joint_limit=True,
-            ),
-            show_viewer=show_viewer,
-        )
-        self.plane = self.scene.add_entity(gs.morphs.URDF(file="urdf/plane/plane.urdf", fixed=True))
-        self.base_init_pos = torch.tensor(env_cfg["base_init_pos"], device=self.device)
-        self.base_init_quat = torch.tensor(env_cfg["base_init_quat"], device=self.device)
-        self.inv_base_init_quat = inv_quat(self.base_init_quat)
-        self.robot = self.scene.add_entity(
-            gs.morphs.URDF(
-                file="/home/dodolab/tkworkspace/My_deeploco/my_deeploco/urdf/g1_12dof.urdf",
-                pos=self.base_init_pos.cpu().numpy(),
-                quat=self.base_init_quat.cpu().numpy(),
-            )
-        )
-        self.scene.build(n_envs=num_envs)
-
-        # Joint setup 
-        self.motor_dofs = [self.robot.get_joint(name).dof_idx_local for name in env_cfg["dof_names"]]
-        self.default_dof_pos = torch.tensor(
-            [env_cfg["default_joint_angles"][name] for name in env_cfg["dof_names"]],
-            device=self.device, dtype=gs.tc_float
-        )
-
-        self.robot.set_dofs_kp([env_cfg["kp"]] * self.num_actions, self.motor_dofs)
-        self.robot.set_dofs_kv([env_cfg["kd"]] * self.num_actions, self.motor_dofs)
-        
-        self.hip_indices = [0, 1, 2, 6, 7, 8]
-        self.hip_roll_indices = [1, 7]
-        
-        # hip
-        self.robot.set_dofs_kv([8.0] * len(self.hip_indices), self.hip_indices)
-        self.robot.set_dofs_kp([150.0] * len(self.hip_roll_indices), self.hip_roll_indices)
-        self.robot.set_dofs_kv([20.0] * len(self.hip_roll_indices), self.hip_roll_indices)
-        
-        self.knee_indices = [3, 9]
-        self.ankle_indices = [4, 5, 10, 11]
-        
-        # knee
-        self.robot.set_dofs_kp([75.0] * len(self.knee_indices), self.knee_indices)
-        self.robot.set_dofs_kv([6.0] * len(self.knee_indices), self.knee_indices)
-        
-        # ankle
-        self.robot.set_dofs_kp([65.0] * len(self.ankle_indices), self.ankle_indices)
-        self.robot.set_dofs_kv([10.0] * len(self.ankle_indices), self.ankle_indices)
-        
-        self.ankle_roll_indices = [5, 11]
-        
-        # ankle roll
-        self.robot.set_dofs_kp([85.0] * len(self.ankle_roll_indices), self.ankle_roll_indices)
-        self.robot.set_dofs_kv([15.0] * len(self.ankle_roll_indices), self.ankle_roll_indices)
-
-        # Reward functions
-        self.reward_functions = {}
-        self._register_reward_functions()
-
-        # Episode sums
-        self.episode_sums = {key: torch.zeros(num_envs, device=self.device, dtype=gs.tc_float) 
-                            for key in self.reward_scales}
-
-        # Initial Buffers 
-        self.base_lin_vel = torch.zeros((num_envs, 3), device=self.device, dtype=gs.tc_float)
-        self.base_ang_vel = torch.zeros((num_envs, 3), device=self.device, dtype=gs.tc_float)
-        self.projected_gravity = torch.zeros((num_envs, 3), device=self.device, dtype=gs.tc_float)
-        self.global_gravity = torch.tensor([0.0, 0.0, -1.0], device=self.device, dtype=gs.tc_float).repeat(num_envs, 1)
-        self.obs_buf = torch.zeros((num_envs, self.num_obs), device=self.device, dtype=gs.tc_float)
-        self.rew_buf = torch.zeros((num_envs,), device=self.device, dtype=gs.tc_float)
-        self.reset_buf = torch.ones((num_envs,), device=self.device, dtype=gs.tc_int)
-        self.episode_length_buf = torch.zeros((num_envs,), device=self.device, dtype=gs.tc_int)
-        self.commands = torch.zeros((num_envs, self.num_commands), device=self.device, dtype=gs.tc_float)
-        self.actions = torch.zeros((num_envs, self.num_actions), device=self.device, dtype=gs.tc_float)
-        self.last_actions = torch.zeros_like(self.actions)
-        self.dof_pos = torch.zeros_like(self.actions)
-        self.dof_vel = torch.zeros_like(self.actions)
-        self.base_pos = torch.zeros((num_envs, 3), device=self.device, dtype=gs.tc_float)
-        self.base_quat = torch.zeros((num_envs, 4), device=self.device, dtype=gs.tc_float)
-        self.goal_pos = torch.zeros((num_envs, 2), device=self.device, dtype=gs.tc_float)  # x, y
-        self.footstep_targets = torch.zeros((num_envs, 2, 3), device=self.device, dtype=gs.tc_float)  # [left, right][x, y]
-        self.extras = dict()
-
-        # Contact and foot data 
-        self.contact_forces = self.robot.get_links_net_contact_force()
-        self.left_foot_link = self.robot.get_link(name='left_ankle_roll_link')
-        self.right_foot_link = self.robot.get_link(name='right_ankle_roll_link')
-        self.left_foot_id_local = self.left_foot_link.idx_local
-        self.right_foot_id_local = self.right_foot_link.idx_local
-        self.feet_indices = [self.left_foot_id_local, self.right_foot_id_local]
-        self.feet_num = len(self.feet_indices)
-        self.links_vel = self.robot.get_links_vel()
-        self.feet_vel = self.links_vel[:, self.feet_indices, :]
-        self.links_pos = self.robot.get_links_pos()
-        self.feet_pos = self.links_pos[:, self.feet_indices, :]
-        self.links_quat = self.robot.get_links_quat()
-        self.feet_quat = self.links_quat[:, self.feet_indices, :]
-        self.feet_quat_euler = quat_to_xyz(self.feet_quat)
-        # reference for feet quat : [90, 0, 0], [-90, 0, 0]
-        self.feet_quat_euler_ref = torch.tensor([[90, 0, 0], [-90, 0, 0]], device=self.device, dtype=gs.tc_float)
-        self.feet_quat_euler_ref = self.feet_quat_euler_ref.repeat(self.num_envs, 1, 1)
-        
-        self.pelvis_link = self.robot.get_link(name='pelvis')
-        self.pelvis_mass = self.pelvis_link.get_mass()
-        self.pelvis_id_local = self.pelvis_link.idx_local
-        self.pelvis_pos = self.links_pos[:, self.pelvis_id_local, :]
-
-        # Initial Phase variables 
-        self.phase = torch.zeros(num_envs, device=self.device, dtype=gs.tc_float)
-        self.phase_left = self.phase.clone()
-        self.phase_right = self.phase.clone()
-        self.leg_phase = torch.zeros((num_envs, 2), device=self.device, dtype=gs.tc_float)
-        self.sin_phase = torch.zeros((num_envs, 1), device=self.device, dtype=gs.tc_float)
-        self.cos_phase = torch.zeros((num_envs, 1), device=self.device, dtype=gs.tc_float)
-        
-        self.original_links_mass = []
-
-        # Termination contacts 
-        termination_contact_names = env_cfg["terminate_after_contacts_on"]
-        self.termination_contact_indices = []
-        for name in termination_contact_names:
-            link = self.robot.get_link(name)
-            self.termination_contact_indices.append(link.idx_local)
-        
-        # Add consecutive contact tracking 
-        self.consecutive_contact_count = torch.zeros((num_envs, 2), device=self.device, dtype=gs.tc_float)
-        
-        # Add feet air time tracking 
-        self.feet_air_time = torch.zeros((num_envs, 2), device=self.device, dtype=gs.tc_float)
-        self.last_contacts = torch.zeros((num_envs, 2), device=self.device, dtype=torch.bool)
-        self.last_dof_vel = torch.zeros_like(self.dof_vel)
-
-        # Initialize goal and footstep targets
-        self.initial_dir_to_goal = torch.zeros((num_envs, 2), device=self.device, dtype=gs.tc_float)
-        self.initial_dir_to_goal[:, 0] = 1.0  # Default: all forward
-        self._resample_goals(torch.arange(num_envs, device=self.device))
-        self.base_euler = torch.zeros((num_envs, 3), device=self.device, dtype=gs.tc_float)
-        # Initialize step sequence and footstep targets
-        self.step_sequence = [self.plan_step_sequence(idx) for idx in range(self.num_envs)]
-        self.current_step_idx = [0 for _ in range(self.num_envs)]
-        for idx in range(self.num_envs):
-            # Set initial footstep_targets to the first two steps in the sequence
-            seq = self.step_sequence[idx]
-            if len(seq) >= 2:
-                self.footstep_targets[idx, 0, :] = torch.tensor(seq[0][:3], device=self.device)
-                self.footstep_targets[idx, 1, :] = torch.tensor(seq[1][:3], device=self.device)
-            elif len(seq) == 1:
-                self.footstep_targets[idx, 0, :] = torch.tensor(seq[0][:3], device=self.device)
-                self.footstep_targets[idx, 1, :] = torch.tensor(seq[0][:3], device=self.device)
-            else:
-                self.footstep_targets[idx, :, :] = 0.0
-
-        # Add footstep planner parameters with defaults
-        self.env_cfg["step_size"] = env_cfg.get("step_size", 0.10)
-        self.env_cfg["step_gap"] = env_cfg.get("step_gap", 0.15)
-        self.env_cfg["feet_height_target"] = env_cfg.get("feet_height_target", 0.075)
-        self.env_cfg["period"] = env_cfg.get("period", 1.1)
-        self.env_cfg["swing_duration"] = env_cfg.get("swing_duration", 0.45)
-        self.env_cfg["stance_duration"] = env_cfg.get("stance_duration", 0.65)
-
-        self.cfg = {
-            "env_cfg": env_cfg,
-            "obs_cfg": obs_cfg,
-            "reward_cfg": reward_cfg,
-            "command_cfg": command_cfg,
-            "domain_rand_cfg": domain_rand_cfg,
-        }
-
-    def _resample_goals(self, envs_idx):
-        self.goal_pos[envs_idx, 0] = self.base_pos[envs_idx, 0] + 50.0  # Far ahead
-        self.goal_pos[envs_idx, 1] = self.base_pos[envs_idx, 1]  # No lateral offset
-
-    def _resample_commands(self, envs_idx):
-        self.commands[envs_idx, 0] = 0.3  # Forward speed
-        self.commands[envs_idx, 1] = 0.0  # No lateral velocity
-        self.commands[envs_idx, 2] = 0.0  # No angular velocity
-
-    def plan_step_sequence(self, idx, num_steps=10):
-        base_pos = self.base_pos[idx, :2].cpu().numpy()
-        dir_to_goal = self.initial_dir_to_goal[idx].cpu().numpy()
-        perp = np.array([-dir_to_goal[1], dir_to_goal[0]])  # Perpendicular vector
-
-        step_size = float(self.env_cfg["step_size"])
-        step_gap = float(self.env_cfg["step_gap"])
-        feet_height_target = float(self.env_cfg["feet_height_target"])
-
-        sequence = []
-        for n in range(num_steps):
-            center = base_pos + (n + 1) * step_size * dir_to_goal
-            if n % 2 == 0:
-                # Left foot
-                target_xy = center + (step_gap / 2) * perp
-            else:
-                # Right foot
-                target_xy = center - (step_gap / 2) * perp
-            sequence.append([target_xy[0], target_xy[1], feet_height_target, 0.0])
-        return sequence
-
-    def step(self, actions):
-        self.actions = torch.clip(actions, -self.env_cfg["clip_actions"], self.env_cfg["clip_actions"])
-        exec_actions = self.last_actions if self.simulate_action_latency else self.actions
-        target_dof_pos = exec_actions * self.env_cfg["action_scale"] + self.default_dof_pos        
-        self.robot.control_dofs_position(target_dof_pos, self.motor_dofs)
-        self.scene.step()
-
-        self.contact_forces = self.robot.get_links_net_contact_force()
-        contact = self.contact_forces[:, self.feet_indices, 2] > 0.5
-        self.consecutive_contact_count = torch.where(
-            contact,
-            self.consecutive_contact_count + 1,
-            torch.zeros_like(self.consecutive_contact_count)
-        )
-
-        self.episode_length_buf += 1
-        self.base_pos[:] = self.robot.get_pos()
-        self.links_vel = self.robot.get_links_vel()
-        self.feet_vel = self.links_vel[:, self.feet_indices, :]
-        if torch.isnan(self.base_pos).any():
-            nan_envs = torch.isnan(self.base_pos).any(dim=1).nonzero(as_tuple=False).flatten()
-            self.reset_idx(nan_envs)
-        self.base_quat[:] = self.robot.get_quat()
-        self.base_euler = quat_to_xyz(
-            transform_quat_by_quat(torch.ones_like(self.base_quat) * self.inv_base_init_quat, self.base_quat)
-        )
-
-        inv_base_quat = inv_quat(self.base_quat)
-        self.base_lin_vel[:] = transform_by_quat(self.robot.get_vel(), inv_base_quat)
-        self.base_ang_vel[:] = transform_by_quat(self.robot.get_ang(), inv_base_quat)
-        self.projected_gravity = transform_by_quat(self.global_gravity, inv_base_quat)
-        self.dof_pos[:] = self.robot.get_dofs_position(self.motor_dofs)
-        self.dof_vel[:] = self.robot.get_dofs_velocity(self.motor_dofs)
-
-        period = 1.0
-        self.phase = (self.episode_length_buf * self.dt) % period / period
-        self.left_stance = self.phase < 0.5
-        self.right_stance = self.phase >= 0.5
-        self.left_swing = ~self.left_stance
-        self.right_swing = ~self.right_stance
-        self.sin_phase = torch.sin(2 * np.pi * self.phase).unsqueeze(1)
-        self.cos_phase = torch.cos(2 * np.pi * self.phase).unsqueeze(1)
-
-        # At the end of each gait cycle, update footstep_targets from the step sequence
-        update_footsteps = (self.episode_length_buf % int(period / self.dt) == 0).nonzero(as_tuple=False).flatten()
-        for idx in update_footsteps:
-            curr_idx = self.current_step_idx[idx]
-            seq = self.step_sequence[idx]
-            # Advance to next two steps if possible
-            if curr_idx + 2 < len(seq):
-                self.footstep_targets[idx, 0, :] = torch.tensor(seq[curr_idx][:3], device=self.device)
-                self.footstep_targets[idx, 1, :] = torch.tensor(seq[curr_idx+1][:3], device=self.device)
-                self.current_step_idx[idx] += 2
-            elif curr_idx + 1 < len(seq):
-                self.footstep_targets[idx, 0, :] = torch.tensor(seq[curr_idx][:3], device=self.device)
-                self.footstep_targets[idx, 1, :] = torch.tensor(seq[curr_idx][:3], device=self.device)
-                self.current_step_idx[idx] += 1
-            else:
-                self.footstep_targets[idx, :, :] = 0.0
-
-        # Resample commands (optional, less frequent)
-        envs_idx = ((self.episode_length_buf % int(self.env_cfg["resampling_time_s"] / self.dt)) == 0).nonzero(as_tuple=False).flatten()
-        self._resample_commands(envs_idx)
-
-        # Compute distance to goal for termination and reward
-        goal_dist = torch.norm(self.goal_pos - self.base_pos[:, :2], dim=1)
-        goal_reached = goal_dist < self.env_cfg.get("goal_reached_threshold", 0.3)
-
-        # Termination
-        self.links_pos = self.robot.get_links_pos()
-        self.feet_pos = self.links_pos[:, self.feet_indices, :]
-        self.pelvis_pos = self.links_pos[:, self.pelvis_id_local, :]
-        self.reset_buf = (self.episode_length_buf > self.max_episode_length) & (self.episode_length_buf > 100)
-        self.reset_buf |= (self.pelvis_pos[:, 2] < self.env_cfg["termination_if_pelvis_z_less_than"]) & (self.episode_length_buf > 100)
-        self.reset_buf |= goal_reached  # Reset on goal reached
-        termination_contacts = torch.any(self.contact_forces[:, self.termination_contact_indices, 2] > 60.0, dim=1)
-        self.reset_buf |= termination_contacts 
-
-        time_out_idx = (self.episode_length_buf > self.max_episode_length).nonzero(as_tuple=False).flatten()
-        self.extras["time_outs"] = torch.zeros_like(self.reset_buf, device=self.device, dtype=gs.tc_float)
-        self.extras["time_outs"][time_out_idx] = 1.0
-        self.extras["goal_reached"] = goal_reached.float()
-
-        self.reset_idx(self.reset_buf.nonzero(as_tuple=False).flatten())
-
-        # Domain randomization (unchanged)
-        if torch.any((self.episode_length_buf % int(self.domain_rand_cfg.get('push_interval_s', 10.0)/self.dt)) == 0):
-            progress = 1.0
-            if self.domain_rand_cfg.get('randomize_friction', False):
-                self.randomize_friction(progress)
-            if self.domain_rand_cfg.get('randomize_mass', False):
-                self.randomize_mass(progress)
-            if self.domain_rand_cfg.get('push_robots', False):
-                self.push_robots(progress)
-
-        # Compute rewards
-        self.rew_buf[:] = 0.0
-        for name, reward_func in self.reward_functions.items():
-            if name in self.reward_scales:
-                raw_rew = reward_func()
-                if isinstance(raw_rew, float):
-                    raw_rew = torch.ones_like(self.rew_buf) * raw_rew
-                clipped_rew = torch.clamp(raw_rew, -25.0, 25.0)
-                rew = clipped_rew * self.reward_scales[name]
-                self.rew_buf += rew
-                self.episode_sums[name] += rew
-
-        contact = (self.contact_forces[:, self.feet_indices, 2] > 1.0).float()
-        feet_pos_rel = self.feet_pos - self.base_pos.unsqueeze(1) # if need 
-
-        # Compute observations
-        rel_goal = self.goal_pos - self.base_pos[:, :2]
-        rel_goal_3d = torch.cat([rel_goal, torch.zeros_like(rel_goal[:, :1])], dim=1)
-        rel_goal_base = transform_by_quat(rel_goal_3d, inv_quat(self.base_quat))[:, :2]
-        # Transform footstep targets to root frame for observation
-        footstep_targets_root = self.footstep_targets
-        # Observation: [base_ang_vel, projected_gravity, commands, dof_pos, dof_vel, actions, sin_phase, cos_phase, rel_goal_base, T1, T2, clock]
-        clock = torch.cat([self.sin_phase, self.cos_phase], dim=1)
-        self.obs_buf = torch.cat([
-            self.base_ang_vel * self.obs_scales["ang_vel"],
-            self.projected_gravity,
-            self.commands[:, :3] * self.commands_scale,
-            (self.dof_pos - self.default_dof_pos) * self.obs_scales["dof_pos"],
-            self.dof_vel * self.obs_scales["dof_vel"],
-            self.actions,
-            rel_goal_base * self.obs_scales.get("goal_pos", 0.1),
-            footstep_targets_root.view(self.num_envs, -1) * self.obs_scales.get("footstep_targets", 1.0),
-            clock
-        ], axis=-1)
-        self.obs_buf = torch.clip(self.obs_buf, -self.env_cfg["clip_observations"], self.env_cfg["clip_observations"])
-        self.last_actions[:] = self.actions[:]
-
-        self.extras["observations"] = {
-            "critic": self.obs_buf,
-            "privileged": None
-        }
-        self.last_dof_vel = self.dof_vel.clone()
-        self.feet_quat = self.links_quat[:, self.feet_indices, :]
-        self.feet_quat_euler = quat_to_xyz(self.feet_quat)
-        self.leg_phase[:, 0] = self.phase
-        self.leg_phase[:, 1] = (self.phase + 0.5) % 1.0
-
-        if torch.rand(1).item() < 0.01:  # Print occasionally
-            print("T1 left x:", footstep_targets_root[:, 0, 0].cpu().numpy())
-            print("T1 right x:", footstep_targets_root[:, 1, 0].cpu().numpy())
-
-        return self.obs_buf, self.rew_buf, self.reset_buf, self.extras
-
-    def randomize_friction(self, progress=1.0):
-        # Calculate progress-based friction range
-        base_range = self.domain_rand_cfg['friction_range']
-        
-        # Start with narrow range, expand to full range as training progresses
-        # Use a more conservative range initially
-        center = 0.75  # Shifted toward higher friction (was 0.5 * (base_range[0] + base_range[1]))
-        half_width = 0.3 * (base_range[1] - base_range[0])  # Reduced variation (was 0.5)
-        
-        # Apply progressively wider range based on training progress but with slower ramp-up
-        progress_scaled = progress * 0.8  # Slow down the progression
-        actual_range = [
-            max(0.6, center - half_width * progress_scaled),  # Set minimum friction
-            min(1.1, center + half_width * progress_scaled)   # Set maximum friction
-        ]
-        
-        # Apply randomization with the calculated range
-        self.robot.set_friction_ratio(
-            friction_ratio = actual_range[0] + torch.rand(self.num_envs, self.robot.n_links, device=self.device) * 
-                            (actual_range[1] - actual_range[0]),
-            link_indices = np.arange(0, self.robot.n_links)
-        )
-
-    def randomize_mass(self, progress=1.0):
-        # Only randomize pelvis mass for simplicity, ignore progress
-        added_mass_range = self.domain_rand_cfg['added_mass_range']
-        added_mass = float(torch.rand(1).item() * (added_mass_range[1] - added_mass_range[0]) + added_mass_range[0])
-        new_mass = max(self.pelvis_mass + added_mass, 0.1)
-        self.pelvis_link.set_mass(new_mass)
-
-    def push_robots(self, progress=1.0):
-        env_ids = torch.arange(self.num_envs, device=self.device)
-        push_env_ids = env_ids[self.episode_length_buf[env_ids] % int(self.domain_rand_cfg['push_interval_s']/self.dt) == 0]
-        if len(push_env_ids) == 0:
-            return
-        
-        # Scale push intensity by progress - much more gradually
-        max_vel_xy = self.domain_rand_cfg['max_push_vel_xy'] * progress * 0.6  # Reduced by 40%
-        max_vel_rp = self.domain_rand_cfg['max_push_vel_rp'] * progress * 0.5   # Reduced by 50%
-        
-        # Reduce number of pushes early in training - more drastically
-        push_probability = min(1.0, 0.2 + 0.8 * progress * progress)  # Square term for slower ramp-up
-        
-        # Randomly select which environments to push based on progress
-        push_mask = torch.rand(len(push_env_ids), device=self.device) < push_probability
-        if not torch.any(push_mask):
-            return
-        
-        # Only push selected environments
-        push_env_ids = push_env_ids[push_mask]
-        
-        # Make pushes smaller in magnitude and biased toward forward direction
-        # This helps the robot learn to recover from pushes in the intended direction
-        new_base_lin_vel = torch.zeros_like(self.base_lin_vel)
-        new_base_ang_vel = torch.zeros_like(self.base_ang_vel)
-        
-        # Generate push velocities with forward bias
-        x_vel = gs_rand_float(-max_vel_xy * 0.5, max_vel_xy, (len(push_env_ids),), self.device)  # Forward bias
-        y_vel = gs_rand_float(-max_vel_xy * 0.7, max_vel_xy * 0.7, (len(push_env_ids),), self.device)  # Reduced lateral
-        z_vel = gs_rand_float(0, max_vel_xy * 0.3, (len(push_env_ids),), self.device)  # Small upward only
-        
-        new_base_lin_vel[push_env_ids] = torch.stack([x_vel, y_vel, z_vel], dim=1)
-        
-        # Reduced angular disturbances, especially for roll (x) which affects lateral stability
-        roll_vel = gs_rand_float(-max_vel_rp * 0.4, max_vel_rp * 0.4, (len(push_env_ids),), self.device)  # Reduced roll
-        pitch_vel = gs_rand_float(-max_vel_rp * 0.7, max_vel_rp * 0.7, (len(push_env_ids),), self.device)  # Less reduction for pitch
-        yaw_vel = gs_rand_float(-max_vel_rp * 0.6, max_vel_rp * 0.6, (len(push_env_ids),), self.device)  # Moderate reduction for yaw
-        
-        new_base_ang_vel[push_env_ids] = torch.stack([roll_vel, pitch_vel, yaw_vel], dim=1)
-        
-        d_vel_xy = new_base_lin_vel - self.base_lin_vel[:, :3]
-        d_vel_rp = new_base_ang_vel - self.base_ang_vel[:, :3]
-        d_pos = d_vel_xy * self.dt
-        d_pos[:, [2]] = 0
-        current_pos = self.robot.get_pos()
-        new_pos = current_pos[push_env_ids] + d_pos[push_env_ids]
-        self.robot.set_pos(new_pos, zero_velocity=False, envs_idx=push_env_ids)
-        current_euler = self.base_euler
-        d_euler = d_vel_rp * self.dt
-        new_euler = current_euler[push_env_ids] + d_euler[push_env_ids]
-        new_quat = xyz_to_quat(new_euler)
-        self.robot.set_quat(new_quat, zero_velocity=False, envs_idx=push_env_ids)
-
-    def get_observations(self):
-        extras = {
-            "observations": {
-                "critic": self.obs_buf,
-                "privileged": None
-            }
-        }
-        return self.obs_buf, extras
-    
-    def get_privileged_observations(self):
-        return None
-
-    def reset_idx(self, envs_idx):
-        if len(envs_idx) == 0:
-            return
-        self.dof_pos[envs_idx] = self.default_dof_pos
-        self.dof_vel[envs_idx] = 0.0
-        self.robot.set_dofs_position(
-            position=self.dof_pos[envs_idx],
-            dofs_idx_local=self.motor_dofs,
-            zero_velocity=True,
-            envs_idx=envs_idx,
-        )
-        self.base_pos[envs_idx] = self.base_init_pos
-        self.base_quat[envs_idx] = self.base_init_quat.reshape(1, -1)
-        self.base_euler = quat_to_xyz(
-            transform_quat_by_quat(torch.ones_like(self.base_quat) * self.inv_base_init_quat, self.base_quat)
-        )
-        self.robot.set_pos(self.base_pos[envs_idx], zero_velocity=True, envs_idx=envs_idx)
-        self.robot.set_quat(self.base_quat[envs_idx], zero_velocity=True, envs_idx=envs_idx)
-        self.base_lin_vel[envs_idx] = 0
-        self.base_ang_vel[envs_idx] = 0
-        self.robot.zero_all_dofs_velocity(envs_idx)
-        self.last_actions[envs_idx] = 0.0
-        self.episode_length_buf[envs_idx] = 0
-        self.reset_buf[envs_idx] = True
-        self.extras["episode"] = {}
-        for key in self.episode_sums.keys():
-            self.extras["episode"]["rew_" + key] = (
-                torch.mean(self.episode_sums[key][envs_idx]).item() / self.env_cfg["episode_length_s"]
-            )
-            self.episode_sums[key][envs_idx] = 0.0
-        self.phase[envs_idx] = gs_rand_float(0.0, 0.2, (len(envs_idx),), self.device)
-        self._resample_goals(envs_idx)
-        # Plan new step sequence and set initial footstep targets
-        for i, idx in enumerate(envs_idx):
-            self.step_sequence[idx] = self.plan_step_sequence(idx)
-            self.current_step_idx[idx] = 0
-            seq = self.step_sequence[idx]
-            if len(seq) >= 2:
-                self.footstep_targets[idx, 0, :] = torch.tensor(seq[0][:3], device=self.device)
-                self.footstep_targets[idx, 1, :] = torch.tensor(seq[1][:3], device=self.device)
-            elif len(seq) == 1:
-                self.footstep_targets[idx, 0, :] = torch.tensor(seq[0][:3], device=self.device)
-                self.footstep_targets[idx, 1, :] = torch.tensor(seq[0][:3], device=self.device)
-            else:
-                self.footstep_targets[idx, :, :] = 0.0
-        self.consecutive_contact_count[envs_idx] = 0
-        self.feet_air_time[envs_idx] = 0.0
-        self.last_contacts[envs_idx] = False
-        self.last_dof_vel[envs_idx] = 0.0
-        self.base_pos[envs_idx, 2] = self.base_init_pos[2] + 0.03
-
-        base_pos = self.base_pos[envs_idx, :2]
-        goal_pos = self.goal_pos[envs_idx, :2]
-        to_goal = goal_pos - base_pos
-        dist_to_goal = torch.norm(to_goal)
-        if dist_to_goal > 1e-6:
-            dir_to_goal = to_goal / dist_to_goal
-        else:
-            dir_to_goal = torch.tensor([1.0, 0.0], device=self.device)
-        self.initial_dir_to_goal[envs_idx] = dir_to_goal
-
-    def reset(self):
-            self.reset_buf[:] = True
-            self.reset_idx(torch.arange(self.num_envs, device=self.device))
-            extras = {
-            "observations": {
-                "critic": self.obs_buf,
-                "privileged": None
-                }
-            }
-            return self.obs_buf, extras
-
-    # --------------------- Reward Functions ---------------------
-    def _reward_tracking_lin_vel(self):
-        lin_vel_error = torch.sum(torch.square(self.commands[:, :2] - self.base_lin_vel[:, :2]), dim=1)
-        return torch.exp(-lin_vel_error / self.reward_cfg["tracking_sigma"])
-
-    def _reward_tracking_ang_vel(self):
-        ang_vel_error = torch.square(self.commands[:, 2] - self.base_ang_vel[:, 2])
-        return torch.exp(-ang_vel_error / self.reward_cfg["tracking_sigma"])
-
-    def _reward_lin_vel_z(self):
-        return torch.square(self.base_lin_vel[:, 2])
-
-    def _reward_action_rate(self):
-        return torch.sum(torch.square(self.last_actions - self.actions), dim=1)
-
-    def _reward_base_height(self):
-        return torch.square(self.base_pos[:, 2] - self.reward_cfg["base_height_target"])
-
-    def _reward_alive(self):
-        return 1.0
-
-    def _reward_gait_contact(self):
-        res = torch.zeros(self.num_envs, dtype=torch.float, device=self.device)
-        for i in range(self.feet_num):
-            is_stance = self.leg_phase[:, i] < 0.55
-            contact = self.contact_forces[:, self.feet_indices[i], 2] > 1
-            res += ~(contact ^ is_stance)
-        return res
-
-    def _reward_gait_swing(self):
-        res = torch.zeros(self.num_envs, dtype=torch.float, device=self.device)
-        for i in range(self.feet_num):
-            is_swing = self.leg_phase[:, i] >= 0.55
-            contact = self.contact_forces[:, self.feet_indices[i], 2] > 1
-            res += ~(contact ^ is_swing)
-        return res
-
-    def _reward_contact_no_vel(self):
-        contact = torch.norm(self.contact_forces[:, self.feet_indices, :3], dim=2) > 1.
-        contact_feet_vel = self.feet_vel * contact.unsqueeze(-1)
-        penalize = torch.square(contact_feet_vel[:, :, :3])
-        return torch.sum(penalize, dim=(1, 2))
-
-    """def _reward_feet_swing_height(self):
-        swing_height_reward = torch.zeros(self.num_envs, device=self.device, dtype=gs.tc_float)
-        for i in range(self.feet_num):
-            is_swing = self.leg_phase[:, i] >= 0.55
-            # Only reward swing foot for being at target height
-            height_error = torch.abs(self.feet_pos[:, i, 2] - self.reward_cfg["feet_height_target"])
-            # Less aggressive penalty, positive reward for being close
-            swing_height_reward += (1.0 - torch.tanh(height_error / 0.05)) * is_swing.float()
-        return swing_height_reward"""
-    
-    def _reward_feet_swing_height(self):
-        contact = torch.norm(self.contact_forces[:, self.feet_indices, :3], dim=2) > 0.5  # Lowered threshold
-        pos_error = torch.square(self.feet_pos[:, :, 2] - self.reward_cfg["feet_height_target"]) * ~contact
-        return torch.sum(pos_error, dim=(1))
-
-    def _reward_orientation(self):
-        return torch.sum(torch.square(self.projected_gravity[:, :2]), dim=1)
-
-    def _reward_ang_vel_xy(self):
-        return torch.sum(torch.square(self.base_ang_vel[:, :2]), dim=1)
-
-    def _reward_dof_vel(self):
-        return torch.sum(torch.square(self.dof_vel), dim=1)
-
-    def _reward_knee_angle(self):
-        return torch.sum(torch.square(self.dof_pos[:, self.knee_indices]), dim=1)
-    
-    def _reward_feet_angle(self):
-        return torch.sum(torch.square(self.feet_quat_euler[:,:,2] - self.feet_quat_euler_ref[:,:,2]), dim=1)
-
-    # New reward functions
-    def _reward_footstep_tracking(self):
-        footstep_reward = torch.zeros(self.num_envs, device=self.device, dtype=gs.tc_float)
-        for i in range(self.feet_num):
-            is_swing = self.leg_phase[:, i] >= 0.55
-            # Compute error between foot position and target in world frame
-            target_world = self.base_pos + self.footstep_targets[:, i, :]
-            error = torch.norm(self.feet_pos[:, i, :] - target_world, dim=1)
-            # Denser reward: exponential decay of error with larger denominator
-            footstep_reward += torch.exp(-error / 0.3) * is_swing.float()
-        return footstep_reward
-
-    def _reward_goal_progress(self):
-        # Reward increase in x-position
-        prev_x = self.base_pos[:, 0] - self.base_lin_vel[:, 0] * self.dt
-        curr_x = self.base_pos[:, 0]
-        return (curr_x - prev_x) * 5.0  # Scale for denser reward
-
-    def _register_reward_functions(self):
-        reward_function_names = [
-            #"tracking_lin_vel",
-            #"tracking_ang_vel",
-            "lin_vel_z",
-            "action_rate",
-            "base_height",
-            "alive",
-            "gait_contact",
-            "gait_swing",
-            "contact_no_vel",
-            "feet_swing_height",
-            "orientation",
-            "ang_vel_xy",
-            "dof_vel",
-            "knee_angle",
-            "feet_angle",
-            "footstep_tracking",
-            "goal_progress",
-        ]
-        for name in reward_function_names:
-            if hasattr(self, f"_reward_{name}"):
-                self.reward_functions[name] = getattr(self, f"_reward_{name}")
-            else:
-                print(f"Warning: No reward function found for {name}")
-        for key in list(self.reward_functions.keys()):
-            if key not in self.reward_scales:
-                print(f"Adding missing reward scale for {key}")
-                self.reward_scales[key] = 0.0
-        for key in list(self.reward_functions.keys()):
-            if self.reward_scales.get(key, 0.0) == 0.0:
-                print(f"Disabling unused reward function: {key}")
-                del self.reward_functions[key]
-
-    def _visualize_footstep_targets(self):
-        self.scene.clear_debug_objects()
-        for env_idx in range(self.num_envs):
-            base_pos = self.base_pos[env_idx].cpu().numpy()
-            base_quat = self.base_quat[env_idx]
-            # Visualize the entire planned step sequence
-            seq = self.step_sequence[env_idx]
-            for i, step in enumerate(seq):
-                # Draw all planned footsteps as spheres
-                pos = np.array(step[:3], dtype=np.float32)
-                # Transform to world frame
-                t_world = transform_by_quat(
-                    torch.tensor(pos, dtype=gs.tc_float, device=self.device).unsqueeze(0), base_quat.unsqueeze(0)
-                )[0].cpu().numpy() + base_pos
-                t_world[2] = 0.01  # Slightly above ground
-                # Color: gray for all, except current targets
-                if i == self.current_step_idx[env_idx]:
-                    color = (0.0, 1.0, 0.0, 0.8)  # Green for current left
-                elif i == self.current_step_idx[env_idx] + 1:
-                    color = (0.0, 0.0, 1.0, 0.8)  # Blue for current right
-                else:
-                    color = (0.5, 0.5, 0.5, 0.5)  # Gray for others
-                self.scene.draw_debug_sphere(
-                    pos=t_world,
-                    radius=0.025,
-                    color=color
-                )
-            # Draw arrows for current footstep targets
-            for foot in range(2):
-                t_base = self.footstep_targets[env_idx, foot].clone()
-                t_base[2] = 0.0
-                t_world = transform_by_quat(
-                    t_base.unsqueeze(0), base_quat.unsqueeze(0)
-                )[0].cpu().numpy() + base_pos
-                t_world[2] = 0.0  # Ensure arrow is on the ground
-                forward_vec_base = torch.tensor([0.15, 0.0, 0.0], device=self.device).unsqueeze(0)
-                forward_vec_world = transform_by_quat(forward_vec_base, base_quat.unsqueeze(0))[0].cpu().numpy()
-                self.scene.draw_debug_arrow(
-                    pos=t_world,
-                    vec=forward_vec_world,
-                    radius=0.01,
-                    color=(0.0, 1.0, 0.0, 0.8) if foot == 0 else (0.0, 0.0, 1.0, 0.8)
-                )
-
-    def _visualize_heading(self):
-        self.scene.clear_debug_objects()  # Optional: clear previous arrows
-        for env_idx in range(self.num_envs):
-            base_pos = self.base_pos[env_idx].cpu().numpy()
-            base_quat = self.base_quat[env_idx]
-            # Forward vector in base frame (length 0.3m)
-            forward_vec_base = torch.tensor([0.15, 0.0, 0.0], device=self.device).unsqueeze(0)
-            # Transform to world frame
-            forward_vec_world = transform_by_quat(forward_vec_base, base_quat.unsqueeze(0))[0].cpu().numpy()
-            # Draw arrow
-            self.scene.draw_debug_arrow(
-                pos=base_pos,
-                vec=forward_vec_world,
-                radius=0.01,
-                color=(1.0, 0.0, 0.0, 0.8)  # Red arrow for heading
-            )
-
-    
diff --git a/my_deeploco/g1_nav_eval.py b/my_deeploco/g1_nav_eval.py
deleted file mode 100644
index 15b1244..0000000
--- a/my_deeploco/g1_nav_eval.py
+++ /dev/null
@@ -1,268 +0,0 @@
-import argparse
-import os
-import pickle
-import torch
-from g1_nav_env import G1DeeplocoEnv
-from rsl_rl.runners import OnPolicyRunner
-import genesis as gs
-import time
-import numpy as np
-from datetime import datetime
-
-def get_eval_train_cfg():
-    """Define a minimal train_cfg for evaluation."""
-    train_cfg_dict = {
-        "algorithm": {
-            "class_name": "PPO",  # Explicitly set to match training
-        },
-        "policy": {
-            "class_name": "ActorCritic",  # Match training policy
-            "activation": "elu",
-            "actor_hidden_dims": [512, 256, 128],
-            "critic_hidden_dims": [512, 256, 128],
-            "init_noise_std": 0.1,  # Lower noise for smoother evaluation
-            "noise_std_type": "log"  # Use log parameterization like in training
-        },
-        "num_steps_per_env": 128,  # Required by OnPolicyRunner
-        "save_interval": 50,       # Required by OnPolicyRunner
-        "empirical_normalization": False,  # Required by OnPolicyRunner
-    }
-    return train_cfg_dict
-
-def add_metrics_tracking(env):
-    """Add metrics tracking to the environment for better visualization."""
-    import types
-    
-    # Setup metrics buffers
-    env.metrics = {
-        "foot_clearance": [],
-        "step_length": [],
-        "base_height": [],
-        "forward_velocity": [],
-        "lateral_velocity": [],
-        "angular_velocity": [],
-        "alternating_contacts": [],
-        "feet_air_time": [],
-        "torso_roll": [],
-        "torso_pitch": []
-    }
-    
-    # Override step function to collect metrics
-    original_step = env.step
-    
-    def step_with_metrics(self, actions):
-        obs, rew, reset, extras = original_step(actions)
-        
-        # Collect metrics
-        if hasattr(self, 'feet_pos'):
-            # Get foot heights when not in contact
-            contact = self.contact_forces[:, self.feet_indices, 2] > 5.0
-            feet_height = self.feet_pos[:, :, 2]
-            non_contact_heights = torch.where(
-                ~contact,
-                feet_height,
-                torch.zeros_like(feet_height)
-            )
-            mean_clearance = torch.mean(non_contact_heights[non_contact_heights > 0]).item() if torch.any(non_contact_heights > 0) else 0
-            
-            # Step length
-            feet_pos_rel = self.feet_pos - self.base_pos.unsqueeze(1)
-            mean_step_length = torch.mean(torch.abs(feet_pos_rel[:, :, 0])).item()
-            
-            # Base height
-            mean_base_height = torch.mean(self.base_pos[:, 2]).item()
-            
-            # Velocities
-            mean_forward_vel = torch.mean(self.base_lin_vel[:, 0]).item()
-            mean_lateral_vel = torch.mean(self.base_lin_vel[:, 1]).item()
-            mean_angular_vel = torch.mean(self.base_ang_vel[:, 2]).item()
-            
-            # Alternating gait - measure if left/right feet are alternating
-            alternating = torch.logical_xor(contact[:, 0], contact[:, 1]).float()
-            mean_alternating = torch.mean(alternating).item()
-            
-            # Feet air time - average time feet spend in air
-            mean_air_time = torch.mean(self.feet_air_time).item()
-            
-            # Torso orientation
-            mean_roll = torch.mean(torch.abs(self.base_euler[:, 0])).item()  # Roll (lateral tilt)
-            mean_pitch = torch.mean(torch.abs(self.base_euler[:, 1])).item()  # Pitch (forward tilt)
-            
-            # Store metrics
-            self.metrics["foot_clearance"].append(mean_clearance)
-            self.metrics["step_length"].append(mean_step_length)
-            self.metrics["base_height"].append(mean_base_height)
-            self.metrics["forward_velocity"].append(mean_forward_vel)
-            self.metrics["lateral_velocity"].append(mean_lateral_vel)
-            self.metrics["angular_velocity"].append(mean_angular_vel)
-            self.metrics["alternating_contacts"].append(mean_alternating)
-            self.metrics["feet_air_time"].append(mean_air_time)
-            self.metrics["torso_roll"].append(mean_roll)
-            self.metrics["torso_pitch"].append(mean_pitch)
-            
-            # Print metrics occasionally
-            if len(self.metrics["foot_clearance"]) % 50 == 0:
-                print(f"\nMetrics (avg last 50 steps):")
-                print(f"  Velocity: forward={sum(self.metrics['forward_velocity'][-50:]) / 50:.3f} m/s, "
-                      f"lateral={sum(self.metrics['lateral_velocity'][-50:]) / 50:.3f} m/s, "
-                      f"angular={sum(self.metrics['angular_velocity'][-50:]) / 50:.3f} rad/s")
-                print(f"  Gait: clearance={sum(self.metrics['foot_clearance'][-50:]) / 50:.3f} m, "
-                      f"step length={sum(self.metrics['step_length'][-50:]) / 50:.3f} m, "
-                      f"air time={sum(self.metrics['feet_air_time'][-50:]) / 50:.3f} s, "
-                      f"alternating={sum(self.metrics['alternating_contacts'][-50:]) / 50:.3f}")
-                print(f"  Stability: height={sum(self.metrics['base_height'][-50:]) / 50:.3f} m, "
-                      f"roll={sum(self.metrics['torso_roll'][-50:]) / 50:.3f} rad, "
-                      f"pitch={sum(self.metrics['torso_pitch'][-50:]) / 50:.3f} rad")
-                
-        return obs, rew, reset, extras
-    
-    env.step = types.MethodType(step_with_metrics, env)
-    return env
-
-def main():
-    parser = argparse.ArgumentParser(description="G1 Deeploco Evaluation Script")
-    parser.add_argument("-e", "--exp_name", type=str, default="g1-deeploco-walk")
-    parser.add_argument("--ckpt", type=int, default=1000, help="Checkpoint iteration to load")
-    parser.add_argument("--cmd_x", type=float, default=0.3, help="Forward velocity command")
-    parser.add_argument("--cmd_y", type=float, default=0.0, help="Lateral velocity command")
-    parser.add_argument("--cmd_yaw", type=float, default=0.0, help="Yaw velocity command")
-    parser.add_argument("--record", action="store_true", help="Record video of evaluation")
-    parser.add_argument("--duration", type=int, default=60, help="Evaluation duration in seconds")
-    args = parser.parse_args()
-
-    gs.init()
-
-    # Load environment configs from cfgs.pkl, but not train_cfg
-    log_dir = f"/home/dodolab/tkworkspace/My_deeploco/my_deeploco/log/{args.exp_name}"
-    if not os.path.exists(f"{log_dir}/cfgs.pkl"):
-        raise FileNotFoundError(f"Configuration file {log_dir}/cfgs.pkl not found. Run training first.")
-    
-    env_cfg, obs_cfg, reward_cfg, command_cfg, train_saved_cfg, domain_rand_cfg = pickle.load(open(f"{log_dir}/cfgs.pkl", "rb"))
-    
-    # Set reward scales to zero for evaluation (instead of clearing them)
-    original_reward_scales = reward_cfg["reward_scales"].copy()
-    reward_cfg["reward_scales"] = {key: 0.0 for key in original_reward_scales.keys()}
-
-    # Set fixed commands for evaluation
-    command_cfg["curriculum"] = False
-    command_cfg["ranges"] = {
-        "lin_vel_x": [args.cmd_x, args.cmd_x],
-        "lin_vel_y": [args.cmd_y, args.cmd_y],
-        "ang_vel_yaw": [args.cmd_yaw, args.cmd_yaw],
-        "heading": [0.0, 0.0]
-    }
-
-    # Disable domain randomization for evaluation
-    domain_rand_cfg["randomize_friction"] = False
-    domain_rand_cfg["randomize_mass"] = False
-    domain_rand_cfg["push_robots"] = False
-
-    # Create environment with 1 env for evaluation
-    env = G1DeeplocoEnv(
-        num_envs=1,
-        env_cfg=env_cfg,
-        obs_cfg=obs_cfg,
-        reward_cfg=reward_cfg,
-        command_cfg=command_cfg,
-        domain_rand_cfg=domain_rand_cfg,
-        show_viewer=True,
-        device="cuda:0"  # Match training device
-    )
-    
-    # Add metrics tracking
-    env = add_metrics_tracking(env)
-
-    # Try to detect whether the model uses log_std or std parameter
-    # First, check the saved training config if it exists
-    if train_saved_cfg and "policy" in train_saved_cfg and "noise_std_type" in train_saved_cfg["policy"]:
-        noise_std_type = train_saved_cfg["policy"]["noise_std_type"]
-    else:
-        # Default to log for newer models
-        noise_std_type = "log"
-    
-    # Use a fresh train_cfg for evaluation
-    train_cfg = get_eval_train_cfg()
-    train_cfg["policy"]["noise_std_type"] = noise_std_type
-    
-    print(f"Using noise_std_type: {noise_std_type}")
-    print(f"Evaluating with commands: forward={args.cmd_x}, lateral={args.cmd_y}, yaw={args.cmd_yaw}")
-
-    # Initialize runner and load checkpoint
-    runner = OnPolicyRunner(env, train_cfg, log_dir, device="cuda:0")
-    resume_path = os.path.join(log_dir, f"model_{args.ckpt}.pt")
-    if not os.path.exists(resume_path):
-        raise FileNotFoundError(f"Checkpoint {resume_path} not found.")
-    
-    # Try loading the model, if it fails due to std/log_std mismatch, try the other type
-    try:
-        runner.load(resume_path)
-    except RuntimeError as e:
-        if "Missing key(s) in state_dict: \"std\"" in str(e):
-            print("Model uses log_std instead of std, updating config...")
-            train_cfg["policy"]["noise_std_type"] = "log"
-            runner = OnPolicyRunner(env, train_cfg, log_dir, device="cuda:0")
-            runner.load(resume_path)
-        elif "Missing key(s) in state_dict: \"log_std\"" in str(e):
-            print("Model uses std instead of log_std, updating config...")
-            train_cfg["policy"]["noise_std_type"] = "scalar"
-            runner = OnPolicyRunner(env, train_cfg, log_dir, device="cuda:0")
-            runner.load(resume_path)
-        else:
-            raise e
-            
-    policy = runner.get_inference_policy(device="cuda:0")
-
-    # Setup video recording if requested
-    if args.record:
-        record_dir = os.path.join(log_dir, "videos")
-        os.makedirs(record_dir, exist_ok=True)
-        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
-        video_path = os.path.join(record_dir, f"eval_x{args.cmd_x}_y{args.cmd_y}_yaw{args.cmd_yaw}_{timestamp}.mp4")
-        print(f"Recording video to {video_path}")
-        env.scene.toggle_video_recording(video_path)
-    
-    # Reset environment and run evaluation loop
-    obs, _ = env.reset()
-    start_time = time.time()
-    step_count = 0
-    
-    with torch.no_grad():
-        while True:
-            actions = policy(obs)
-            obs, rews, dones, infos = env.step(actions)
-            
-            if hasattr(env, '_visualize_footstep_targets'):
-                env._visualize_footstep_targets()
-            if hasattr(env, '_visualize_heading'):
-                env._visualize_heading()
-            
-            step_count += 1
-            elapsed_time = time.time() - start_time
-            
-            # Print status every 5 seconds
-            if step_count % 250 == 0:
-                print(f"Evaluation progress: {elapsed_time:.1f}s / {args.duration}s")
-            
-            # Reset if done or if we've reached the evaluation duration
-            if dones.any() or elapsed_time >= args.duration:
-                if elapsed_time >= args.duration:
-                    print(f"\nEvaluation completed after {elapsed_time:.1f} seconds ({step_count} steps)")
-                    # Print summary statistics
-                    if hasattr(env, 'metrics') and len(env.metrics['forward_velocity']) > 0:
-                        print("\nFinal metrics summary:")
-                        metrics_len = min(step_count, len(env.metrics['forward_velocity']))
-                        print(f"  Avg forward velocity: {np.mean(env.metrics['forward_velocity'][-metrics_len:]):.3f} m/s")
-                        print(f"  Avg step length: {np.mean(env.metrics['step_length'][-metrics_len:]):.3f} m")
-                        print(f"  Avg foot clearance: {np.mean(env.metrics['foot_clearance'][-metrics_len:]):.3f} m")
-                        print(f"  Avg alternating gait: {np.mean(env.metrics['alternating_contacts'][-metrics_len:]):.3f}")
-                        print(f"  Avg torso roll: {np.mean(env.metrics['torso_roll'][-metrics_len:]):.3f} rad")
-                    
-                    if args.record:
-                        print("Finalizing video recording...")
-                        env.scene.toggle_video_recording(None)
-                    break
-                
-                obs, _ = env.reset()  # Reset if done
-
-if __name__ == "__main__":
-    main() 
\ No newline at end of file
diff --git a/my_deeploco/g1_nav_train.py b/my_deeploco/g1_nav_train.py
deleted file mode 100644
index a5bd41d..0000000
--- a/my_deeploco/g1_nav_train.py
+++ /dev/null
@@ -1,252 +0,0 @@
-import argparse
-import os
-import pickle
-import shutil
-import types
-from datetime import datetime
-from g1_nav_env import G1DeeplocoEnv
-from rsl_rl.runners import OnPolicyRunner
-import genesis as gs
-
-def get_train_cfg(exp_name, max_iterations):
-    train_cfg_dict = {
-        "algorithm": {
-            "class_name": "PPO",
-            "clip_param": 0.2,
-            "desired_kl": 0.01,
-            "entropy_coef": 0.01,  # Increased to encourage more exploration of goal_directed actions
-            "gamma": 0.99,
-            "lam": 0.95,
-            "learning_rate": 1e-3,  # Further reduced for smoother learning
-            "max_grad_norm": 1.0,
-            "num_learning_epochs": 10,  # Reduced to prevent overfitting to heuristics footsteps
-            "num_mini_batches": 4,
-            "schedule": "adaptive",
-            "use_clipped_value_loss": True,
-            "value_loss_coef": 0.25, # Increased to prioritize value learning for goal tasks
-        },
-        "init_member_classes": {},
-        "policy": {
-            "class_name": "ActorCritic",
-            "activation": "elu",
-            "actor_hidden_dims": [512, 256, 128],  # Larger network
-            "critic_hidden_dims": [512, 256, 128],  # Larger network
-            "init_noise_std": 0.5,  # Increased for more exploration
-            "noise_std_type": "log",
-        },
-        "runner": {
-            "checkpoint": -1,
-            "experiment_name": exp_name,
-            "load_run": -1,
-            "log_interval": 10,  # More frequent logging to monitor goal progress
-            "max_iterations": max_iterations,
-            "record_interval": -1,
-            "resume": True,  # Start fresh for new task
-            "resume_path": "/home/dodolab/tkworkspace/My_deeploco/my_deeploco/log/g1-deeploco-goal_20250426_140711",  
-            "run_name": "",
-            "runner_class_name": "OnPolicyRunner",
-        },
-        "save_interval": 50,
-        "empirical_normalization": False,
-        "num_steps_per_env": 24, # Increased for longer rollouts to capture goal-reaching dynamics
-        "seed": 1,
-        "logger": "wandb",
-        "wandb_project": exp_name,
-    }
-    return train_cfg_dict
-
-def get_cfgs():
-    env_cfg = {
-        "num_actions": 12,
-        "dof_names": [
-            "left_hip_pitch_joint",
-            "left_hip_roll_joint",
-            "left_hip_yaw_joint",
-            "left_knee_joint",
-            "left_ankle_pitch_joint",
-            "left_ankle_roll_joint",
-            "right_hip_pitch_joint",
-            "right_hip_roll_joint",
-            "right_hip_yaw_joint",
-            "right_knee_joint",
-            "right_ankle_pitch_joint",
-            "right_ankle_roll_joint"
-        ],
-        "default_joint_angles": {
-            "left_hip_pitch_joint": -0.1,
-            "left_hip_roll_joint": 0.02,
-            "left_hip_yaw_joint": 0.0,
-            "left_knee_joint": 0.1,
-            "left_ankle_pitch_joint": -0.1,
-            "left_ankle_roll_joint": -0.05,
-            "right_hip_pitch_joint": -0.1,
-            "right_hip_roll_joint": -0.02,
-            "right_hip_yaw_joint": 0.0,
-            "right_knee_joint": 0.1,
-            "right_ankle_pitch_joint": -0.1,
-            "right_ankle_roll_joint": 0.05
-        },
-        "kp": 100.0,
-        "kd": 2.5,
-        "stiffness": {
-            "left_hip_pitch_joint": 120.0,
-            "left_hip_roll_joint": 160.0,
-            "left_hip_yaw_joint": 100.0,
-            "left_knee_joint": 140.0,
-            "left_ankle_pitch_joint": 70.0,
-            "left_ankle_roll_joint": 70.0,
-            "right_hip_pitch_joint": 120.0,
-            "right_hip_roll_joint": 160.0,
-            "right_hip_yaw_joint": 100.0,
-            "right_knee_joint": 140.0,
-            "right_ankle_pitch_joint": 70.0,
-            "right_ankle_roll_joint": 70.0
-        },
-        "damping": {
-            "left_hip_pitch_joint": 3.5,
-            "left_hip_roll_joint": 4.5,
-            "left_hip_yaw_joint": 3.0,
-            "left_knee_joint": 6.5,
-            "left_ankle_pitch_joint": 3.0,
-            "left_ankle_roll_joint": 3.0,
-            "right_hip_pitch_joint": 3.5,
-            "right_hip_roll_joint": 4.5,
-            "right_hip_yaw_joint": 3.0,
-            "right_knee_joint": 6.5,
-            "right_ankle_pitch_joint": 3.0,
-            "right_ankle_roll_joint": 3.0
-        },
-        "terminate_after_contacts_on": ["pelvis"],
-        "termination_if_pelvis_z_less_than": 0.35,
-        "base_init_pos": [0.0, 0.0, 0.8],
-        "base_init_quat": [1.0, 0.0, 0.0, 0.0],
-        "action_scale": 1.0,
-        "episode_length_s": 20.0, # Shorter for faster goal-reaching
-        "resampling_time_s": 10.0, # More frequent resampling to capture goal-reaching dynamics
-        "simulation_action_latency": False,
-        "clip_actions": 1.0,
-        "clip_observations": 10.0,
-        "feet_height_target": 0.075,
-        # New parameters for goal-directed task, updating for new heuristic goal/footstep methods
-        "goal_distance_range": [3.0, 10.0],
-        "goal_angle_range": [-0.785, 0.785],
-        "goal_reached_threshold": 0.2,
-        "step_size": 0.10,
-        "step_gap": 0.20,
-        "period": 1.10,
-        "swing_duration": 0.45,
-        "stance_duration": 0.65,
-    }
-    obs_cfg = {
-        "num_obs": 3 + 3 + 3 + 12 + 12 + 12 + 2 + 6 + 2,  # adjust as needed
-        "obs_scales": {
-            "lin_vel": 2.0,
-            "ang_vel": 0.25,
-            "dof_pos": 1.0,
-            "dof_vel": 0.05,
-            "goal_pos": 0.1,
-            "footstep_targets": 2.0,
-        }
-    }
-    reward_cfg = {
-        "tracking_sigma": 0.25,
-        "base_height_target": 0.75,
-        "feet_height_target": 0.075,
-        "reward_scales": {
-            "tracking_lin_vel": 0.0,
-            "tracking_ang_vel": 0.0,
-            "lin_vel_z": -1.0,
-            "action_rate": -0.2,
-            "base_height": -2.0,
-            "alive": 0.2,
-            "gait_contact": 0.5,
-            "gait_swing": -0.5,
-            "contact_no_vel": -0.5,
-            "feet_swing_height": -2.0,
-            "orientation": -2.0,
-            "ang_vel_xy": -1.0,
-            "dof_vel": -0.01,
-            "knee_angle": 0.1,
-            "feet_angle": -0.01,
-            "goal_progress": 3.0,
-            "footstep_tracking": 8.0,
-        }
-    }
-    command_cfg = {
-        "num_commands": 3,
-        "lin_vel_x_range": [0.3, 0.3],  # or [0.3, 0.4]
-        "lin_vel_y_range": [0.0, 0.0],
-        "ang_vel_range": [0.0, 0.0],
-    }
-    domain_rand_cfg = {
-        "randomize_friction": True,
-        "friction_range": [0.8, 1.2],
-        "randomize_mass": True,
-        "added_mass_range": [-0.1, 0.2],
-        "push_robots": True,
-        "push_interval_s": 20.0,
-        "max_push_vel_xy": 0.1,  # Reduced for goal-reaching
-        "max_push_vel_rp": 0.5,
-    }
-    return env_cfg, obs_cfg, reward_cfg, command_cfg, domain_rand_cfg
-
-def main():
-    parser = argparse.ArgumentParser(description="G1 Deeploco Goal-Directed Training Script")
-    parser.add_argument("-e", "--exp_name", type=str, default="g1-deeploco-goal")
-    parser.add_argument("-B", "--num_envs", type=int, default=4096)  # Reduced for stability
-    parser.add_argument("--max_iterations", type=int, default=10_000)
-    parser.add_argument("--show_viewer", action="store_true", help="Show the viewer during training")
-    parser.add_argument("--path", type=str, default=None)  # No default checkpoint
-    args = parser.parse_args()
-
-    gs.init(logging_level="warning")
-
-    # Generate a unique log directory with timestamp
-    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
-    log_dir = f"my_deeploco/log/{args.exp_name}_{timestamp}"
-    os.makedirs(log_dir, exist_ok=True)
-
-    env_cfg, obs_cfg, reward_cfg, command_cfg, domain_rand_cfg = get_cfgs()
-    train_cfg = get_train_cfg(args.exp_name, args.max_iterations)
-
-    def env_modifier(env):
-        for i, name in enumerate(env_cfg["dof_names"]):
-            env.robot.set_dofs_kp([env_cfg["stiffness"][name]], [env.motor_dofs[i]])
-            env.robot.set_dofs_kv([env_cfg["damping"][name]], [env.motor_dofs[i]])
-        ankle_indices = [4, 5, 10, 11]
-        hip_roll_indices = [1, 7]
-        knee_indices = [3, 9]
-        env.robot.set_dofs_kp([100.0] * len(ankle_indices), ankle_indices)
-        env.robot.set_dofs_kv([15.0] * len(ankle_indices), ankle_indices)
-        env.robot.set_dofs_kp([150.0] * len(hip_roll_indices), hip_roll_indices)
-        env.robot.set_dofs_kv([20.0] * len(hip_roll_indices), hip_roll_indices)
-        env.robot.set_dofs_kp([120.0] * len(knee_indices), knee_indices)
-        env.robot.set_dofs_kv([15.0] * len(knee_indices), knee_indices)  # Increased damping
-        return env
-
-    env = G1DeeplocoEnv(
-        num_envs=args.num_envs,
-        env_cfg=env_cfg,
-        obs_cfg=obs_cfg,
-        reward_cfg=reward_cfg,
-        command_cfg=command_cfg,
-        domain_rand_cfg=domain_rand_cfg,
-        show_viewer=args.show_viewer,
-        device="cuda"
-    )
-    
-    env = env_modifier(env)
-    
-    runner = OnPolicyRunner(env, train_cfg, log_dir, device="cuda:0")
-    if args.path and os.path.exists(args.path):
-        runner.load(args.path)
-
-    pickle.dump(
-        [env_cfg, obs_cfg, reward_cfg, command_cfg, train_cfg, domain_rand_cfg],
-        open(f"{log_dir}/cfgs.pkl", "wb"),
-    )
-
-    runner.learn(num_learning_iterations=args.max_iterations, init_at_random_ep_len=True)
-
-if __name__ == "__main__":
-    main()
\ No newline at end of file
diff --git a/my_deeploco/g1_train.py b/my_deeploco/g1_train.py
deleted file mode 100644
index 68796f4..0000000
--- a/my_deeploco/g1_train.py
+++ /dev/null
@@ -1,249 +0,0 @@
-import argparse
-import os
-import pickle
-import shutil
-import types  # Add this import
-from datetime import datetime
-from g1_env import G1DeeplocoEnv
-from rsl_rl.runners import OnPolicyRunner
-import genesis as gs
-
-def get_train_cfg(exp_name, max_iterations):
-    train_cfg_dict = {
-        "algorithm": {
-            "class_name": "PPO",
-            "clip_param": 0.2,
-            "desired_kl": 0.01,
-            "entropy_coef": 0.01,
-            "gamma": 0.99,
-            "lam": 0.95,
-            "learning_rate": 1e-3,
-            "max_grad_norm": 1.0,
-            "num_learning_epochs": 10,
-            "num_mini_batches": 4,
-            "schedule": "adaptive",
-            "use_clipped_value_loss": True,
-            "value_loss_coef": 0.25,
-        },
-        "init_member_classes": {},
-        "policy": {
-            "class_name": "ActorCritic",
-            "activation": "elu",
-            "actor_hidden_dims": [512, 256, 128],
-            "critic_hidden_dims": [512, 256, 128],
-            "init_noise_std": 0.5,
-            "noise_std_type": "log",
-        },
-        "runner": {
-            "checkpoint": -1,
-            "experiment_name": exp_name,
-            "load_run": -1,
-            "log_interval": 10,
-            "max_iterations": max_iterations,
-            "record_interval": -1,
-            "resume": True,
-            "resume_path": "home/dodolab/tkworkspace/My_deeploco/log/g1-deeploco-walk",
-            "run_name": "",
-            "runner_class_name": "OnPolicyRunner",
-        },
-        "save_interval": 50,
-        "empirical_normalization": False,
-        "num_steps_per_env": 24,
-        "seed": 1,
-    }
-
-    return train_cfg_dict
-
-def get_cfgs():
-    env_cfg = {
-        "num_actions": 12, 
-        "dof_names": [
-            "left_hip_pitch_joint",
-            "left_hip_roll_joint",
-            "left_hip_yaw_joint",
-            "left_knee_joint",
-            "left_ankle_pitch_joint",
-            "left_ankle_roll_joint",
-            "right_hip_pitch_joint",
-            "right_hip_roll_joint",
-            "right_hip_yaw_joint",
-            "right_knee_joint",
-            "right_ankle_pitch_joint",
-            "right_ankle_roll_joint"
-        ],
-        "default_joint_angles": {
-            "left_hip_pitch_joint": -0.1,
-            "left_hip_roll_joint": 0.02,      # Slight outward roll
-            "left_hip_yaw_joint": 0.0,
-            "left_knee_joint": 0.2,
-            "left_ankle_pitch_joint": -0.1,
-            "left_ankle_roll_joint": -0.05,
-            "right_hip_pitch_joint": -0.1,
-            "right_hip_roll_joint": -0.02,    # Mirror hip roll
-            "right_hip_yaw_joint": 0.0,
-            "right_knee_joint": 0.2,
-            "right_ankle_pitch_joint": -0.1,
-            "right_ankle_roll_joint": 0.05   # Mirror of left
-        },
-        # PD gains matching Unitree config
-        "kp": 100.0,
-        "kd": 2.5,
-        "stiffness": {
-            "left_hip_pitch_joint": 100.0,   # Increased from 88.0
-            "left_hip_roll_joint": 150.0,    # Increased from 139.0
-            "left_hip_yaw_joint": 100.0,     # Increased from 88.0
-            "left_knee_joint": 150.0,        # Increased from 139.0
-            "left_ankle_pitch_joint": 60.0,  # Increased from 50.0
-            "left_ankle_roll_joint": 60.0,   # Increased from 50.0
-            "right_hip_pitch_joint": 100.0,  # Increased from 88.0
-            "right_hip_roll_joint": 150.0,   # Increased from 139.0
-            "right_hip_yaw_joint": 100.0,    # Increased from 88.0
-            "right_knee_joint": 150.0,       # Increased from 139.0
-            "right_ankle_pitch_joint": 60.0, # Increased from 50.0
-            "right_ankle_roll_joint": 60.0   # Increased from 50.0
-        },
-        "damping": {
-            "left_hip_pitch_joint": 3.0,     # Increased from 2.0
-            "left_hip_roll_joint": 4.0,      # Increased from 2.5
-            "left_hip_yaw_joint": 3.0,       # Increased from 2.0
-            "left_knee_joint": 6.0,          # Increased from 5.0
-            "left_ankle_pitch_joint": 2.5,   # Increased from 1.5
-            "left_ankle_roll_joint": 2.5,    # Increased from 1.5
-            "right_hip_pitch_joint": 3.0,    # Increased from 2.0
-            "right_hip_roll_joint": 4.0,     # Increased from 2.5
-            "right_hip_yaw_joint": 3.0,      # Increased from 2.0
-            "right_knee_joint": 6.0,         # Increased from 5.0
-            "right_ankle_pitch_joint": 2.5,  # Increased from 1.5
-            "right_ankle_roll_joint": 2.5    # Increased from 1.5
-        },
-        # Termination
-        "terminate_after_contacts_on": ["pelvis"],  # Remove knee links to allow for recovery
-        "termination_if_pelvis_z_less_than": 0.35,  # Reduced from 0.5 to allow crouching
-        # Base pose
-        "base_init_pos": [0.0, 0.0, 0.8],  # Increased height for better starting position
-        "base_init_quat": [1.0, 0.0, 0.0, 0.0],
-        "action_scale": 1.0,
-        "episode_length_s": 20.0,
-        "resampling_time_s": 10.0,
-        "simulation_action_latency": False,
-        "clip_actions": 100.0,
-        "clip_observations": 100.0,
-        "feet_height_target": 0.085,  
-    }
-    obs_cfg = {
-        "num_obs": 47,
-        "obs_scales": {
-            "lin_vel": 2.0,
-            "ang_vel": 0.25,
-            "dof_pos": 1.0,
-            "dof_vel": 0.05,
-        }
-    }
-    reward_cfg = {
-        "tracking_sigma": 0.25,
-        "base_height_target": 0.75,  # Increased from 0.6 for more upright posture
-        "feet_height_target": 0.085,   # Increased from 0.08 for better foot clearance
-        "reward_scales": {
-            "tracking_lin_vel": 1.0,
-            "tracking_ang_vel": 0.5,
-            "lin_vel_z": -1.0,
-            "action_rate": -0.2,
-            "base_height": -30.0,
-            "alive": 0.2,
-            "gait_contact": 0.5,
-            "gait_swing": -0.5,
-            "contact_no_vel": -0.5,
-            "feet_swing_height": -20.0,
-            "orientation": -1.0,
-            "ang_vel_xy": -1.0,
-            "dof_vel": -0.01,
-            "knee_angle": 0.1,
-            "feet_angle": -0.01,
-        }
-    }
-    command_cfg = {
-        "num_commands": 3,
-        "lin_vel_x_range": [0.3, 0.3],
-        "lin_vel_y_range": [-0.2, 0.2],
-        "ang_vel_range": [-0.5, 0.5],
-    }
-    domain_rand_cfg = {
-        "randomize_friction": True,
-        "friction_range": [0.8, 1.2],
-        "randomize_mass": True,
-        "added_mass_range": [-0.1, 0.2],
-        "push_robots": True,
-        "push_interval_s": 20.0,
-        "max_push_vel_xy": 0.2,
-        "max_push_vel_rp": 0.5,
-    }
-    return env_cfg, obs_cfg, reward_cfg, command_cfg, domain_rand_cfg
-
-def main():
-    parser = argparse.ArgumentParser(description="G1 Deeploco Training Script")
-    parser.add_argument("-e", "--exp_name", type=str, default="g1-deeploco-walk")
-    parser.add_argument("-B", "--num_envs", type=int, default=4096)
-    parser.add_argument("--max_iterations", type=int, default=10_000)
-    parser.add_argument("--show_viewer", action="store_true", help="Show the viewer during training")
-    parser.add_argument("--path", type=str, default="home/dodolab/tkworkspace/My_deeploco/my_deeploco/log/g1-deeploco-walk")
-    args = parser.parse_args()
-
-    gs.init(logging_level="warning")
-
-    # Generate a unique log directory with timestamp
-    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
-    log_dir = f"my_deeploco/log/{args.exp_name}_{timestamp}"
-    os.makedirs(log_dir, exist_ok=True)
-
-    env_cfg, obs_cfg, reward_cfg, command_cfg, domain_rand_cfg = get_cfgs()
-    train_cfg = get_train_cfg(args.exp_name, args.max_iterations)
-
-    # Create a function to modify the environment's joint stiffness and damping
-    def env_modifier(env):
-        # Apply stiffness and damping values from config
-        for i, name in enumerate(env_cfg["dof_names"]):
-            env.robot.set_dofs_kp([env_cfg["stiffness"][name]], [env.motor_dofs[i]])
-            env.robot.set_dofs_kv([env_cfg["damping"][name]], [env.motor_dofs[i]])
-        
-        # Adjust specific joint gains for better walking stability
-        ankle_indices = [4, 5, 10, 11]
-        hip_roll_indices = [1, 7]
-        knee_indices = [3, 9]
-
-        env.robot.set_dofs_kp([100.0] * len(ankle_indices), ankle_indices)
-        env.robot.set_dofs_kv([15.0] * len(ankle_indices), ankle_indices)
-        env.robot.set_dofs_kp([150.0] * len(hip_roll_indices), hip_roll_indices)
-        env.robot.set_dofs_kv([20.0] * len(hip_roll_indices), hip_roll_indices)
-        env.robot.set_dofs_kp([120.0] * len(knee_indices), knee_indices)
-        env.robot.set_dofs_kv([12.0] * len(knee_indices), knee_indices)
-
-        # Remove domain randomization delay logic, always use original step
-        return env
-
-    env = G1DeeplocoEnv(
-        num_envs=args.num_envs,
-        env_cfg=env_cfg,
-        obs_cfg=obs_cfg,
-        reward_cfg=reward_cfg,
-        command_cfg=command_cfg,
-        domain_rand_cfg=domain_rand_cfg,
-        show_viewer=args.show_viewer,
-        device="cuda"
-    )
-    
-    # Apply the environment modifications
-    env = env_modifier(env)
-    
-    runner = OnPolicyRunner(env, train_cfg, log_dir, device="cuda:0")
-    if args.path and os.path.exists(args.path):
-        runner.load(args.path)
-
-    pickle.dump(
-        [env_cfg, obs_cfg, reward_cfg, command_cfg, train_cfg, domain_rand_cfg],
-        open(f"{log_dir}/cfgs.pkl", "wb"),
-    )
-
-    runner.learn(num_learning_iterations=args.max_iterations, init_at_random_ep_len=True)
-
-if __name__ == "__main__":
-    main()
\ No newline at end of file
diff --git a/my_deeploco/rsl_rl/__init__.py b/my_deeploco/rsl_rl/__init__.py
deleted file mode 100644
index ebc2e20..0000000
--- a/my_deeploco/rsl_rl/__init__.py
+++ /dev/null
@@ -1,6 +0,0 @@
-# Copyright (c) 2021-2025, ETH Zurich and NVIDIA CORPORATION
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-"""Main module for the rsl_rl package."""
diff --git a/my_deeploco/rsl_rl/__pycache__/__init__.cpython-310.pyc b/my_deeploco/rsl_rl/__pycache__/__init__.cpython-310.pyc
deleted file mode 100644
index e169a96..0000000
Binary files a/my_deeploco/rsl_rl/__pycache__/__init__.cpython-310.pyc and /dev/null differ
diff --git a/my_deeploco/rsl_rl/algorithms/__init__.py b/my_deeploco/rsl_rl/algorithms/__init__.py
deleted file mode 100644
index 297b259..0000000
--- a/my_deeploco/rsl_rl/algorithms/__init__.py
+++ /dev/null
@@ -1,11 +0,0 @@
-# Copyright (c) 2021-2025, ETH Zurich and NVIDIA CORPORATION
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-"""Implementation of different RL agents."""
-
-from .distillation import Distillation
-from .ppo import PPO
-
-__all__ = ["PPO", "Distillation"]
\ No newline at end of file
diff --git a/my_deeploco/rsl_rl/algorithms/__pycache__/__init__.cpython-310.pyc b/my_deeploco/rsl_rl/algorithms/__pycache__/__init__.cpython-310.pyc
deleted file mode 100644
index ad2ecab..0000000
Binary files a/my_deeploco/rsl_rl/algorithms/__pycache__/__init__.cpython-310.pyc and /dev/null differ
diff --git a/my_deeploco/rsl_rl/algorithms/__pycache__/distillation.cpython-310.pyc b/my_deeploco/rsl_rl/algorithms/__pycache__/distillation.cpython-310.pyc
deleted file mode 100644
index 557e579..0000000
Binary files a/my_deeploco/rsl_rl/algorithms/__pycache__/distillation.cpython-310.pyc and /dev/null differ
diff --git a/my_deeploco/rsl_rl/algorithms/__pycache__/ppo.cpython-310.pyc b/my_deeploco/rsl_rl/algorithms/__pycache__/ppo.cpython-310.pyc
deleted file mode 100644
index 57e440d..0000000
Binary files a/my_deeploco/rsl_rl/algorithms/__pycache__/ppo.cpython-310.pyc and /dev/null differ
diff --git a/my_deeploco/rsl_rl/algorithms/distillation.py b/my_deeploco/rsl_rl/algorithms/distillation.py
deleted file mode 100644
index 347c03c..0000000
--- a/my_deeploco/rsl_rl/algorithms/distillation.py
+++ /dev/null
@@ -1,180 +0,0 @@
-# Copyright (c) 2021-2025, ETH Zurich and NVIDIA CORPORATION
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-# torch
-import torch
-import torch.nn as nn
-import torch.optim as optim
-
-# rsl-rl
-from rsl_rl.modules import StudentTeacher, StudentTeacherRecurrent
-from rsl_rl.storage import RolloutStorage
-
-
-class Distillation:
-    """Distillation algorithm for training a student model to mimic a teacher model."""
-
-    policy: StudentTeacher | StudentTeacherRecurrent
-    """The student teacher model."""
-
-    def __init__(
-        self,
-        policy,
-        num_learning_epochs=1,
-        gradient_length=15,
-        learning_rate=1e-3,
-        loss_type="mse",
-        device="cpu",
-        # Distributed training parameters
-        multi_gpu_cfg: dict | None = None,
-    ):
-        # device-related parameters
-        self.device = device
-        self.is_multi_gpu = multi_gpu_cfg is not None
-        # Multi-GPU parameters
-        if multi_gpu_cfg is not None:
-            self.gpu_global_rank = multi_gpu_cfg["global_rank"]
-            self.gpu_world_size = multi_gpu_cfg["world_size"]
-        else:
-            self.gpu_global_rank = 0
-            self.gpu_world_size = 1
-
-        self.rnd = None  # TODO: remove when runner has a proper base class
-
-        # distillation components
-        self.policy = policy
-        self.policy.to(self.device)
-        self.storage = None  # initialized later
-        self.optimizer = optim.Adam(self.policy.student.parameters(), lr=learning_rate)
-        self.transition = RolloutStorage.Transition()
-        self.last_hidden_states = None
-
-        # distillation parameters
-        self.num_learning_epochs = num_learning_epochs
-        self.gradient_length = gradient_length
-        self.learning_rate = learning_rate
-
-        # initialize the loss function
-        if loss_type == "mse":
-            self.loss_fn = nn.functional.mse_loss
-        elif loss_type == "huber":
-            self.loss_fn = nn.functional.huber_loss
-        else:
-            raise ValueError(f"Unknown loss type: {loss_type}. Supported types are: mse, huber")
-
-        self.num_updates = 0
-
-    def init_storage(
-        self, training_type, num_envs, num_transitions_per_env, student_obs_shape, teacher_obs_shape, actions_shape
-    ):
-        # create rollout storage
-        self.storage = RolloutStorage(
-            training_type,
-            num_envs,
-            num_transitions_per_env,
-            student_obs_shape,
-            teacher_obs_shape,
-            actions_shape,
-            None,
-            self.device,
-        )
-
-    def act(self, obs, teacher_obs):
-        # compute the actions
-        self.transition.actions = self.policy.act(obs).detach()
-        self.transition.privileged_actions = self.policy.evaluate(teacher_obs).detach()
-        # record the observations
-        self.transition.observations = obs
-        self.transition.privileged_observations = teacher_obs
-        return self.transition.actions
-
-    def process_env_step(self, rewards, dones, infos):
-        # record the rewards and dones
-        self.transition.rewards = rewards
-        self.transition.dones = dones
-        # record the transition
-        self.storage.add_transitions(self.transition)
-        self.transition.clear()
-        self.policy.reset(dones)
-
-    def update(self):
-        self.num_updates += 1
-        mean_behavior_loss = 0
-        loss = 0
-        cnt = 0
-
-        for epoch in range(self.num_learning_epochs):
-            self.policy.reset(hidden_states=self.last_hidden_states)
-            self.policy.detach_hidden_states()
-            for obs, _, _, privileged_actions, dones in self.storage.generator():
-
-                # inference the student for gradient computation
-                actions = self.policy.act_inference(obs)
-
-                # behavior cloning loss
-                behavior_loss = self.loss_fn(actions, privileged_actions)
-
-                # total loss
-                loss = loss + behavior_loss
-                mean_behavior_loss += behavior_loss.item()
-                cnt += 1
-
-                # gradient step
-                if cnt % self.gradient_length == 0:
-                    self.optimizer.zero_grad()
-                    loss.backward()
-                    if self.is_multi_gpu:
-                        self.reduce_parameters()
-                    self.optimizer.step()
-                    self.policy.detach_hidden_states()
-                    loss = 0
-
-                # reset dones
-                self.policy.reset(dones.view(-1))
-                self.policy.detach_hidden_states(dones.view(-1))
-
-        mean_behavior_loss /= cnt
-        self.storage.clear()
-        self.last_hidden_states = self.policy.get_hidden_states()
-        self.policy.detach_hidden_states()
-
-        # construct the loss dictionary
-        loss_dict = {"behavior": mean_behavior_loss}
-
-        return loss_dict
-
-    """
-    Helper functions
-    """
-
-    def broadcast_parameters(self):
-        """Broadcast model parameters to all GPUs."""
-        # obtain the model parameters on current GPU
-        model_params = [self.policy.state_dict()]
-        # broadcast the model parameters
-        torch.distributed.broadcast_object_list(model_params, src=0)
-        # load the model parameters on all GPUs from source GPU
-        self.policy.load_state_dict(model_params[0])
-
-    def reduce_parameters(self):
-        """Collect gradients from all GPUs and average them.
-
-        This function is called after the backward pass to synchronize the gradients across all GPUs.
-        """
-        # Create a tensor to store the gradients
-        grads = [param.grad.view(-1) for param in self.policy.parameters() if param.grad is not None]
-        all_grads = torch.cat(grads)
-        # Average the gradients across all GPUs
-        torch.distributed.all_reduce(all_grads, op=torch.distributed.ReduceOp.SUM)
-        all_grads /= self.gpu_world_size
-        # Update the gradients for all parameters with the reduced gradients
-        offset = 0
-        for param in self.policy.parameters():
-            if param.grad is not None:
-                numel = param.numel()
-                # copy data back from shared buffer
-                param.grad.data.copy_(all_grads[offset : offset + numel].view_as(param.grad.data))
-                # update the offset for the next parameter
-                offset += numel
\ No newline at end of file
diff --git a/my_deeploco/rsl_rl/algorithms/ppo.py b/my_deeploco/rsl_rl/algorithms/ppo.py
deleted file mode 100644
index 38efc1f..0000000
--- a/my_deeploco/rsl_rl/algorithms/ppo.py
+++ /dev/null
@@ -1,474 +0,0 @@
-# Copyright (c) 2021-2025, ETH Zurich and NVIDIA CORPORATION
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-from __future__ import annotations
-
-import torch
-import torch.nn as nn
-import torch.optim as optim
-from itertools import chain
-
-from rsl_rl.modules import ActorCritic
-from rsl_rl.modules.rnd import RandomNetworkDistillation
-from rsl_rl.storage import RolloutStorage
-from rsl_rl.utils import string_to_callable
-
-
-class PPO:
-    """Proximal Policy Optimization algorithm (https://arxiv.org/abs/1707.06347)."""
-
-    policy: ActorCritic
-    """The actor critic module."""
-
-    def __init__(
-        self,
-        policy,
-        num_learning_epochs=1,
-        num_mini_batches=1,
-        clip_param=0.2,
-        gamma=0.998,
-        lam=0.95,
-        value_loss_coef=1.0,
-        entropy_coef=0.0,
-        learning_rate=1e-3,
-        max_grad_norm=1.0,
-        use_clipped_value_loss=True,
-        schedule="fixed",
-        desired_kl=0.01,
-        device="cpu",
-        normalize_advantage_per_mini_batch=False,
-        # RND parameters
-        rnd_cfg: dict | None = None,
-        # Symmetry parameters
-        symmetry_cfg: dict | None = None,
-        # Distributed training parameters
-        multi_gpu_cfg: dict | None = None,
-    ):
-        # device-related parameters
-        self.device = device
-        self.is_multi_gpu = multi_gpu_cfg is not None
-        # Multi-GPU parameters
-        if multi_gpu_cfg is not None:
-            self.gpu_global_rank = multi_gpu_cfg["global_rank"]
-            self.gpu_world_size = multi_gpu_cfg["world_size"]
-        else:
-            self.gpu_global_rank = 0
-            self.gpu_world_size = 1
-
-        # RND components
-        if rnd_cfg is not None:
-            # Create RND module
-            self.rnd = RandomNetworkDistillation(device=self.device, **rnd_cfg)
-            # Create RND optimizer
-            params = self.rnd.predictor.parameters()
-            self.rnd_optimizer = optim.Adam(params, lr=rnd_cfg.get("learning_rate", 1e-3))
-        else:
-            self.rnd = None
-            self.rnd_optimizer = None
-
-        # Symmetry components
-        if symmetry_cfg is not None:
-            # Check if symmetry is enabled
-            use_symmetry = symmetry_cfg["use_data_augmentation"] or symmetry_cfg["use_mirror_loss"]
-            # Print that we are not using symmetry
-            if not use_symmetry:
-                print("Symmetry not used for learning. We will use it for logging instead.")
-            # If function is a string then resolve it to a function
-            if isinstance(symmetry_cfg["data_augmentation_func"], str):
-                symmetry_cfg["data_augmentation_func"] = string_to_callable(symmetry_cfg["data_augmentation_func"])
-            # Check valid configuration
-            if symmetry_cfg["use_data_augmentation"] and not callable(symmetry_cfg["data_augmentation_func"]):
-                raise ValueError(
-                    "Data augmentation enabled but the function is not callable:"
-                    f" {symmetry_cfg['data_augmentation_func']}"
-                )
-            # Store symmetry configuration
-            self.symmetry = symmetry_cfg
-        else:
-            self.symmetry = None
-
-        # PPO components
-        self.policy = policy
-        self.policy.to(self.device)
-        # Create optimizer
-        self.optimizer = optim.Adam(self.policy.parameters(), lr=learning_rate)
-        # Create rollout storage
-        self.storage: RolloutStorage = None  # type: ignore
-        self.transition = RolloutStorage.Transition()
-
-        # PPO parameters
-        self.clip_param = clip_param
-        self.num_learning_epochs = num_learning_epochs
-        self.num_mini_batches = num_mini_batches
-        self.value_loss_coef = value_loss_coef
-        self.entropy_coef = entropy_coef
-        self.gamma = gamma
-        self.lam = lam
-        self.max_grad_norm = max_grad_norm
-        self.use_clipped_value_loss = use_clipped_value_loss
-        self.desired_kl = desired_kl
-        self.schedule = schedule
-        self.learning_rate = learning_rate
-        self.normalize_advantage_per_mini_batch = normalize_advantage_per_mini_batch
-
-    def init_storage(
-        self, training_type, num_envs, num_transitions_per_env, actor_obs_shape, critic_obs_shape, actions_shape
-    ):
-        # create memory for RND as well :)
-        if self.rnd:
-            rnd_state_shape = [self.rnd.num_states]
-        else:
-            rnd_state_shape = None
-        # create rollout storage
-        self.storage = RolloutStorage(
-            training_type,
-            num_envs,
-            num_transitions_per_env,
-            actor_obs_shape,
-            critic_obs_shape,
-            actions_shape,
-            rnd_state_shape,
-            self.device,
-        )
-
-    def act(self, obs, critic_obs):
-        if self.policy.is_recurrent:
-            self.transition.hidden_states = self.policy.get_hidden_states()
-        # compute the actions and values
-        self.transition.actions = self.policy.act(obs).detach()
-        self.transition.values = self.policy.evaluate(critic_obs).detach()
-        self.transition.actions_log_prob = self.policy.get_actions_log_prob(self.transition.actions).detach()
-        self.transition.action_mean = self.policy.action_mean.detach()
-        self.transition.action_sigma = self.policy.action_std.detach()
-        # need to record obs and critic_obs before env.step()
-        self.transition.observations = obs
-        self.transition.privileged_observations = critic_obs
-        return self.transition.actions
-
-    def process_env_step(self, rewards, dones, infos):
-        # Record the rewards and dones
-        # Note: we clone here because later on we bootstrap the rewards based on timeouts
-        self.transition.rewards = rewards.clone()
-        self.transition.dones = dones
-
-        # Compute the intrinsic rewards and add to extrinsic rewards
-        if self.rnd:
-            # Obtain curiosity gates / observations from infos
-            rnd_state = infos["observations"]["rnd_state"]
-            # Compute the intrinsic rewards
-            # note: rnd_state is the gated_state after normalization if normalization is used
-            self.intrinsic_rewards, rnd_state = self.rnd.get_intrinsic_reward(rnd_state)
-            # Add intrinsic rewards to extrinsic rewards
-            self.transition.rewards += self.intrinsic_rewards
-            # Record the curiosity gates
-            self.transition.rnd_state = rnd_state.clone()
-
-        # Bootstrapping on time outs
-        if "time_outs" in infos:
-            self.transition.rewards += self.gamma * torch.squeeze(
-                self.transition.values * infos["time_outs"].unsqueeze(1).to(self.device), 1
-            )
-
-        # record the transition
-        self.storage.add_transitions(self.transition)
-        self.transition.clear()
-        self.policy.reset(dones)
-
-    def compute_returns(self, last_critic_obs):
-        # compute value for the last step
-        last_values = self.policy.evaluate(last_critic_obs).detach()
-        self.storage.compute_returns(
-            last_values, self.gamma, self.lam, normalize_advantage=not self.normalize_advantage_per_mini_batch
-        )
-
-    def update(self):  # noqa: C901
-        mean_value_loss = 0
-        mean_surrogate_loss = 0
-        mean_entropy = 0
-        # -- RND loss
-        if self.rnd:
-            mean_rnd_loss = 0
-        else:
-            mean_rnd_loss = None
-        # -- Symmetry loss
-        if self.symmetry:
-            mean_symmetry_loss = 0
-        else:
-            mean_symmetry_loss = None
-
-        # generator for mini batches
-        if self.policy.is_recurrent:
-            generator = self.storage.recurrent_mini_batch_generator(self.num_mini_batches, self.num_learning_epochs)
-        else:
-            generator = self.storage.mini_batch_generator(self.num_mini_batches, self.num_learning_epochs)
-
-        # iterate over batches
-        for (
-            obs_batch,
-            critic_obs_batch,
-            actions_batch,
-            target_values_batch,
-            advantages_batch,
-            returns_batch,
-            old_actions_log_prob_batch,
-            old_mu_batch,
-            old_sigma_batch,
-            hid_states_batch,
-            masks_batch,
-            rnd_state_batch,
-        ) in generator:
-
-            # number of augmentations per sample
-            # we start with 1 and increase it if we use symmetry augmentation
-            num_aug = 1
-            # original batch size
-            original_batch_size = obs_batch.shape[0]
-
-            # check if we should normalize advantages per mini batch
-            if self.normalize_advantage_per_mini_batch:
-                with torch.no_grad():
-                    advantages_batch = (advantages_batch - advantages_batch.mean()) / (advantages_batch.std() + 1e-8)
-
-            # Perform symmetric augmentation
-            if self.symmetry and self.symmetry["use_data_augmentation"]:
-                # augmentation using symmetry
-                data_augmentation_func = self.symmetry["data_augmentation_func"]
-                # returned shape: [batch_size * num_aug, ...]
-                obs_batch, actions_batch = data_augmentation_func(
-                    obs=obs_batch, actions=actions_batch, env=self.symmetry["_env"], obs_type="policy"
-                )
-                critic_obs_batch, _ = data_augmentation_func(
-                    obs=critic_obs_batch, actions=None, env=self.symmetry["_env"], obs_type="critic"
-                )
-                # compute number of augmentations per sample
-                num_aug = int(obs_batch.shape[0] / original_batch_size)
-                # repeat the rest of the batch
-                # -- actor
-                old_actions_log_prob_batch = old_actions_log_prob_batch.repeat(num_aug, 1)
-                # -- critic
-                target_values_batch = target_values_batch.repeat(num_aug, 1)
-                advantages_batch = advantages_batch.repeat(num_aug, 1)
-                returns_batch = returns_batch.repeat(num_aug, 1)
-
-            # Recompute actions log prob and entropy for current batch of transitions
-            # Note: we need to do this because we updated the policy with the new parameters
-            # -- actor
-            self.policy.act(obs_batch, masks=masks_batch, hidden_states=hid_states_batch[0])
-            actions_log_prob_batch = self.policy.get_actions_log_prob(actions_batch)
-            # -- critic
-            value_batch = self.policy.evaluate(critic_obs_batch, masks=masks_batch, hidden_states=hid_states_batch[1])
-            # -- entropy
-            # we only keep the entropy of the first augmentation (the original one)
-            mu_batch = self.policy.action_mean[:original_batch_size]
-            sigma_batch = self.policy.action_std[:original_batch_size]
-            entropy_batch = self.policy.entropy[:original_batch_size]
-
-            # KL
-            if self.desired_kl is not None and self.schedule == "adaptive":
-                with torch.inference_mode():
-                    kl = torch.sum(
-                        torch.log(sigma_batch / old_sigma_batch + 1.0e-5)
-                        + (torch.square(old_sigma_batch) + torch.square(old_mu_batch - mu_batch))
-                        / (2.0 * torch.square(sigma_batch))
-                        - 0.5,
-                        axis=-1,
-                    )
-                    kl_mean = torch.mean(kl)
-
-                    # Reduce the KL divergence across all GPUs
-                    if self.is_multi_gpu:
-                        torch.distributed.all_reduce(kl_mean, op=torch.distributed.ReduceOp.SUM)
-                        kl_mean /= self.gpu_world_size
-
-                    # Update the learning rate
-                    # Perform this adaptation only on the main process
-                    # TODO: Is this needed? If KL-divergence is the "same" across all GPUs,
-                    #       then the learning rate should be the same across all GPUs.
-                    if self.gpu_global_rank == 0:
-                        if kl_mean > self.desired_kl * 2.0:
-                            self.learning_rate = max(1e-5, self.learning_rate / 1.5)
-                        elif kl_mean < self.desired_kl / 2.0 and kl_mean > 0.0:
-                            self.learning_rate = min(1e-2, self.learning_rate * 1.5)
-
-                    # Update the learning rate for all GPUs
-                    if self.is_multi_gpu:
-                        lr_tensor = torch.tensor(self.learning_rate, device=self.device)
-                        torch.distributed.broadcast(lr_tensor, src=0)
-                        self.learning_rate = lr_tensor.item()
-
-                    # Update the learning rate for all parameter groups
-                    for param_group in self.optimizer.param_groups:
-                        param_group["lr"] = self.learning_rate
-
-            # Surrogate loss
-            ratio = torch.exp(actions_log_prob_batch - torch.squeeze(old_actions_log_prob_batch))
-            surrogate = -torch.squeeze(advantages_batch) * ratio
-            surrogate_clipped = -torch.squeeze(advantages_batch) * torch.clamp(
-                ratio, 1.0 - self.clip_param, 1.0 + self.clip_param
-            )
-            surrogate_loss = torch.max(surrogate, surrogate_clipped).mean()
-
-            # Value function loss
-            if self.use_clipped_value_loss:
-                value_clipped = target_values_batch + (value_batch - target_values_batch).clamp(
-                    -self.clip_param, self.clip_param
-                )
-                value_losses = (value_batch - returns_batch).pow(2)
-                value_losses_clipped = (value_clipped - returns_batch).pow(2)
-                value_loss = torch.max(value_losses, value_losses_clipped).mean()
-            else:
-                value_loss = (returns_batch - value_batch).pow(2).mean()
-
-            loss = surrogate_loss + self.value_loss_coef * value_loss - self.entropy_coef * entropy_batch.mean()
-
-            # Symmetry loss
-            if self.symmetry:
-                # obtain the symmetric actions
-                # if we did augmentation before then we don't need to augment again
-                if not self.symmetry["use_data_augmentation"]:
-                    data_augmentation_func = self.symmetry["data_augmentation_func"]
-                    obs_batch, _ = data_augmentation_func(
-                        obs=obs_batch, actions=None, env=self.symmetry["_env"], obs_type="policy"
-                    )
-                    # compute number of augmentations per sample
-                    num_aug = int(obs_batch.shape[0] / original_batch_size)
-
-                # actions predicted by the actor for symmetrically-augmented observations
-                mean_actions_batch = self.policy.act_inference(obs_batch.detach().clone())
-
-                # compute the symmetrically augmented actions
-                # note: we are assuming the first augmentation is the original one.
-                #   We do not use the action_batch from earlier since that action was sampled from the distribution.
-                #   However, the symmetry loss is computed using the mean of the distribution.
-                action_mean_orig = mean_actions_batch[:original_batch_size]
-                _, actions_mean_symm_batch = data_augmentation_func(
-                    obs=None, actions=action_mean_orig, env=self.symmetry["_env"], obs_type="policy"
-                )
-
-                # compute the loss (we skip the first augmentation as it is the original one)
-                mse_loss = torch.nn.MSELoss()
-                symmetry_loss = mse_loss(
-                    mean_actions_batch[original_batch_size:], actions_mean_symm_batch.detach()[original_batch_size:]
-                )
-                # add the loss to the total loss
-                if self.symmetry["use_mirror_loss"]:
-                    loss += self.symmetry["mirror_loss_coeff"] * symmetry_loss
-                else:
-                    symmetry_loss = symmetry_loss.detach()
-
-            # Random Network Distillation loss
-            if self.rnd:
-                # predict the embedding and the target
-                predicted_embedding = self.rnd.predictor(rnd_state_batch)
-                target_embedding = self.rnd.target(rnd_state_batch).detach()
-                # compute the loss as the mean squared error
-                mseloss = torch.nn.MSELoss()
-                rnd_loss = mseloss(predicted_embedding, target_embedding)
-
-            # Compute the gradients
-            # -- For PPO
-            self.optimizer.zero_grad()
-            loss.backward()
-            # -- For RND
-            if self.rnd:
-                self.rnd_optimizer.zero_grad()  # type: ignore
-                rnd_loss.backward()
-
-            # Collect gradients from all GPUs
-            if self.is_multi_gpu:
-                self.reduce_parameters()
-
-            # Apply the gradients
-            # -- For PPO
-            nn.utils.clip_grad_norm_(self.policy.parameters(), self.max_grad_norm)
-            self.optimizer.step()
-            # -- For RND
-            if self.rnd_optimizer:
-                self.rnd_optimizer.step()
-
-            # Store the losses
-            mean_value_loss += value_loss.item()
-            mean_surrogate_loss += surrogate_loss.item()
-            mean_entropy += entropy_batch.mean().item()
-            # -- RND loss
-            if mean_rnd_loss is not None:
-                mean_rnd_loss += rnd_loss.item()
-            # -- Symmetry loss
-            if mean_symmetry_loss is not None:
-                mean_symmetry_loss += symmetry_loss.item()
-
-        # -- For PPO
-        num_updates = self.num_learning_epochs * self.num_mini_batches
-        mean_value_loss /= num_updates
-        mean_surrogate_loss /= num_updates
-        mean_entropy /= num_updates
-        # -- For RND
-        if mean_rnd_loss is not None:
-            mean_rnd_loss /= num_updates
-        # -- For Symmetry
-        if mean_symmetry_loss is not None:
-            mean_symmetry_loss /= num_updates
-        # -- Clear the storage
-        self.storage.clear()
-
-        # construct the loss dictionary
-        loss_dict = {
-            "value_function": mean_value_loss,
-            "surrogate": mean_surrogate_loss,
-            "entropy": mean_entropy,
-        }
-        if self.rnd:
-            loss_dict["rnd"] = mean_rnd_loss
-        if self.symmetry:
-            loss_dict["symmetry"] = mean_symmetry_loss
-
-        return loss_dict
-
-    """
-    Helper functions
-    """
-
-    def broadcast_parameters(self):
-        """Broadcast model parameters to all GPUs."""
-        # obtain the model parameters on current GPU
-        model_params = [self.policy.state_dict()]
-        if self.rnd:
-            model_params.append(self.rnd.predictor.state_dict())
-        # broadcast the model parameters
-        torch.distributed.broadcast_object_list(model_params, src=0)
-        # load the model parameters on all GPUs from source GPU
-        self.policy.load_state_dict(model_params[0])
-        if self.rnd:
-            self.rnd.predictor.load_state_dict(model_params[1])
-
-    def reduce_parameters(self):
-        """Collect gradients from all GPUs and average them.
-
-        This function is called after the backward pass to synchronize the gradients across all GPUs.
-        """
-        # Create a tensor to store the gradients
-        grads = [param.grad.view(-1) for param in self.policy.parameters() if param.grad is not None]
-        if self.rnd:
-            grads += [param.grad.view(-1) for param in self.rnd.parameters() if param.grad is not None]
-        all_grads = torch.cat(grads)
-
-        # Average the gradients across all GPUs
-        torch.distributed.all_reduce(all_grads, op=torch.distributed.ReduceOp.SUM)
-        all_grads /= self.gpu_world_size
-
-        # Get all parameters
-        all_params = self.policy.parameters()
-        if self.rnd:
-            all_params = chain(all_params, self.rnd.parameters())
-
-        # Update the gradients for all parameters with the reduced gradients
-        offset = 0
-        for param in all_params:
-            if param.grad is not None:
-                numel = param.numel()
-                # copy data back from shared buffer
-                param.grad.data.copy_(all_grads[offset : offset + numel].view_as(param.grad.data))
-                # update the offset for the next parameter
-                offset += numel
\ No newline at end of file
diff --git a/my_deeploco/rsl_rl/env/__init__.py b/my_deeploco/rsl_rl/env/__init__.py
deleted file mode 100644
index ab7c056..0000000
--- a/my_deeploco/rsl_rl/env/__init__.py
+++ /dev/null
@@ -1,10 +0,0 @@
-# Copyright (c) 2021-2025, ETH Zurich and NVIDIA CORPORATION
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-"""Submodule defining the environment definitions."""
-
-from .vec_env import VecEnv
-
-__all__ = ["VecEnv"]
diff --git a/my_deeploco/rsl_rl/env/__pycache__/__init__.cpython-310.pyc b/my_deeploco/rsl_rl/env/__pycache__/__init__.cpython-310.pyc
deleted file mode 100644
index 5510bcb..0000000
Binary files a/my_deeploco/rsl_rl/env/__pycache__/__init__.cpython-310.pyc and /dev/null differ
diff --git a/my_deeploco/rsl_rl/env/__pycache__/vec_env.cpython-310.pyc b/my_deeploco/rsl_rl/env/__pycache__/vec_env.cpython-310.pyc
deleted file mode 100644
index a61f0ae..0000000
Binary files a/my_deeploco/rsl_rl/env/__pycache__/vec_env.cpython-310.pyc and /dev/null differ
diff --git a/my_deeploco/rsl_rl/env/vec_env.py b/my_deeploco/rsl_rl/env/vec_env.py
deleted file mode 100644
index 1e6b8ab..0000000
--- a/my_deeploco/rsl_rl/env/vec_env.py
+++ /dev/null
@@ -1,101 +0,0 @@
-# Copyright (c) 2021-2025, ETH Zurich and NVIDIA CORPORATION
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-from __future__ import annotations
-
-import torch
-from abc import ABC, abstractmethod
-
-
-class VecEnv(ABC):
-    """Abstract class for vectorized environment.
-
-    The vectorized environment is a collection of environments that are synchronized. This means that
-    the same action is applied to all environments and the same observation is returned from all environments.
-
-    All extra observations must be provided as a dictionary to "extras" in the step() method. Based on the
-    configuration, the extra observations are used for different purposes. The following keys are used by the
-    environment:
-
-    - "observations" (dict[str, dict[str, torch.Tensor]]):
-        Additional observations that are not used by the actor networks. The keys are the names of the observations
-        and the values are the observations themselves. The following are reserved keys for the observations:
-
-        - "critic": The observation is used as input to the critic network. Useful for asymmetric observation spaces.
-        - "rnd_state": The observation is used as input to the RND network. Useful for random network distillation.
-
-    - "time_outs" (torch.Tensor): Timeouts for the environments. These correspond to terminations that happen due to time limits and
-      not due to the environment reaching a terminal state. This is useful for environments that have a fixed
-      episode length.
-
-    - "log" (dict[str, float | torch.Tensor]): Additional information for logging and debugging purposes.
-      The key should be a string and start with "/" for namespacing. The value can be a scalar or a tensor.
-      If it is a tensor, the mean of the tensor is used for logging.
-
-      .. deprecated:: 2.0.0
-
-        Use "log" in the extra information dictionary instead of the "episode" key.
-
-    """
-
-    num_envs: int
-    """Number of environments."""
-
-    num_actions: int
-    """Number of actions."""
-
-    max_episode_length: int | torch.Tensor
-    """Maximum episode length.
-
-    The maximum episode length can be a scalar or a tensor. If it is a scalar, it is the same for all environments.
-    If it is a tensor, it is the maximum episode length for each environment. This is useful for dynamic episode
-    lengths.
-    """
-
-    episode_length_buf: torch.Tensor
-    """Buffer for current episode lengths."""
-
-    device: torch.device
-    """Device to use."""
-
-    cfg: dict | object
-    """Configuration object."""
-
-    """
-    Operations.
-    """
-
-    @abstractmethod
-    def get_observations(self) -> tuple[torch.Tensor, dict]:
-        """Return the current observations.
-
-        Returns:
-            Tuple containing the observations and extras.
-        """
-        raise NotImplementedError
-
-    @abstractmethod
-    def reset(self) -> tuple[torch.Tensor, dict]:
-        """Reset all environment instances.
-
-        Returns:
-            Tuple containing the observations and extras.
-        """
-        raise NotImplementedError
-
-    @abstractmethod
-    def step(self, actions: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor, dict]:
-        """Apply input action on the environment.
-
-        The extra information is a dictionary. It includes metrics such as the episode reward, episode length,
-        etc. Additional information can be stored in the dictionary such as observations for the critic network, etc.
-
-        Args:
-            actions: Input actions to apply. Shape: (num_envs, num_actions)
-
-        Returns:
-            A tuple containing the observations, rewards, dones and extra information (metrics).
-        """
-        raise NotImplementedError
\ No newline at end of file
diff --git a/my_deeploco/rsl_rl/modules/__init__.py b/my_deeploco/rsl_rl/modules/__init__.py
deleted file mode 100644
index 81a8d99..0000000
--- a/my_deeploco/rsl_rl/modules/__init__.py
+++ /dev/null
@@ -1,22 +0,0 @@
-# Copyright (c) 2021-2025, ETH Zurich and NVIDIA CORPORATION
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-"""Definitions for neural-network components for RL-agents."""
-
-from .actor_critic import ActorCritic
-from .actor_critic_recurrent import ActorCriticRecurrent
-from .normalizer import EmpiricalNormalization
-from .rnd import RandomNetworkDistillation
-from .student_teacher import StudentTeacher
-from .student_teacher_recurrent import StudentTeacherRecurrent
-
-__all__ = [
-    "ActorCritic",
-    "ActorCriticRecurrent",
-    "EmpiricalNormalization",
-    "RandomNetworkDistillation",
-    "StudentTeacher",
-    "StudentTeacherRecurrent",
-]
\ No newline at end of file
diff --git a/my_deeploco/rsl_rl/modules/__pycache__/__init__.cpython-310.pyc b/my_deeploco/rsl_rl/modules/__pycache__/__init__.cpython-310.pyc
deleted file mode 100644
index 72e7437..0000000
Binary files a/my_deeploco/rsl_rl/modules/__pycache__/__init__.cpython-310.pyc and /dev/null differ
diff --git a/my_deeploco/rsl_rl/modules/__pycache__/actor_critic.cpython-310.pyc b/my_deeploco/rsl_rl/modules/__pycache__/actor_critic.cpython-310.pyc
deleted file mode 100644
index d4e6f0c..0000000
Binary files a/my_deeploco/rsl_rl/modules/__pycache__/actor_critic.cpython-310.pyc and /dev/null differ
diff --git a/my_deeploco/rsl_rl/modules/__pycache__/actor_critic_recurrent.cpython-310.pyc b/my_deeploco/rsl_rl/modules/__pycache__/actor_critic_recurrent.cpython-310.pyc
deleted file mode 100644
index bc514d8..0000000
Binary files a/my_deeploco/rsl_rl/modules/__pycache__/actor_critic_recurrent.cpython-310.pyc and /dev/null differ
diff --git a/my_deeploco/rsl_rl/modules/__pycache__/normalizer.cpython-310.pyc b/my_deeploco/rsl_rl/modules/__pycache__/normalizer.cpython-310.pyc
deleted file mode 100644
index 1d84015..0000000
Binary files a/my_deeploco/rsl_rl/modules/__pycache__/normalizer.cpython-310.pyc and /dev/null differ
diff --git a/my_deeploco/rsl_rl/modules/__pycache__/rnd.cpython-310.pyc b/my_deeploco/rsl_rl/modules/__pycache__/rnd.cpython-310.pyc
deleted file mode 100644
index ba555d1..0000000
Binary files a/my_deeploco/rsl_rl/modules/__pycache__/rnd.cpython-310.pyc and /dev/null differ
diff --git a/my_deeploco/rsl_rl/modules/__pycache__/student_teacher.cpython-310.pyc b/my_deeploco/rsl_rl/modules/__pycache__/student_teacher.cpython-310.pyc
deleted file mode 100644
index f1e3eb0..0000000
Binary files a/my_deeploco/rsl_rl/modules/__pycache__/student_teacher.cpython-310.pyc and /dev/null differ
diff --git a/my_deeploco/rsl_rl/modules/__pycache__/student_teacher_recurrent.cpython-310.pyc b/my_deeploco/rsl_rl/modules/__pycache__/student_teacher_recurrent.cpython-310.pyc
deleted file mode 100644
index 8f580a7..0000000
Binary files a/my_deeploco/rsl_rl/modules/__pycache__/student_teacher_recurrent.cpython-310.pyc and /dev/null differ
diff --git a/my_deeploco/rsl_rl/modules/actor_critic.py b/my_deeploco/rsl_rl/modules/actor_critic.py
deleted file mode 100644
index a1b90cf..0000000
--- a/my_deeploco/rsl_rl/modules/actor_critic.py
+++ /dev/null
@@ -1,149 +0,0 @@
-# Copyright (c) 2021-2025, ETH Zurich and NVIDIA CORPORATION
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-from __future__ import annotations
-
-import torch
-import torch.nn as nn
-from torch.distributions import Normal
-
-from rsl_rl.utils import resolve_nn_activation
-
-
-class ActorCritic(nn.Module):
-    is_recurrent = False
-
-    def __init__(
-        self,
-        num_actor_obs,
-        num_critic_obs,
-        num_actions,
-        actor_hidden_dims=[256, 256, 256],
-        critic_hidden_dims=[256, 256, 256],
-        activation="elu",
-        init_noise_std=1.0,
-        noise_std_type: str = "scalar",
-        **kwargs,
-    ):
-        if kwargs:
-            print(
-                "ActorCritic.__init__ got unexpected arguments, which will be ignored: "
-                + str([key for key in kwargs.keys()])
-            )
-        super().__init__()
-        activation = resolve_nn_activation(activation)
-
-        mlp_input_dim_a = num_actor_obs
-        mlp_input_dim_c = num_critic_obs
-        # Policy
-        actor_layers = []
-        actor_layers.append(nn.Linear(mlp_input_dim_a, actor_hidden_dims[0]))
-        actor_layers.append(activation)
-        for layer_index in range(len(actor_hidden_dims)):
-            if layer_index == len(actor_hidden_dims) - 1:
-                actor_layers.append(nn.Linear(actor_hidden_dims[layer_index], num_actions))
-            else:
-                actor_layers.append(nn.Linear(actor_hidden_dims[layer_index], actor_hidden_dims[layer_index + 1]))
-                actor_layers.append(activation)
-        self.actor = nn.Sequential(*actor_layers)
-
-        # Value function
-        critic_layers = []
-        critic_layers.append(nn.Linear(mlp_input_dim_c, critic_hidden_dims[0]))
-        critic_layers.append(activation)
-        for layer_index in range(len(critic_hidden_dims)):
-            if layer_index == len(critic_hidden_dims) - 1:
-                critic_layers.append(nn.Linear(critic_hidden_dims[layer_index], 1))
-            else:
-                critic_layers.append(nn.Linear(critic_hidden_dims[layer_index], critic_hidden_dims[layer_index + 1]))
-                critic_layers.append(activation)
-        self.critic = nn.Sequential(*critic_layers)
-
-        print(f"Actor MLP: {self.actor}")
-        print(f"Critic MLP: {self.critic}")
-
-        # Action noise
-        self.noise_std_type = noise_std_type
-        if self.noise_std_type == "scalar":
-            self.std = nn.Parameter(init_noise_std * torch.ones(num_actions))
-        elif self.noise_std_type == "log":
-            self.log_std = nn.Parameter(torch.log(init_noise_std * torch.ones(num_actions)))
-        else:
-            raise ValueError(f"Unknown standard deviation type: {self.noise_std_type}. Should be 'scalar' or 'log'")
-
-        # Action distribution (populated in update_distribution)
-        self.distribution = None
-        # disable args validation for speedup
-        Normal.set_default_validate_args(False)
-
-    @staticmethod
-    # not used at the moment
-    def init_weights(sequential, scales):
-        [
-            torch.nn.init.orthogonal_(module.weight, gain=scales[idx])
-            for idx, module in enumerate(mod for mod in sequential if isinstance(mod, nn.Linear))
-        ]
-
-    def reset(self, dones=None):
-        pass
-
-    def forward(self):
-        raise NotImplementedError
-
-    @property
-    def action_mean(self):
-        return self.distribution.mean
-
-    @property
-    def action_std(self):
-        return self.distribution.stddev
-
-    @property
-    def entropy(self):
-        return self.distribution.entropy().sum(dim=-1)
-
-    def update_distribution(self, observations):
-        # compute mean
-        mean = self.actor(observations)
-        # compute standard deviation
-        if self.noise_std_type == "scalar":
-            std = self.std.expand_as(mean)
-        elif self.noise_std_type == "log":
-            std = torch.exp(self.log_std).expand_as(mean)
-        else:
-            raise ValueError(f"Unknown standard deviation type: {self.noise_std_type}. Should be 'scalar' or 'log'")
-        # create distribution
-        self.distribution = Normal(mean, std)
-
-    def act(self, observations, **kwargs):
-        self.update_distribution(observations)
-        return self.distribution.sample()
-
-    def get_actions_log_prob(self, actions):
-        return self.distribution.log_prob(actions).sum(dim=-1)
-
-    def act_inference(self, observations):
-        actions_mean = self.actor(observations)
-        return actions_mean
-
-    def evaluate(self, critic_observations, **kwargs):
-        value = self.critic(critic_observations)
-        return value
-
-    def load_state_dict(self, state_dict, strict=True):
-        """Load the parameters of the actor-critic model.
-
-        Args:
-            state_dict (dict): State dictionary of the model.
-            strict (bool): Whether to strictly enforce that the keys in state_dict match the keys returned by this
-                           module's state_dict() function.
-
-        Returns:
-            bool: Whether this training resumes a previous training. This flag is used by the `load()` function of
-                  `OnPolicyRunner` to determine how to load further parameters (relevant for, e.g., distillation).
-        """
-
-        super().load_state_dict(state_dict, strict=strict)
-        return True
\ No newline at end of file
diff --git a/my_deeploco/rsl_rl/modules/actor_critic_recurrent.py b/my_deeploco/rsl_rl/modules/actor_critic_recurrent.py
deleted file mode 100644
index c229b9f..0000000
--- a/my_deeploco/rsl_rl/modules/actor_critic_recurrent.py
+++ /dev/null
@@ -1,80 +0,0 @@
-# Copyright (c) 2021-2025, ETH Zurich and NVIDIA CORPORATION
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-from __future__ import annotations
-
-import warnings
-
-from rsl_rl.modules import ActorCritic
-from rsl_rl.networks import Memory
-from rsl_rl.utils import resolve_nn_activation
-
-
-class ActorCriticRecurrent(ActorCritic):
-    is_recurrent = True
-
-    def __init__(
-        self,
-        num_actor_obs,
-        num_critic_obs,
-        num_actions,
-        actor_hidden_dims=[256, 256, 256],
-        critic_hidden_dims=[256, 256, 256],
-        activation="elu",
-        rnn_type="lstm",
-        rnn_hidden_dim=256,
-        rnn_num_layers=1,
-        init_noise_std=1.0,
-        **kwargs,
-    ):
-        if "rnn_hidden_size" in kwargs:
-            warnings.warn(
-                "The argument `rnn_hidden_size` is deprecated and will be removed in a future version. "
-                "Please use `rnn_hidden_dim` instead.",
-                DeprecationWarning,
-            )
-            if rnn_hidden_dim == 256:  # Only override if the new argument is at its default
-                rnn_hidden_dim = kwargs.pop("rnn_hidden_size")
-        if kwargs:
-            print(
-                "ActorCriticRecurrent.__init__ got unexpected arguments, which will be ignored: " + str(kwargs.keys()),
-            )
-
-        super().__init__(
-            num_actor_obs=rnn_hidden_dim,
-            num_critic_obs=rnn_hidden_dim,
-            num_actions=num_actions,
-            actor_hidden_dims=actor_hidden_dims,
-            critic_hidden_dims=critic_hidden_dims,
-            activation=activation,
-            init_noise_std=init_noise_std,
-        )
-
-        activation = resolve_nn_activation(activation)
-
-        self.memory_a = Memory(num_actor_obs, type=rnn_type, num_layers=rnn_num_layers, hidden_size=rnn_hidden_dim)
-        self.memory_c = Memory(num_critic_obs, type=rnn_type, num_layers=rnn_num_layers, hidden_size=rnn_hidden_dim)
-
-        print(f"Actor RNN: {self.memory_a}")
-        print(f"Critic RNN: {self.memory_c}")
-
-    def reset(self, dones=None):
-        self.memory_a.reset(dones)
-        self.memory_c.reset(dones)
-
-    def act(self, observations, masks=None, hidden_states=None):
-        input_a = self.memory_a(observations, masks, hidden_states)
-        return super().act(input_a.squeeze(0))
-
-    def act_inference(self, observations):
-        input_a = self.memory_a(observations)
-        return super().act_inference(input_a.squeeze(0))
-
-    def evaluate(self, critic_observations, masks=None, hidden_states=None):
-        input_c = self.memory_c(critic_observations, masks, hidden_states)
-        return super().evaluate(input_c.squeeze(0))
-
-    def get_hidden_states(self):
-        return self.memory_a.hidden_states, self.memory_c.hidden_states
\ No newline at end of file
diff --git a/my_deeploco/rsl_rl/modules/normalizer.py b/my_deeploco/rsl_rl/modules/normalizer.py
deleted file mode 100644
index 4172d93..0000000
--- a/my_deeploco/rsl_rl/modules/normalizer.py
+++ /dev/null
@@ -1,129 +0,0 @@
-# Copyright (c) 2021-2025, ETH Zurich and NVIDIA CORPORATION
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-#  Copyright (c) 2020 Preferred Networks, Inc.
-
-from __future__ import annotations
-
-import torch
-from torch import nn
-
-
-class EmpiricalNormalization(nn.Module):
-    """Normalize mean and variance of values based on empirical values."""
-
-    def __init__(self, shape, eps=1e-2, until=None):
-        """Initialize EmpiricalNormalization module.
-
-        Args:
-            shape (int or tuple of int): Shape of input values except batch axis.
-            eps (float): Small value for stability.
-            until (int or None): If this arg is specified, the link learns input values until the sum of batch sizes
-            exceeds it.
-        """
-        super().__init__()
-        self.eps = eps
-        self.until = until
-        self.register_buffer("_mean", torch.zeros(shape).unsqueeze(0))
-        self.register_buffer("_var", torch.ones(shape).unsqueeze(0))
-        self.register_buffer("_std", torch.ones(shape).unsqueeze(0))
-        self.register_buffer("count", torch.tensor(0, dtype=torch.long))
-
-    @property
-    def mean(self):
-        return self._mean.squeeze(0).clone()
-
-    @property
-    def std(self):
-        return self._std.squeeze(0).clone()
-
-    def forward(self, x):
-        """Normalize mean and variance of values based on empirical values.
-
-        Args:
-            x (ndarray or Variable): Input values
-
-        Returns:
-            ndarray or Variable: Normalized output values
-        """
-
-        if self.training:
-            self.update(x)
-        return (x - self._mean) / (self._std + self.eps)
-
-    @torch.jit.unused
-    def update(self, x):
-        """Learn input values without computing the output values of them"""
-
-        if self.until is not None and self.count >= self.until:
-            return
-
-        count_x = x.shape[0]
-        self.count += count_x
-        rate = count_x / self.count
-
-        var_x = torch.var(x, dim=0, unbiased=False, keepdim=True)
-        mean_x = torch.mean(x, dim=0, keepdim=True)
-        delta_mean = mean_x - self._mean
-        self._mean += rate * delta_mean
-        self._var += rate * (var_x - self._var + delta_mean * (mean_x - self._mean))
-        self._std = torch.sqrt(self._var)
-
-    @torch.jit.unused
-    def inverse(self, y):
-        return y * (self._std + self.eps) + self._mean
-
-
-class EmpiricalDiscountedVariationNormalization(nn.Module):
-    """Reward normalization from Pathak's large scale study on PPO.
-
-    Reward normalization. Since the reward function is non-stationary, it is useful to normalize
-    the scale of the rewards so that the value function can learn quickly. We did this by dividing
-    the rewards by a running estimate of the standard deviation of the sum of discounted rewards.
-    """
-
-    def __init__(self, shape, eps=1e-2, gamma=0.99, until=None):
-        super().__init__()
-
-        self.emp_norm = EmpiricalNormalization(shape, eps, until)
-        self.disc_avg = DiscountedAverage(gamma)
-
-    def forward(self, rew):
-        if self.training:
-            # update discounected rewards
-            avg = self.disc_avg.update(rew)
-
-            # update moments from discounted rewards
-            self.emp_norm.update(avg)
-
-        if self.emp_norm._std > 0:
-            return rew / self.emp_norm._std
-        else:
-            return rew
-
-
-class DiscountedAverage:
-    r"""Discounted average of rewards.
-
-    The discounted average is defined as:
-
-    .. math::
-
-        \bar{R}_t = \gamma \bar{R}_{t-1} + r_t
-
-    Args:
-        gamma (float): Discount factor.
-    """
-
-    def __init__(self, gamma):
-        self.avg = None
-        self.gamma = gamma
-
-    def update(self, rew: torch.Tensor) -> torch.Tensor:
-        if self.avg is None:
-            self.avg = rew
-        else:
-            self.avg = self.avg * self.gamma + rew
-        return self.avg
\ No newline at end of file
diff --git a/my_deeploco/rsl_rl/modules/rnd.py b/my_deeploco/rsl_rl/modules/rnd.py
deleted file mode 100644
index 3049d2c..0000000
--- a/my_deeploco/rsl_rl/modules/rnd.py
+++ /dev/null
@@ -1,196 +0,0 @@
-# Copyright (c) 2021-2025, ETH Zurich and NVIDIA CORPORATION
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-from __future__ import annotations
-
-import torch
-import torch.nn as nn
-
-from rsl_rl.modules.normalizer import EmpiricalDiscountedVariationNormalization, EmpiricalNormalization
-from rsl_rl.utils import resolve_nn_activation
-
-
-class RandomNetworkDistillation(nn.Module):
-    """Implementation of Random Network Distillation (RND) [1]
-
-    References:
-        .. [1] Burda, Yuri, et al. "Exploration by random network distillation." arXiv preprint arXiv:1810.12894 (2018).
-    """
-
-    def __init__(
-        self,
-        num_states: int,
-        num_outputs: int,
-        predictor_hidden_dims: list[int],
-        target_hidden_dims: list[int],
-        activation: str = "elu",
-        weight: float = 0.0,
-        state_normalization: bool = False,
-        reward_normalization: bool = False,
-        device: str = "cpu",
-        weight_schedule: dict | None = None,
-    ):
-        """Initialize the RND module.
-
-        - If :attr:`state_normalization` is True, then the input state is normalized using an Empirical Normalization layer.
-        - If :attr:`reward_normalization` is True, then the intrinsic reward is normalized using an Empirical Discounted
-          Variation Normalization layer.
-
-        .. note::
-            If the hidden dimensions are -1 in the predictor and target networks configuration, then the number of states
-            is used as the hidden dimension.
-
-        Args:
-            num_states: Number of states/inputs to the predictor and target networks.
-            num_outputs: Number of outputs (embedding size) of the predictor and target networks.
-            predictor_hidden_dims: List of hidden dimensions of the predictor network.
-            target_hidden_dims: List of hidden dimensions of the target network.
-            activation: Activation function. Defaults to "elu".
-            weight: Scaling factor of the intrinsic reward. Defaults to 0.0.
-            state_normalization: Whether to normalize the input state. Defaults to False.
-            reward_normalization: Whether to normalize the intrinsic reward. Defaults to False.
-            device: Device to use. Defaults to "cpu".
-            weight_schedule: The type of schedule to use for the RND weight parameter.
-                Defaults to None, in which case the weight parameter is constant.
-                It is a dictionary with the following keys:
-
-                - "mode": The type of schedule to use for the RND weight parameter.
-                    - "constant": Constant weight schedule.
-                    - "step": Step weight schedule.
-                    - "linear": Linear weight schedule.
-
-                For the "step" weight schedule, the following parameters are required:
-
-                - "final_step": The step at which the weight parameter is set to the final value.
-                - "final_value": The final value of the weight parameter.
-
-                For the "linear" weight schedule, the following parameters are required:
-                - "initial_step": The step at which the weight parameter is set to the initial value.
-                - "final_step": The step at which the weight parameter is set to the final value.
-                - "final_value": The final value of the weight parameter.
-        """
-        # initialize parent class
-        super().__init__()
-
-        # Store parameters
-        self.num_states = num_states
-        self.num_outputs = num_outputs
-        self.initial_weight = weight
-        self.device = device
-        self.state_normalization = state_normalization
-        self.reward_normalization = reward_normalization
-
-        # Normalization of input gates
-        if state_normalization:
-            self.state_normalizer = EmpiricalNormalization(shape=[self.num_states], until=1.0e8).to(self.device)
-        else:
-            self.state_normalizer = torch.nn.Identity()
-        # Normalization of intrinsic reward
-        if reward_normalization:
-            self.reward_normalizer = EmpiricalDiscountedVariationNormalization(shape=[], until=1.0e8).to(self.device)
-        else:
-            self.reward_normalizer = torch.nn.Identity()
-
-        # counter for the number of updates
-        self.update_counter = 0
-
-        # resolve weight schedule
-        if weight_schedule is not None:
-            self.weight_scheduler_params = weight_schedule
-            self.weight_scheduler = getattr(self, f"_{weight_schedule['mode']}_weight_schedule")
-        else:
-            self.weight_scheduler = None
-        # Create network architecture
-        self.predictor = self._build_mlp(num_states, predictor_hidden_dims, num_outputs, activation).to(self.device)
-        self.target = self._build_mlp(num_states, target_hidden_dims, num_outputs, activation).to(self.device)
-
-        # make target network not trainable
-        self.target.eval()
-
-    def get_intrinsic_reward(self, rnd_state) -> tuple[torch.Tensor, torch.Tensor]:
-        # note: the counter is updated number of env steps per learning iteration
-        self.update_counter += 1
-        # Normalize rnd state
-        rnd_state = self.state_normalizer(rnd_state)
-        # Obtain the embedding of the rnd state from the target and predictor networks
-        target_embedding = self.target(rnd_state).detach()
-        predictor_embedding = self.predictor(rnd_state).detach()
-        # Compute the intrinsic reward as the distance between the embeddings
-        intrinsic_reward = torch.linalg.norm(target_embedding - predictor_embedding, dim=1)
-        # Normalize intrinsic reward
-        intrinsic_reward = self.reward_normalizer(intrinsic_reward)
-
-        # Check the weight schedule
-        if self.weight_scheduler is not None:
-            self.weight = self.weight_scheduler(step=self.update_counter, **self.weight_scheduler_params)
-        else:
-            self.weight = self.initial_weight
-        # Scale intrinsic reward
-        intrinsic_reward *= self.weight
-
-        return intrinsic_reward, rnd_state
-
-    def forward(self, *args, **kwargs):
-        raise RuntimeError("Forward method is not implemented. Use get_intrinsic_reward instead.")
-
-    def train(self, mode: bool = True):
-        # sets module into training mode
-        self.predictor.train(mode)
-        if self.state_normalization:
-            self.state_normalizer.train(mode)
-        if self.reward_normalization:
-            self.reward_normalizer.train(mode)
-        return self
-
-    def eval(self):
-        return self.train(False)
-
-    """
-    Private Methods
-    """
-
-    @staticmethod
-    def _build_mlp(input_dims: int, hidden_dims: list[int], output_dims: int, activation_name: str = "elu"):
-        """Builds target and predictor networks"""
-
-        network_layers = []
-        # resolve hidden dimensions
-        # if dims is -1 then we use the number of observations
-        hidden_dims = [input_dims if dim == -1 else dim for dim in hidden_dims]
-        # resolve activation function
-        activation = resolve_nn_activation(activation_name)
-        # first layer
-        network_layers.append(nn.Linear(input_dims, hidden_dims[0]))
-        network_layers.append(activation)
-        # subsequent layers
-        for layer_index in range(len(hidden_dims)):
-            if layer_index == len(hidden_dims) - 1:
-                # last layer
-                network_layers.append(nn.Linear(hidden_dims[layer_index], output_dims))
-            else:
-                # hidden layers
-                network_layers.append(nn.Linear(hidden_dims[layer_index], hidden_dims[layer_index + 1]))
-                network_layers.append(activation)
-        return nn.Sequential(*network_layers)
-
-    """
-    Different weight schedules.
-    """
-
-    def _constant_weight_schedule(self, step: int, **kwargs):
-        return self.initial_weight
-
-    def _step_weight_schedule(self, step: int, final_step: int, final_value: float, **kwargs):
-        return self.initial_weight if step < final_step else final_value
-
-    def _linear_weight_schedule(self, step: int, initial_step: int, final_step: int, final_value: float, **kwargs):
-        if step < initial_step:
-            return self.initial_weight
-        elif step > final_step:
-            return final_value
-        else:
-            return self.initial_weight + (final_value - self.initial_weight) * (step - initial_step) / (
-                final_step - initial_step
-            )
\ No newline at end of file
diff --git a/my_deeploco/rsl_rl/modules/student_teacher.py b/my_deeploco/rsl_rl/modules/student_teacher.py
deleted file mode 100644
index 829193b..0000000
--- a/my_deeploco/rsl_rl/modules/student_teacher.py
+++ /dev/null
@@ -1,152 +0,0 @@
-# Copyright (c) 2021-2025, ETH Zurich and NVIDIA CORPORATION
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-from __future__ import annotations
-
-import torch
-import torch.nn as nn
-from torch.distributions import Normal
-
-from rsl_rl.utils import resolve_nn_activation
-
-
-class StudentTeacher(nn.Module):
-    is_recurrent = False
-
-    def __init__(
-        self,
-        num_student_obs,
-        num_teacher_obs,
-        num_actions,
-        student_hidden_dims=[256, 256, 256],
-        teacher_hidden_dims=[256, 256, 256],
-        activation="elu",
-        init_noise_std=0.1,
-        **kwargs,
-    ):
-        if kwargs:
-            print(
-                "StudentTeacher.__init__ got unexpected arguments, which will be ignored: "
-                + str([key for key in kwargs.keys()])
-            )
-        super().__init__()
-        activation = resolve_nn_activation(activation)
-        self.loaded_teacher = False  # indicates if teacher has been loaded
-
-        mlp_input_dim_s = num_student_obs
-        mlp_input_dim_t = num_teacher_obs
-
-        # student
-        student_layers = []
-        student_layers.append(nn.Linear(mlp_input_dim_s, student_hidden_dims[0]))
-        student_layers.append(activation)
-        for layer_index in range(len(student_hidden_dims)):
-            if layer_index == len(student_hidden_dims) - 1:
-                student_layers.append(nn.Linear(student_hidden_dims[layer_index], num_actions))
-            else:
-                student_layers.append(nn.Linear(student_hidden_dims[layer_index], student_hidden_dims[layer_index + 1]))
-                student_layers.append(activation)
-        self.student = nn.Sequential(*student_layers)
-
-        # teacher
-        teacher_layers = []
-        teacher_layers.append(nn.Linear(mlp_input_dim_t, teacher_hidden_dims[0]))
-        teacher_layers.append(activation)
-        for layer_index in range(len(teacher_hidden_dims)):
-            if layer_index == len(teacher_hidden_dims) - 1:
-                teacher_layers.append(nn.Linear(teacher_hidden_dims[layer_index], num_actions))
-            else:
-                teacher_layers.append(nn.Linear(teacher_hidden_dims[layer_index], teacher_hidden_dims[layer_index + 1]))
-                teacher_layers.append(activation)
-        self.teacher = nn.Sequential(*teacher_layers)
-        self.teacher.eval()
-
-        print(f"Student MLP: {self.student}")
-        print(f"Teacher MLP: {self.teacher}")
-
-        # action noise
-        self.std = nn.Parameter(init_noise_std * torch.ones(num_actions))
-        self.distribution = None
-        # disable args validation for speedup
-        Normal.set_default_validate_args = False
-
-    def reset(self, dones=None, hidden_states=None):
-        pass
-
-    def forward(self):
-        raise NotImplementedError
-
-    @property
-    def action_mean(self):
-        return self.distribution.mean
-
-    @property
-    def action_std(self):
-        return self.distribution.stddev
-
-    @property
-    def entropy(self):
-        return self.distribution.entropy().sum(dim=-1)
-
-    def update_distribution(self, observations):
-        mean = self.student(observations)
-        std = self.std.expand_as(mean)
-        self.distribution = Normal(mean, std)
-
-    def act(self, observations):
-        self.update_distribution(observations)
-        return self.distribution.sample()
-
-    def act_inference(self, observations):
-        actions_mean = self.student(observations)
-        return actions_mean
-
-    def evaluate(self, teacher_observations):
-        with torch.no_grad():
-            actions = self.teacher(teacher_observations)
-        return actions
-
-    def load_state_dict(self, state_dict, strict=True):
-        """Load the parameters of the student and teacher networks.
-
-        Args:
-            state_dict (dict): State dictionary of the model.
-            strict (bool): Whether to strictly enforce that the keys in state_dict match the keys returned by this
-                           module's state_dict() function.
-
-        Returns:
-            bool: Whether this training resumes a previous training. This flag is used by the `load()` function of
-                  `OnPolicyRunner` to determine how to load further parameters.
-        """
-
-        # check if state_dict contains teacher and student or just teacher parameters
-        if any("actor" in key for key in state_dict.keys()):  # loading parameters from rl training
-            # rename keys to match teacher and remove critic parameters
-            teacher_state_dict = {}
-            for key, value in state_dict.items():
-                if "actor." in key:
-                    teacher_state_dict[key.replace("actor.", "")] = value
-            self.teacher.load_state_dict(teacher_state_dict, strict=strict)
-            # also load recurrent memory if teacher is recurrent
-            if self.is_recurrent and self.teacher_recurrent:
-                raise NotImplementedError("Loading recurrent memory for the teacher is not implemented yet")  # TODO
-            # set flag for successfully loading the parameters
-            self.loaded_teacher = True
-            self.teacher.eval()
-            return False
-        elif any("student" in key for key in state_dict.keys()):  # loading parameters from distillation training
-            super().load_state_dict(state_dict, strict=strict)
-            # set flag for successfully loading the parameters
-            self.loaded_teacher = True
-            self.teacher.eval()
-            return True
-        else:
-            raise ValueError("state_dict does not contain student or teacher parameters")
-
-    def get_hidden_states(self):
-        return None
-
-    def detach_hidden_states(self, dones=None):
-        pass
\ No newline at end of file
diff --git a/my_deeploco/rsl_rl/modules/student_teacher_recurrent.py b/my_deeploco/rsl_rl/modules/student_teacher_recurrent.py
deleted file mode 100644
index 9fc7077..0000000
--- a/my_deeploco/rsl_rl/modules/student_teacher_recurrent.py
+++ /dev/null
@@ -1,100 +0,0 @@
-# Copyright (c) 2021-2025, ETH Zurich and NVIDIA CORPORATION
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-from __future__ import annotations
-
-import warnings
-
-from rsl_rl.modules import StudentTeacher
-from rsl_rl.networks import Memory
-from rsl_rl.utils import resolve_nn_activation
-
-
-class StudentTeacherRecurrent(StudentTeacher):
-    is_recurrent = True
-
-    def __init__(
-        self,
-        num_student_obs,
-        num_teacher_obs,
-        num_actions,
-        student_hidden_dims=[256, 256, 256],
-        teacher_hidden_dims=[256, 256, 256],
-        activation="elu",
-        rnn_type="lstm",
-        rnn_hidden_dim=256,
-        rnn_num_layers=1,
-        init_noise_std=0.1,
-        teacher_recurrent=False,
-        **kwargs,
-    ):
-        if "rnn_hidden_size" in kwargs:
-            warnings.warn(
-                "The argument `rnn_hidden_size` is deprecated and will be removed in a future version. "
-                "Please use `rnn_hidden_dim` instead.",
-                DeprecationWarning,
-            )
-            if rnn_hidden_dim == 256:  # Only override if the new argument is at its default
-                rnn_hidden_dim = kwargs.pop("rnn_hidden_size")
-        if kwargs:
-            print(
-                "StudentTeacherRecurrent.__init__ got unexpected arguments, which will be ignored: "
-                + str(kwargs.keys()),
-            )
-
-        self.teacher_recurrent = teacher_recurrent
-
-        super().__init__(
-            num_student_obs=rnn_hidden_dim,
-            num_teacher_obs=rnn_hidden_dim if teacher_recurrent else num_teacher_obs,
-            num_actions=num_actions,
-            student_hidden_dims=student_hidden_dims,
-            teacher_hidden_dims=teacher_hidden_dims,
-            activation=activation,
-            init_noise_std=init_noise_std,
-        )
-
-        activation = resolve_nn_activation(activation)
-
-        self.memory_s = Memory(num_student_obs, type=rnn_type, num_layers=rnn_num_layers, hidden_size=rnn_hidden_dim)
-        if self.teacher_recurrent:
-            self.memory_t = Memory(
-                num_teacher_obs, type=rnn_type, num_layers=rnn_num_layers, hidden_size=rnn_hidden_dim
-            )
-
-        print(f"Student RNN: {self.memory_s}")
-        if self.teacher_recurrent:
-            print(f"Teacher RNN: {self.memory_t}")
-
-    def reset(self, dones=None, hidden_states=None):
-        if hidden_states is None:
-            hidden_states = (None, None)
-        self.memory_s.reset(dones, hidden_states[0])
-        if self.teacher_recurrent:
-            self.memory_t.reset(dones, hidden_states[1])
-
-    def act(self, observations):
-        input_s = self.memory_s(observations)
-        return super().act(input_s.squeeze(0))
-
-    def act_inference(self, observations):
-        input_s = self.memory_s(observations)
-        return super().act_inference(input_s.squeeze(0))
-
-    def evaluate(self, teacher_observations):
-        if self.teacher_recurrent:
-            teacher_observations = self.memory_t(teacher_observations)
-        return super().evaluate(teacher_observations.squeeze(0))
-
-    def get_hidden_states(self):
-        if self.teacher_recurrent:
-            return self.memory_s.hidden_states, self.memory_t.hidden_states
-        else:
-            return self.memory_s.hidden_states, None
-
-    def detach_hidden_states(self, dones=None):
-        self.memory_s.detach_hidden_states(dones)
-        if self.teacher_recurrent:
-            self.memory_t.detach_hidden_states(dones)
\ No newline at end of file
diff --git a/my_deeploco/rsl_rl/networks/__init__.py b/my_deeploco/rsl_rl/networks/__init__.py
deleted file mode 100644
index 9862208..0000000
--- a/my_deeploco/rsl_rl/networks/__init__.py
+++ /dev/null
@@ -1,10 +0,0 @@
-# Copyright (c) 2021-2025, ETH Zurich and NVIDIA CORPORATION
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-"""Definitions for neural networks."""
-
-from .memory import Memory
-
-__all__ = ["Memory"]
\ No newline at end of file
diff --git a/my_deeploco/rsl_rl/networks/__pycache__/__init__.cpython-310.pyc b/my_deeploco/rsl_rl/networks/__pycache__/__init__.cpython-310.pyc
deleted file mode 100644
index 6bbcd1c..0000000
Binary files a/my_deeploco/rsl_rl/networks/__pycache__/__init__.cpython-310.pyc and /dev/null differ
diff --git a/my_deeploco/rsl_rl/networks/__pycache__/memory.cpython-310.pyc b/my_deeploco/rsl_rl/networks/__pycache__/memory.cpython-310.pyc
deleted file mode 100644
index 2c198b6..0000000
Binary files a/my_deeploco/rsl_rl/networks/__pycache__/memory.cpython-310.pyc and /dev/null differ
diff --git a/my_deeploco/rsl_rl/networks/memory.py b/my_deeploco/rsl_rl/networks/memory.py
deleted file mode 100644
index 30e8b45..0000000
--- a/my_deeploco/rsl_rl/networks/memory.py
+++ /dev/null
@@ -1,65 +0,0 @@
-# Copyright (c) 2021-2025, ETH Zurich and NVIDIA CORPORATION
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-from __future__ import annotations
-
-import torch
-import torch.nn as nn
-
-from rsl_rl.utils import unpad_trajectories
-
-
-class Memory(torch.nn.Module):
-    def __init__(self, input_size, type="lstm", num_layers=1, hidden_size=256):
-        super().__init__()
-        # RNN
-        rnn_cls = nn.GRU if type.lower() == "gru" else nn.LSTM
-        self.rnn = rnn_cls(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers)
-        self.hidden_states = None
-
-    def forward(self, input, masks=None, hidden_states=None):
-        batch_mode = masks is not None
-        if batch_mode:
-            # batch mode: needs saved hidden states
-            if hidden_states is None:
-                raise ValueError("Hidden states not passed to memory module during policy update")
-            out, _ = self.rnn(input, hidden_states)
-            out = unpad_trajectories(out, masks)
-        else:
-            # inference/distillation mode: uses hidden states of last step
-            out, self.hidden_states = self.rnn(input.unsqueeze(0), self.hidden_states)
-        return out
-
-    def reset(self, dones=None, hidden_states=None):
-        if dones is None:  # reset all hidden states
-            if hidden_states is None:
-                self.hidden_states = None
-            else:
-                self.hidden_states = hidden_states
-        elif self.hidden_states is not None:  # reset hidden states of done environments
-            if hidden_states is None:
-                if isinstance(self.hidden_states, tuple):  # tuple in case of LSTM
-                    for hidden_state in self.hidden_states:
-                        hidden_state[..., dones == 1, :] = 0.0
-                else:
-                    self.hidden_states[..., dones == 1, :] = 0.0
-            else:
-                NotImplementedError(
-                    "Resetting hidden states of done environments with custom hidden states is not implemented"
-                )
-
-    def detach_hidden_states(self, dones=None):
-        if self.hidden_states is not None:
-            if dones is None:  # detach all hidden states
-                if isinstance(self.hidden_states, tuple):  # tuple in case of LSTM
-                    self.hidden_states = tuple(hidden_state.detach() for hidden_state in self.hidden_states)
-                else:
-                    self.hidden_states = self.hidden_states.detach()
-            else:  # detach hidden states of done environments
-                if isinstance(self.hidden_states, tuple):  # tuple in case of LSTM
-                    for hidden_state in self.hidden_states:
-                        hidden_state[..., dones == 1, :] = hidden_state[..., dones == 1, :].detach()
-                else:
-                    self.hidden_states[..., dones == 1, :] = self.hidden_states[..., dones == 1, :].detach()
\ No newline at end of file
diff --git a/my_deeploco/rsl_rl/runners/__init__.py b/my_deeploco/rsl_rl/runners/__init__.py
deleted file mode 100644
index e1713b2..0000000
--- a/my_deeploco/rsl_rl/runners/__init__.py
+++ /dev/null
@@ -1,10 +0,0 @@
-# Copyright (c) 2021-2025, ETH Zurich and NVIDIA CORPORATION
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-"""Implementation of runners for environment-agent interaction."""
-
-from .on_policy_runner import OnPolicyRunner
-
-__all__ = ["OnPolicyRunner"]
diff --git a/my_deeploco/rsl_rl/runners/__pycache__/__init__.cpython-310.pyc b/my_deeploco/rsl_rl/runners/__pycache__/__init__.cpython-310.pyc
deleted file mode 100644
index fea1f24..0000000
Binary files a/my_deeploco/rsl_rl/runners/__pycache__/__init__.cpython-310.pyc and /dev/null differ
diff --git a/my_deeploco/rsl_rl/runners/__pycache__/on_policy_runner.cpython-310.pyc b/my_deeploco/rsl_rl/runners/__pycache__/on_policy_runner.cpython-310.pyc
deleted file mode 100644
index a7b8f97..0000000
Binary files a/my_deeploco/rsl_rl/runners/__pycache__/on_policy_runner.cpython-310.pyc and /dev/null differ
diff --git a/my_deeploco/rsl_rl/runners/on_policy_runner.py b/my_deeploco/rsl_rl/runners/on_policy_runner.py
deleted file mode 100644
index 6c55b82..0000000
--- a/my_deeploco/rsl_rl/runners/on_policy_runner.py
+++ /dev/null
@@ -1,528 +0,0 @@
-# Copyright (c) 2021-2025, ETH Zurich and NVIDIA CORPORATION
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-from __future__ import annotations
-
-import os
-import statistics
-import time
-import torch
-from collections import deque
-
-import rsl_rl
-from rsl_rl.algorithms import PPO, Distillation
-from rsl_rl.env import VecEnv
-from rsl_rl.modules import (
-    ActorCritic,
-    ActorCriticRecurrent,
-    EmpiricalNormalization,
-    StudentTeacher,
-    StudentTeacherRecurrent,
-)
-from rsl_rl.utils import store_code_state
-
-
-class OnPolicyRunner:
-    """On-policy runner for training and evaluation."""
-
-    def __init__(self, env: VecEnv, train_cfg: dict, log_dir: str | None = None, device="cpu"):
-        self.cfg = train_cfg
-        self.alg_cfg = train_cfg["algorithm"]
-        self.policy_cfg = train_cfg["policy"]
-        self.device = device
-        self.env = env
-
-        # check if multi-gpu is enabled
-        self._configure_multi_gpu()
-
-        # resolve training type depending on the algorithm
-        if self.alg_cfg["class_name"] == "PPO":
-            self.training_type = "rl"
-        elif self.alg_cfg["class_name"] == "Distillation":
-            self.training_type = "distillation"
-        else:
-            raise ValueError(f"Training type not found for algorithm {self.alg_cfg['class_name']}.")
-
-        # resolve dimensions of observations
-        obs, extras = self.env.get_observations()
-        num_obs = obs.shape[1]
-
-        # resolve type of privileged observations
-        if self.training_type == "rl":
-            if "critic" in extras["observations"]:
-                self.privileged_obs_type = "critic"  # actor-critic reinforcement learnig, e.g., PPO
-            else:
-                self.privileged_obs_type = None
-        if self.training_type == "distillation":
-            if "teacher" in extras["observations"]:
-                self.privileged_obs_type = "teacher"  # policy distillation
-            else:
-                self.privileged_obs_type = None
-
-        # resolve dimensions of privileged observations
-        if self.privileged_obs_type is not None:
-            num_privileged_obs = extras["observations"][self.privileged_obs_type].shape[1]
-        else:
-            num_privileged_obs = num_obs
-
-        # evaluate the policy class
-        policy_class = eval(self.policy_cfg.pop("class_name"))
-        policy: ActorCritic | ActorCriticRecurrent | StudentTeacher | StudentTeacherRecurrent = policy_class(
-            num_obs, num_privileged_obs, self.env.num_actions, **self.policy_cfg
-        ).to(self.device)
-
-        # resolve dimension of rnd gated state
-        if "rnd_cfg" in self.alg_cfg and self.alg_cfg["rnd_cfg"] is not None:
-            # check if rnd gated state is present
-            rnd_state = extras["observations"].get("rnd_state")
-            if rnd_state is None:
-                raise ValueError("Observations for the key 'rnd_state' not found in infos['observations'].")
-            # get dimension of rnd gated state
-            num_rnd_state = rnd_state.shape[1]
-            # add rnd gated state to config
-            self.alg_cfg["rnd_cfg"]["num_states"] = num_rnd_state
-            # scale down the rnd weight with timestep (similar to how rewards are scaled down in legged_gym envs)
-            self.alg_cfg["rnd_cfg"]["weight"] *= env.unwrapped.step_dt
-
-        # if using symmetry then pass the environment config object
-        if "symmetry_cfg" in self.alg_cfg and self.alg_cfg["symmetry_cfg"] is not None:
-            # this is used by the symmetry function for handling different observation terms
-            self.alg_cfg["symmetry_cfg"]["_env"] = env
-
-        # initialize algorithm
-        alg_class = eval(self.alg_cfg.pop("class_name"))
-        self.alg: PPO | Distillation = alg_class(policy, device=self.device, **self.alg_cfg, multi_gpu_cfg=self.multi_gpu_cfg)
-
-        # store training configuration
-        self.num_steps_per_env = self.cfg["num_steps_per_env"]
-        self.save_interval = self.cfg["save_interval"]
-        self.empirical_normalization = self.cfg["empirical_normalization"]
-        if self.empirical_normalization:
-            self.obs_normalizer = EmpiricalNormalization(shape=[num_obs], until=1.0e8).to(self.device)
-            self.privileged_obs_normalizer = EmpiricalNormalization(shape=[num_privileged_obs], until=1.0e8).to(
-                self.device
-            )
-        else:
-            self.obs_normalizer = torch.nn.Identity().to(self.device)  # no normalization
-            self.privileged_obs_normalizer = torch.nn.Identity().to(self.device)  # no normalization
-
-        # init storage and model
-        self.alg.init_storage(
-            self.training_type,
-            self.env.num_envs,
-            self.num_steps_per_env,
-            [num_obs],
-            [num_privileged_obs],
-            [self.env.num_actions],
-        )
-
-        # Decide whether to disable logging
-        # We only log from the process with rank 0 (main process)
-        self.disable_logs = self.is_distributed and self.gpu_global_rank != 0
-        # Logging
-        self.log_dir = log_dir
-        self.writer = None
-        self.tot_timesteps = 0
-        self.tot_time = 0
-        self.current_learning_iteration = 0
-        self.git_status_repos = [rsl_rl.__file__]
-
-    def learn(self, num_learning_iterations: int, init_at_random_ep_len: bool = False):  # noqa: C901
-        # initialize writer
-        if self.log_dir is not None and self.writer is None and not self.disable_logs:
-            # Launch either Tensorboard or Neptune & Tensorboard summary writer(s), default: Tensorboard.
-            self.logger_type = self.cfg.get("logger", "tensorboard")
-            self.logger_type = self.logger_type.lower()
-
-            if self.logger_type == "neptune":
-                from rsl_rl.utils.neptune_utils import NeptuneSummaryWriter
-
-                self.writer = NeptuneSummaryWriter(log_dir=self.log_dir, flush_secs=10, cfg=self.cfg)
-                self.writer.log_config(self.env.cfg, self.cfg, self.alg_cfg, self.policy_cfg)
-            elif self.logger_type == "wandb":
-                from rsl_rl.utils.wandb_utils import WandbSummaryWriter
-
-                self.writer = WandbSummaryWriter(log_dir=self.log_dir, flush_secs=10, cfg=self.cfg)
-                self.writer.log_config(self.env.cfg, self.cfg, self.alg_cfg, self.policy_cfg)
-            elif self.logger_type == "tensorboard":
-                from torch.utils.tensorboard import SummaryWriter
-
-                self.writer = SummaryWriter(log_dir=self.log_dir, flush_secs=10)
-            else:
-                raise ValueError("Logger type not found. Please choose 'neptune', 'wandb' or 'tensorboard'.")
-
-        # check if teacher is loaded
-        if self.training_type == "distillation" and not self.alg.policy.loaded_teacher:
-            raise ValueError("Teacher model parameters not loaded. Please load a teacher model to distill.")
-
-        # randomize initial episode lengths (for exploration)
-        if init_at_random_ep_len:
-            self.env.episode_length_buf = torch.randint_like(
-                self.env.episode_length_buf, high=int(self.env.max_episode_length)
-            )
-
-        # start learning
-        obs, extras = self.env.get_observations()
-        privileged_obs = extras["observations"].get(self.privileged_obs_type, obs)
-        obs, privileged_obs = obs.to(self.device), privileged_obs.to(self.device)
-        self.train_mode()  # switch to train mode (for dropout for example)
-
-        # Book keeping
-        ep_infos = []
-        rewbuffer = deque(maxlen=100)
-        lenbuffer = deque(maxlen=100)
-        cur_reward_sum = torch.zeros(self.env.num_envs, dtype=torch.float, device=self.device)
-        cur_episode_length = torch.zeros(self.env.num_envs, dtype=torch.float, device=self.device)
-
-        # create buffers for logging extrinsic and intrinsic rewards
-        if self.alg.rnd:
-            erewbuffer = deque(maxlen=100)
-            irewbuffer = deque(maxlen=100)
-            cur_ereward_sum = torch.zeros(self.env.num_envs, dtype=torch.float, device=self.device)
-            cur_ireward_sum = torch.zeros(self.env.num_envs, dtype=torch.float, device=self.device)
-
-        # Ensure all parameters are in-synced
-        if self.is_distributed:
-            print(f"Synchronizing parameters for rank {self.gpu_global_rank}...")
-            self.alg.broadcast_parameters()
-            # TODO: Do we need to synchronize empirical normalizers?
-            #   Right now: No, because they all should converge to the same values "asymptotically".
-
-        # Start training
-        start_iter = self.current_learning_iteration
-        tot_iter = start_iter + num_learning_iterations
-        for it in range(start_iter, tot_iter):
-            start = time.time()
-            # Rollout
-            with torch.inference_mode():
-                for _ in range(self.num_steps_per_env):
-                    # Sample actions
-                    actions = self.alg.act(obs, privileged_obs)
-                    # Step the environment
-                    obs, rewards, dones, infos = self.env.step(actions.to(self.env.device))
-                    # Move to device
-                    obs, rewards, dones = (obs.to(self.device), rewards.to(self.device), dones.to(self.device))
-                    # perform normalization
-                    obs = self.obs_normalizer(obs)
-                    if self.privileged_obs_type is not None:
-                        privileged_obs = self.privileged_obs_normalizer(
-                            infos["observations"][self.privileged_obs_type].to(self.device)
-                        )
-                    else:
-                        privileged_obs = obs
-
-                    # process the step
-                    self.alg.process_env_step(rewards, dones, infos)
-
-                    # Extract intrinsic rewards (only for logging)
-                    intrinsic_rewards = self.alg.intrinsic_rewards if self.alg.rnd else None
-
-                    # book keeping
-                    if self.log_dir is not None:
-                        if "episode" in infos:
-                            ep_infos.append(infos["episode"])
-                        elif "log" in infos:
-                            ep_infos.append(infos["log"])
-                        # Update rewards
-                        if self.alg.rnd:
-                            cur_ereward_sum += rewards
-                            cur_ireward_sum += intrinsic_rewards  # type: ignore
-                            cur_reward_sum += rewards + intrinsic_rewards
-                        else:
-                            cur_reward_sum += rewards
-                        # Update episode length
-                        cur_episode_length += 1
-                        # Clear data for completed episodes
-                        # -- common
-                        new_ids = (dones > 0).nonzero(as_tuple=False)
-                        rewbuffer.extend(cur_reward_sum[new_ids][:, 0].cpu().numpy().tolist())
-                        lenbuffer.extend(cur_episode_length[new_ids][:, 0].cpu().numpy().tolist())
-                        cur_reward_sum[new_ids] = 0
-                        cur_episode_length[new_ids] = 0
-                        # -- intrinsic and extrinsic rewards
-                        if self.alg.rnd:
-                            erewbuffer.extend(cur_ereward_sum[new_ids][:, 0].cpu().numpy().tolist())
-                            irewbuffer.extend(cur_ireward_sum[new_ids][:, 0].cpu().numpy().tolist())
-                            cur_ereward_sum[new_ids] = 0
-                            cur_ireward_sum[new_ids] = 0
-
-                stop = time.time()
-                collection_time = stop - start
-                start = stop
-
-                # compute returns
-                if self.training_type == "rl":
-                    self.alg.compute_returns(privileged_obs)
-
-            # update policy
-            loss_dict = self.alg.update()
-
-            stop = time.time()
-            learn_time = stop - start
-            self.current_learning_iteration = it
-            # log info
-            if self.log_dir is not None and not self.disable_logs:
-                # Log information
-                self.log(locals())
-                # Save model
-                if it % self.save_interval == 0:
-                    self.save(os.path.join(self.log_dir, f"model_{it}.pt"))
-
-            # Clear episode infos
-            ep_infos.clear()
-            # Save code state
-            if it == start_iter and not self.disable_logs:
-                # obtain all the diff files
-                git_file_paths = store_code_state(self.log_dir, self.git_status_repos)
-                # if possible store them to wandb
-                if self.logger_type in ["wandb", "neptune"] and git_file_paths:
-                    for path in git_file_paths:
-                        self.writer.save_file(path)
-
-        # Save the final model after training
-        if self.log_dir is not None and not self.disable_logs:
-            self.save(os.path.join(self.log_dir, f"model_{self.current_learning_iteration}.pt"))
-
-    def log(self, locs: dict, width: int = 80, pad: int = 35):
-        # Compute the collection size
-        collection_size = self.num_steps_per_env * self.env.num_envs * self.gpu_world_size
-        # Update total time-steps and time
-        self.tot_timesteps += collection_size
-        self.tot_time += locs["collection_time"] + locs["learn_time"]
-        iteration_time = locs["collection_time"] + locs["learn_time"]
-
-        # -- Episode info
-        ep_string = ""
-        if locs["ep_infos"]:
-            for key in locs["ep_infos"][0]:
-                infotensor = torch.tensor([], device=self.device)
-                for ep_info in locs["ep_infos"]:
-                    # handle scalar and zero dimensional tensor infos
-                    if key not in ep_info:
-                        continue
-                    if not isinstance(ep_info[key], torch.Tensor):
-                        ep_info[key] = torch.Tensor([ep_info[key]])
-                    if len(ep_info[key].shape) == 0:
-                        ep_info[key] = ep_info[key].unsqueeze(0)
-                    infotensor = torch.cat((infotensor, ep_info[key].to(self.device)))
-                value = torch.mean(infotensor)
-                # log to logger and terminal
-                if "/" in key:
-                    self.writer.add_scalar(key, value, locs["it"])
-                    ep_string += f"""{f'{key}:':>{pad}} {value:.4f}\n"""
-                else:
-                    self.writer.add_scalar("Episode/" + key, value, locs["it"])
-                    ep_string += f"""{f'Mean episode {key}:':>{pad}} {value:.4f}\n"""
-
-        mean_std = self.alg.policy.action_std.mean()
-        fps = int(collection_size / (locs["collection_time"] + locs["learn_time"]))
-
-        # -- Losses
-        for key, value in locs["loss_dict"].items():
-            self.writer.add_scalar(f"Loss/{key}", value, locs["it"])
-        self.writer.add_scalar("Loss/learning_rate", self.alg.learning_rate, locs["it"])
-
-        # -- Policy
-        self.writer.add_scalar("Policy/mean_noise_std", mean_std.item(), locs["it"])
-
-        # -- Performance
-        self.writer.add_scalar("Perf/total_fps", fps, locs["it"])
-        self.writer.add_scalar("Perf/collection time", locs["collection_time"], locs["it"])
-        self.writer.add_scalar("Perf/learning_time", locs["learn_time"], locs["it"])
-
-        # -- Training
-        if len(locs["rewbuffer"]) > 0:
-            # separate logging for intrinsic and extrinsic rewards
-            if self.alg.rnd:
-                self.writer.add_scalar("Rnd/mean_extrinsic_reward", statistics.mean(locs["erewbuffer"]), locs["it"])
-                self.writer.add_scalar("Rnd/mean_intrinsic_reward", statistics.mean(locs["irewbuffer"]), locs["it"])
-                self.writer.add_scalar("Rnd/weight", self.alg.rnd.weight, locs["it"])
-            # everything else
-            self.writer.add_scalar("Train/mean_reward", statistics.mean(locs["rewbuffer"]), locs["it"])
-            self.writer.add_scalar("Train/mean_episode_length", statistics.mean(locs["lenbuffer"]), locs["it"])
-            if self.logger_type != "wandb":  # wandb does not support non-integer x-axis logging
-                self.writer.add_scalar("Train/mean_reward/time", statistics.mean(locs["rewbuffer"]), self.tot_time)
-                self.writer.add_scalar(
-                    "Train/mean_episode_length/time", statistics.mean(locs["lenbuffer"]), self.tot_time
-                )
-
-        str = f" \033[1m Learning iteration {locs['it']}/{locs['tot_iter']} \033[0m "
-
-        if len(locs["rewbuffer"]) > 0:
-            log_string = (
-                f"""{'#' * width}\n"""
-                f"""{str.center(width, ' ')}\n\n"""
-                f"""{'Computation:':>{pad}} {fps:.0f} steps/s (collection: {locs[
-                    'collection_time']:.3f}s, learning {locs['learn_time']:.3f}s)\n"""
-                f"""{'Mean action noise std:':>{pad}} {mean_std.item():.2f}\n"""
-            )
-            # -- Losses
-            for key, value in locs["loss_dict"].items():
-                log_string += f"""{f'Mean {key} loss:':>{pad}} {value:.4f}\n"""
-            # -- Rewards
-            if self.alg.rnd:
-                log_string += (
-                    f"""{'Mean extrinsic reward:':>{pad}} {statistics.mean(locs['erewbuffer']):.2f}\n"""
-                    f"""{'Mean intrinsic reward:':>{pad}} {statistics.mean(locs['irewbuffer']):.2f}\n"""
-                )
-            log_string += f"""{'Mean reward:':>{pad}} {statistics.mean(locs['rewbuffer']):.2f}\n"""
-            # -- episode info
-            log_string += f"""{'Mean episode length:':>{pad}} {statistics.mean(locs['lenbuffer']):.2f}\n"""
-        else:
-            log_string = (
-                f"""{'#' * width}\n"""
-                f"""{str.center(width, ' ')}\n\n"""
-                f"""{'Computation:':>{pad}} {fps:.0f} steps/s (collection: {locs[
-                    'collection_time']:.3f}s, learning {locs['learn_time']:.3f}s)\n"""
-                f"""{'Mean action noise std:':>{pad}} {mean_std.item():.2f}\n"""
-            )
-            for key, value in locs["loss_dict"].items():
-                log_string += f"""{f'{key}:':>{pad}} {value:.4f}\n"""
-
-        log_string += ep_string
-        log_string += (
-            f"""{'-' * width}\n"""
-            f"""{'Total timesteps:':>{pad}} {self.tot_timesteps}\n"""
-            f"""{'Iteration time:':>{pad}} {iteration_time:.2f}s\n"""
-            f"""{'Time elapsed:':>{pad}} {time.strftime("%H:%M:%S", time.gmtime(self.tot_time))}\n"""
-            f"""{'ETA:':>{pad}} {time.strftime("%H:%M:%S", time.gmtime(self.tot_time / (locs['it'] - locs['start_iter'] + 1) * (
-                               locs['start_iter'] + locs['num_learning_iterations'] - locs['it'])))}\n"""
-        )
-        print(log_string)
-
-    def save(self, path: str, infos=None):
-        # -- Save model
-        saved_dict = {
-            "model_state_dict": self.alg.policy.state_dict(),
-            "optimizer_state_dict": self.alg.optimizer.state_dict(),
-            "iter": self.current_learning_iteration,
-            "infos": infos,
-        }
-        # -- Save RND model if used
-        if self.alg.rnd:
-            saved_dict["rnd_state_dict"] = self.alg.rnd.state_dict()
-            saved_dict["rnd_optimizer_state_dict"] = self.alg.rnd_optimizer.state_dict()
-        # -- Save observation normalizer if used
-        if self.empirical_normalization:
-            saved_dict["obs_norm_state_dict"] = self.obs_normalizer.state_dict()
-            saved_dict["privileged_obs_norm_state_dict"] = self.privileged_obs_normalizer.state_dict()
-
-        # save model
-        torch.save(saved_dict, path)
-
-        # upload model to external logging service
-        if self.logger_type in ["neptune", "wandb"] and not self.disable_logs:
-            self.writer.save_model(path, self.current_learning_iteration)
-
-    def load(self, path: str, load_optimizer: bool = True):
-        loaded_dict = torch.load(path, weights_only=False)
-        # -- Load model
-        resumed_training = self.alg.policy.load_state_dict(loaded_dict["model_state_dict"])
-        # -- Load RND model if used
-        if self.alg.rnd:
-            self.alg.rnd.load_state_dict(loaded_dict["rnd_state_dict"])
-        # -- Load observation normalizer if used
-        if self.empirical_normalization:
-            if resumed_training:
-                # if a previous training is resumed, the actor/student normalizer is loaded for the actor/student
-                # and the critic/teacher normalizer is loaded for the critic/teacher
-                self.obs_normalizer.load_state_dict(loaded_dict["obs_norm_state_dict"])
-                self.privileged_obs_normalizer.load_state_dict(loaded_dict["privileged_obs_norm_state_dict"])
-            else:
-                # if the training is not resumed but a model is loaded, this run must be distillation training following
-                # an rl training. Thus the actor normalizer is loaded for the teacher model. The student's normalizer
-                # is not loaded, as the observation space could differ from the previous rl training.
-                self.privileged_obs_normalizer.load_state_dict(loaded_dict["obs_norm_state_dict"])
-        # -- load optimizer if used
-        if load_optimizer and resumed_training:
-            # -- algorithm optimizer
-            self.alg.optimizer.load_state_dict(loaded_dict["optimizer_state_dict"])
-            # -- RND optimizer if used
-            if self.alg.rnd:
-                self.alg.rnd_optimizer.load_state_dict(loaded_dict["rnd_optimizer_state_dict"])
-        # -- load current learning iteration
-        if resumed_training:
-            self.current_learning_iteration = loaded_dict["iter"]
-        return loaded_dict["infos"]
-
-    def get_inference_policy(self, device=None):
-        self.eval_mode()  # switch to evaluation mode (dropout for example)
-        if device is not None:
-            self.alg.policy.to(device)
-        policy = self.alg.policy.act_inference
-        if self.cfg["empirical_normalization"]:
-            if device is not None:
-                self.obs_normalizer.to(device)
-            policy = lambda x: self.alg.policy.act_inference(self.obs_normalizer(x))  # noqa: E731
-        return policy
-
-    def train_mode(self):
-        # -- PPO
-        self.alg.policy.train()
-        # -- RND
-        if self.alg.rnd:
-            self.alg.rnd.train()
-        # -- Normalization
-        if self.empirical_normalization:
-            self.obs_normalizer.train()
-            self.privileged_obs_normalizer.train()
-
-    def eval_mode(self):
-        # -- PPO
-        self.alg.policy.eval()
-        # -- RND
-        if self.alg.rnd:
-            self.alg.rnd.eval()
-        # -- Normalization
-        if self.empirical_normalization:
-            self.obs_normalizer.eval()
-            self.privileged_obs_normalizer.eval()
-
-    def add_git_repo_to_log(self, repo_file_path):
-        self.git_status_repos.append(repo_file_path)
-
-    """
-    Helper functions.
-    """
-
-    def _configure_multi_gpu(self):
-        """Configure multi-gpu training."""
-        # check if distributed training is enabled
-        self.gpu_world_size = int(os.getenv("WORLD_SIZE", "1"))
-        self.is_distributed = self.gpu_world_size > 1
-
-        # if not distributed training, set local and global rank to 0 and return
-        if not self.is_distributed:
-            self.gpu_local_rank = 0
-            self.gpu_global_rank = 0
-            self.multi_gpu_cfg = None
-            return
-
-        # get rank and world size
-        self.gpu_local_rank = int(os.getenv("LOCAL_RANK", "0"))
-        self.gpu_global_rank = int(os.getenv("RANK", "0"))
-
-        # make a configuration dictionary
-        self.multi_gpu_cfg = {
-            "global_rank": self.gpu_global_rank,  # rank of the main process
-            "local_rank": self.gpu_local_rank,  # rank of the current process
-            "world_size": self.gpu_world_size,  # total number of processes
-        }
-
-        # check if user has device specified for local rank
-        if self.device != f"cuda:{self.gpu_local_rank}":
-            raise ValueError(f"Device '{self.device}' does not match expected device for local rank '{self.gpu_local_rank}'.")
-        # validate multi-gpu configuration
-        if self.gpu_local_rank >= self.gpu_world_size:
-            raise ValueError(f"Local rank '{self.gpu_local_rank}' is greater than or equal to world size '{self.gpu_world_size}'.")
-        if self.gpu_global_rank >= self.gpu_world_size:
-            raise ValueError(f"Global rank '{self.gpu_global_rank}' is greater than or equal to world size '{self.gpu_world_size}'.")
-
-        # initialize torch distributed
-        torch.distributed.init_process_group(
-            backend="nccl", rank=self.gpu_global_rank, world_size=self.gpu_world_size
-        )
-        # set device to the local rank
-        torch.cuda.set_device(self.gpu_local_rank)
\ No newline at end of file
diff --git a/my_deeploco/rsl_rl/storage/__init__.py b/my_deeploco/rsl_rl/storage/__init__.py
deleted file mode 100644
index 9bb13c9..0000000
--- a/my_deeploco/rsl_rl/storage/__init__.py
+++ /dev/null
@@ -1,10 +0,0 @@
-# Copyright (c) 2021-2025, ETH Zurich and NVIDIA CORPORATION
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-"""Implementation of transitions storage for RL-agent."""
-
-from .rollout_storage import RolloutStorage
-
-__all__ = ["RolloutStorage"]
diff --git a/my_deeploco/rsl_rl/storage/__pycache__/__init__.cpython-310.pyc b/my_deeploco/rsl_rl/storage/__pycache__/__init__.cpython-310.pyc
deleted file mode 100644
index 17a37ca..0000000
Binary files a/my_deeploco/rsl_rl/storage/__pycache__/__init__.cpython-310.pyc and /dev/null differ
diff --git a/my_deeploco/rsl_rl/storage/__pycache__/rollout_storage.cpython-310.pyc b/my_deeploco/rsl_rl/storage/__pycache__/rollout_storage.cpython-310.pyc
deleted file mode 100644
index ccd26c2..0000000
Binary files a/my_deeploco/rsl_rl/storage/__pycache__/rollout_storage.cpython-310.pyc and /dev/null differ
diff --git a/my_deeploco/rsl_rl/storage/rollout_storage.py b/my_deeploco/rsl_rl/storage/rollout_storage.py
deleted file mode 100644
index 3b7ee7d..0000000
--- a/my_deeploco/rsl_rl/storage/rollout_storage.py
+++ /dev/null
@@ -1,316 +0,0 @@
-# Copyright (c) 2021-2025, ETH Zurich and NVIDIA CORPORATION
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-from __future__ import annotations
-
-import torch
-
-from rsl_rl.utils import split_and_pad_trajectories
-
-
-class RolloutStorage:
-    class Transition:
-        def __init__(self):
-            self.observations = None
-            self.privileged_observations = None
-            self.actions = None
-            self.privileged_actions = None
-            self.rewards = None
-            self.dones = None
-            self.values = None
-            self.actions_log_prob = None
-            self.action_mean = None
-            self.action_sigma = None
-            self.hidden_states = None
-            self.rnd_state = None
-
-        def clear(self):
-            self.__init__()
-
-    def __init__(
-        self,
-        training_type,
-        num_envs,
-        num_transitions_per_env,
-        obs_shape,
-        privileged_obs_shape,
-        actions_shape,
-        rnd_state_shape=None,
-        device="cpu",
-    ):
-        # store inputs
-        self.training_type = training_type
-        self.device = device
-        self.num_transitions_per_env = num_transitions_per_env
-        self.num_envs = num_envs
-        self.obs_shape = obs_shape
-        self.privileged_obs_shape = privileged_obs_shape
-        self.rnd_state_shape = rnd_state_shape
-        self.actions_shape = actions_shape
-
-        # Core
-        self.observations = torch.zeros(num_transitions_per_env, num_envs, *obs_shape, device=self.device)
-        if privileged_obs_shape is not None:
-            self.privileged_observations = torch.zeros(
-                num_transitions_per_env, num_envs, *privileged_obs_shape, device=self.device
-            )
-        else:
-            self.privileged_observations = None
-        self.rewards = torch.zeros(num_transitions_per_env, num_envs, 1, device=self.device)
-        self.actions = torch.zeros(num_transitions_per_env, num_envs, *actions_shape, device=self.device)
-        self.dones = torch.zeros(num_transitions_per_env, num_envs, 1, device=self.device).byte()
-
-        # for distillation
-        if training_type == "distillation":
-            self.privileged_actions = torch.zeros(num_transitions_per_env, num_envs, *actions_shape, device=self.device)
-
-        # for reinforcement learning
-        if training_type == "rl":
-            self.values = torch.zeros(num_transitions_per_env, num_envs, 1, device=self.device)
-            self.actions_log_prob = torch.zeros(num_transitions_per_env, num_envs, 1, device=self.device)
-            self.mu = torch.zeros(num_transitions_per_env, num_envs, *actions_shape, device=self.device)
-            self.sigma = torch.zeros(num_transitions_per_env, num_envs, *actions_shape, device=self.device)
-            self.returns = torch.zeros(num_transitions_per_env, num_envs, 1, device=self.device)
-            self.advantages = torch.zeros(num_transitions_per_env, num_envs, 1, device=self.device)
-
-        # For RND
-        if rnd_state_shape is not None:
-            self.rnd_state = torch.zeros(num_transitions_per_env, num_envs, *rnd_state_shape, device=self.device)
-
-        # For RNN networks
-        self.saved_hidden_states_a = None
-        self.saved_hidden_states_c = None
-
-        # counter for the number of transitions stored
-        self.step = 0
-
-    def add_transitions(self, transition: Transition):
-        # check if the transition is valid
-        if self.step >= self.num_transitions_per_env:
-            raise OverflowError("Rollout buffer overflow! You should call clear() before adding new transitions.")
-
-        # Core
-        self.observations[self.step].copy_(transition.observations)
-        if self.privileged_observations is not None:
-            self.privileged_observations[self.step].copy_(transition.privileged_observations)
-        self.actions[self.step].copy_(transition.actions)
-        self.rewards[self.step].copy_(transition.rewards.view(-1, 1))
-        self.dones[self.step].copy_(transition.dones.view(-1, 1))
-
-        # for distillation
-        if self.training_type == "distillation":
-            self.privileged_actions[self.step].copy_(transition.privileged_actions)
-
-        # for reinforcement learning
-        if self.training_type == "rl":
-            self.values[self.step].copy_(transition.values)
-            self.actions_log_prob[self.step].copy_(transition.actions_log_prob.view(-1, 1))
-            self.mu[self.step].copy_(transition.action_mean)
-            self.sigma[self.step].copy_(transition.action_sigma)
-
-        # For RND
-        if self.rnd_state_shape is not None:
-            self.rnd_state[self.step].copy_(transition.rnd_state)
-
-        # For RNN networks
-        self._save_hidden_states(transition.hidden_states)
-
-        # increment the counter
-        self.step += 1
-
-    def _save_hidden_states(self, hidden_states):
-        if hidden_states is None or hidden_states == (None, None):
-            return
-        # make a tuple out of GRU hidden state sto match the LSTM format
-        hid_a = hidden_states[0] if isinstance(hidden_states[0], tuple) else (hidden_states[0],)
-        hid_c = hidden_states[1] if isinstance(hidden_states[1], tuple) else (hidden_states[1],)
-        # initialize if needed
-        if self.saved_hidden_states_a is None:
-            self.saved_hidden_states_a = [
-                torch.zeros(self.observations.shape[0], *hid_a[i].shape, device=self.device) for i in range(len(hid_a))
-            ]
-            self.saved_hidden_states_c = [
-                torch.zeros(self.observations.shape[0], *hid_c[i].shape, device=self.device) for i in range(len(hid_c))
-            ]
-        # copy the states
-        for i in range(len(hid_a)):
-            self.saved_hidden_states_a[i][self.step].copy_(hid_a[i])
-            self.saved_hidden_states_c[i][self.step].copy_(hid_c[i])
-
-    def clear(self):
-        self.step = 0
-
-    def compute_returns(self, last_values, gamma, lam, normalize_advantage: bool = True):
-        advantage = 0
-        for step in reversed(range(self.num_transitions_per_env)):
-            # if we are at the last step, bootstrap the return value
-            if step == self.num_transitions_per_env - 1:
-                next_values = last_values
-            else:
-                next_values = self.values[step + 1]
-            # 1 if we are not in a terminal state, 0 otherwise
-            next_is_not_terminal = 1.0 - self.dones[step].float()
-            # TD error: r_t + gamma * V(s_{t+1}) - V(s_t)
-            delta = self.rewards[step] + next_is_not_terminal * gamma * next_values - self.values[step]
-            # Advantage: A(s_t, a_t) = delta_t + gamma * lambda * A(s_{t+1}, a_{t+1})
-            advantage = delta + next_is_not_terminal * gamma * lam * advantage
-            # Return: R_t = A(s_t, a_t) + V(s_t)
-            self.returns[step] = advantage + self.values[step]
-
-        # Compute the advantages
-        self.advantages = self.returns - self.values
-        # Normalize the advantages if flag is set
-        # This is to prevent double normalization (i.e. if per minibatch normalization is used)
-        if normalize_advantage:
-            self.advantages = (self.advantages - self.advantages.mean()) / (self.advantages.std() + 1e-8)
-
-    # for distillation
-    def generator(self):
-        if self.training_type != "distillation":
-            raise ValueError("This function is only available for distillation training.")
-
-        for i in range(self.num_transitions_per_env):
-            if self.privileged_observations is not None:
-                privileged_observations = self.privileged_observations[i]
-            else:
-                privileged_observations = self.observations[i]
-            yield self.observations[i], privileged_observations, self.actions[i], self.privileged_actions[
-                i
-            ], self.dones[i]
-
-    # for reinforcement learning with feedforward networks
-    def mini_batch_generator(self, num_mini_batches, num_epochs=8):
-        if self.training_type != "rl":
-            raise ValueError("This function is only available for reinforcement learning training.")
-        batch_size = self.num_envs * self.num_transitions_per_env
-        mini_batch_size = batch_size // num_mini_batches
-        indices = torch.randperm(num_mini_batches * mini_batch_size, requires_grad=False, device=self.device)
-
-        # Core
-        observations = self.observations.flatten(0, 1)
-        if self.privileged_observations is not None:
-            privileged_observations = self.privileged_observations.flatten(0, 1)
-        else:
-            privileged_observations = observations
-
-        actions = self.actions.flatten(0, 1)
-        values = self.values.flatten(0, 1)
-        returns = self.returns.flatten(0, 1)
-
-        # For PPO
-        old_actions_log_prob = self.actions_log_prob.flatten(0, 1)
-        advantages = self.advantages.flatten(0, 1)
-        old_mu = self.mu.flatten(0, 1)
-        old_sigma = self.sigma.flatten(0, 1)
-
-        # For RND
-        if self.rnd_state_shape is not None:
-            rnd_state = self.rnd_state.flatten(0, 1)
-
-        for epoch in range(num_epochs):
-            for i in range(num_mini_batches):
-                # Select the indices for the mini-batch
-                start = i * mini_batch_size
-                end = (i + 1) * mini_batch_size
-                batch_idx = indices[start:end]
-
-                # Create the mini-batch
-                # -- Core
-                obs_batch = observations[batch_idx]
-                privileged_observations_batch = privileged_observations[batch_idx]
-                actions_batch = actions[batch_idx]
-
-                # -- For PPO
-                target_values_batch = values[batch_idx]
-                returns_batch = returns[batch_idx]
-                old_actions_log_prob_batch = old_actions_log_prob[batch_idx]
-                advantages_batch = advantages[batch_idx]
-                old_mu_batch = old_mu[batch_idx]
-                old_sigma_batch = old_sigma[batch_idx]
-
-                # -- For RND
-                if self.rnd_state_shape is not None:
-                    rnd_state_batch = rnd_state[batch_idx]
-                else:
-                    rnd_state_batch = None
-
-                # yield the mini-batch
-                yield obs_batch, privileged_observations_batch, actions_batch, target_values_batch, advantages_batch, returns_batch, old_actions_log_prob_batch, old_mu_batch, old_sigma_batch, (
-                    None,
-                    None,
-                ), None, rnd_state_batch
-
-    # for reinfrocement learning with recurrent networks
-    def recurrent_mini_batch_generator(self, num_mini_batches, num_epochs=8):
-        if self.training_type != "rl":
-            raise ValueError("This function is only available for reinforcement learning training.")
-        padded_obs_trajectories, trajectory_masks = split_and_pad_trajectories(self.observations, self.dones)
-        if self.privileged_observations is not None:
-            padded_privileged_obs_trajectories, _ = split_and_pad_trajectories(self.privileged_observations, self.dones)
-        else:
-            padded_privileged_obs_trajectories = padded_obs_trajectories
-
-        if self.rnd_state_shape is not None:
-            padded_rnd_state_trajectories, _ = split_and_pad_trajectories(self.rnd_state, self.dones)
-        else:
-            padded_rnd_state_trajectories = None
-
-        mini_batch_size = self.num_envs // num_mini_batches
-        for ep in range(num_epochs):
-            first_traj = 0
-            for i in range(num_mini_batches):
-                start = i * mini_batch_size
-                stop = (i + 1) * mini_batch_size
-
-                dones = self.dones.squeeze(-1)
-                last_was_done = torch.zeros_like(dones, dtype=torch.bool)
-                last_was_done[1:] = dones[:-1]
-                last_was_done[0] = True
-                trajectories_batch_size = torch.sum(last_was_done[:, start:stop])
-                last_traj = first_traj + trajectories_batch_size
-
-                masks_batch = trajectory_masks[:, first_traj:last_traj]
-                obs_batch = padded_obs_trajectories[:, first_traj:last_traj]
-                privileged_obs_batch = padded_privileged_obs_trajectories[:, first_traj:last_traj]
-
-                if padded_rnd_state_trajectories is not None:
-                    rnd_state_batch = padded_rnd_state_trajectories[:, first_traj:last_traj]
-                else:
-                    rnd_state_batch = None
-
-                actions_batch = self.actions[:, start:stop]
-                old_mu_batch = self.mu[:, start:stop]
-                old_sigma_batch = self.sigma[:, start:stop]
-                returns_batch = self.returns[:, start:stop]
-                advantages_batch = self.advantages[:, start:stop]
-                values_batch = self.values[:, start:stop]
-                old_actions_log_prob_batch = self.actions_log_prob[:, start:stop]
-
-                # reshape to [num_envs, time, num layers, hidden dim] (original shape: [time, num_layers, num_envs, hidden_dim])
-                # then take only time steps after dones (flattens num envs and time dimensions),
-                # take a batch of trajectories and finally reshape back to [num_layers, batch, hidden_dim]
-                last_was_done = last_was_done.permute(1, 0)
-                hid_a_batch = [
-                    saved_hidden_states.permute(2, 0, 1, 3)[last_was_done][first_traj:last_traj]
-                    .transpose(1, 0)
-                    .contiguous()
-                    for saved_hidden_states in self.saved_hidden_states_a
-                ]
-                hid_c_batch = [
-                    saved_hidden_states.permute(2, 0, 1, 3)[last_was_done][first_traj:last_traj]
-                    .transpose(1, 0)
-                    .contiguous()
-                    for saved_hidden_states in self.saved_hidden_states_c
-                ]
-                # remove the tuple for GRU
-                hid_a_batch = hid_a_batch[0] if len(hid_a_batch) == 1 else hid_a_batch
-                hid_c_batch = hid_c_batch[0] if len(hid_c_batch) == 1 else hid_c_batch
-
-                yield obs_batch, privileged_obs_batch, actions_batch, values_batch, advantages_batch, returns_batch, old_actions_log_prob_batch, old_mu_batch, old_sigma_batch, (
-                    hid_a_batch,
-                    hid_c_batch,
-                ), masks_batch, rnd_state_batch
-
-                first_traj = last_traj
\ No newline at end of file
diff --git a/my_deeploco/rsl_rl/utils/__init__.py b/my_deeploco/rsl_rl/utils/__init__.py
deleted file mode 100644
index 2741729..0000000
--- a/my_deeploco/rsl_rl/utils/__init__.py
+++ /dev/null
@@ -1,14 +0,0 @@
-# Copyright (c) 2021-2025, ETH Zurich and NVIDIA CORPORATION
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-"""Helper functions."""
-
-from .utils import (
-    resolve_nn_activation,
-    split_and_pad_trajectories,
-    store_code_state,
-    string_to_callable,
-    unpad_trajectories,
-)
diff --git a/my_deeploco/rsl_rl/utils/__pycache__/__init__.cpython-310.pyc b/my_deeploco/rsl_rl/utils/__pycache__/__init__.cpython-310.pyc
deleted file mode 100644
index d6772cb..0000000
Binary files a/my_deeploco/rsl_rl/utils/__pycache__/__init__.cpython-310.pyc and /dev/null differ
diff --git a/my_deeploco/rsl_rl/utils/__pycache__/utils.cpython-310.pyc b/my_deeploco/rsl_rl/utils/__pycache__/utils.cpython-310.pyc
deleted file mode 100644
index 6b6ada5..0000000
Binary files a/my_deeploco/rsl_rl/utils/__pycache__/utils.cpython-310.pyc and /dev/null differ
diff --git a/my_deeploco/rsl_rl/utils/__pycache__/wandb_utils.cpython-310.pyc b/my_deeploco/rsl_rl/utils/__pycache__/wandb_utils.cpython-310.pyc
deleted file mode 100644
index 235033f..0000000
Binary files a/my_deeploco/rsl_rl/utils/__pycache__/wandb_utils.cpython-310.pyc and /dev/null differ
diff --git a/my_deeploco/rsl_rl/utils/neptune_utils.py b/my_deeploco/rsl_rl/utils/neptune_utils.py
deleted file mode 100644
index 0d76996..0000000
--- a/my_deeploco/rsl_rl/utils/neptune_utils.py
+++ /dev/null
@@ -1,94 +0,0 @@
-# Copyright (c) 2021-2025, ETH Zurich and NVIDIA CORPORATION
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-from __future__ import annotations
-
-import os
-from dataclasses import asdict
-from torch.utils.tensorboard import SummaryWriter
-
-try:
-    import neptune
-except ModuleNotFoundError:
-    raise ModuleNotFoundError("neptune-client is required to log to Neptune.")
-
-
-class NeptuneLogger:
-    def __init__(self, project, token):
-        self.run = neptune.init_run(project=project, api_token=token)
-
-    def store_config(self, env_cfg, runner_cfg, alg_cfg, policy_cfg):
-        self.run["runner_cfg"] = runner_cfg
-        self.run["policy_cfg"] = policy_cfg
-        self.run["alg_cfg"] = alg_cfg
-        self.run["env_cfg"] = asdict(env_cfg)
-
-
-class NeptuneSummaryWriter(SummaryWriter):
-    """Summary writer for Neptune."""
-
-    def __init__(self, log_dir: str, flush_secs: int, cfg):
-        super().__init__(log_dir, flush_secs)
-
-        try:
-            project = cfg["neptune_project"]
-        except KeyError:
-            raise KeyError("Please specify neptune_project in the runner config, e.g. legged_gym.")
-
-        try:
-            token = os.environ["NEPTUNE_API_TOKEN"]
-        except KeyError:
-            raise KeyError(
-                "Neptune api token not found. Please run or add to ~/.bashrc: export NEPTUNE_API_TOKEN=YOUR_API_TOKEN"
-            )
-
-        try:
-            entity = os.environ["NEPTUNE_USERNAME"]
-        except KeyError:
-            raise KeyError(
-                "Neptune username not found. Please run or add to ~/.bashrc: export NEPTUNE_USERNAME=YOUR_USERNAME"
-            )
-
-        neptune_project = entity + "/" + project
-
-        self.neptune_logger = NeptuneLogger(neptune_project, token)
-
-        self.name_map = {
-            "Train/mean_reward/time": "Train/mean_reward_time",
-            "Train/mean_episode_length/time": "Train/mean_episode_length_time",
-        }
-
-        run_name = os.path.split(log_dir)[-1]
-
-        self.neptune_logger.run["log_dir"].log(run_name)
-
-    def _map_path(self, path):
-        if path in self.name_map:
-            return self.name_map[path]
-        else:
-            return path
-
-    def add_scalar(self, tag, scalar_value, global_step=None, walltime=None, new_style=False):
-        super().add_scalar(
-            tag,
-            scalar_value,
-            global_step=global_step,
-            walltime=walltime,
-            new_style=new_style,
-        )
-        self.neptune_logger.run[self._map_path(tag)].log(scalar_value, step=global_step)
-
-    def stop(self):
-        self.neptune_logger.run.stop()
-
-    def log_config(self, env_cfg, runner_cfg, alg_cfg, policy_cfg):
-        self.neptune_logger.store_config(env_cfg, runner_cfg, alg_cfg, policy_cfg)
-
-    def save_model(self, model_path, iter):
-        self.neptune_logger.run["model/saved_model_" + str(iter)].upload(model_path)
-
-    def save_file(self, path, iter=None):
-        name = path.rsplit("/", 1)[-1].split(".")[0]
-        self.neptune_logger.run["git_diff/" + name].upload(path)
\ No newline at end of file
diff --git a/my_deeploco/rsl_rl/utils/utils.py b/my_deeploco/rsl_rl/utils/utils.py
deleted file mode 100644
index 51c856f..0000000
--- a/my_deeploco/rsl_rl/utils/utils.py
+++ /dev/null
@@ -1,141 +0,0 @@
-# Copyright (c) 2021-2025, ETH Zurich and NVIDIA CORPORATION
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-from __future__ import annotations
-
-import git
-import importlib
-import os
-import pathlib
-import torch
-from typing import Callable
-
-
-def resolve_nn_activation(act_name: str) -> torch.nn.Module:
-    if act_name == "elu":
-        return torch.nn.ELU()
-    elif act_name == "selu":
-        return torch.nn.SELU()
-    elif act_name == "relu":
-        return torch.nn.ReLU()
-    elif act_name == "crelu":
-        return torch.nn.CELU()
-    elif act_name == "lrelu":
-        return torch.nn.LeakyReLU()
-    elif act_name == "tanh":
-        return torch.nn.Tanh()
-    elif act_name == "sigmoid":
-        return torch.nn.Sigmoid()
-    elif act_name == "identity":
-        return torch.nn.Identity()
-    else:
-        raise ValueError(f"Invalid activation function '{act_name}'.")
-
-
-def split_and_pad_trajectories(tensor, dones):
-    """Splits trajectories at done indices. Then concatenates them and pads with zeros up to the length og the longest trajectory.
-    Returns masks corresponding to valid parts of the trajectories
-    Example:
-        Input: [ [a1, a2, a3, a4 | a5, a6],
-                 [b1, b2 | b3, b4, b5 | b6]
-                ]
-
-        Output:[ [a1, a2, a3, a4], | [  [True, True, True, True],
-                 [a5, a6, 0, 0],   |    [True, True, False, False],
-                 [b1, b2, 0, 0],   |    [True, True, False, False],
-                 [b3, b4, b5, 0],  |    [True, True, True, False],
-                 [b6, 0, 0, 0]     |    [True, False, False, False],
-                ]                  | ]
-
-    Assumes that the inputy has the following dimension order: [time, number of envs, additional dimensions]
-    """
-    dones = dones.clone()
-    dones[-1] = 1
-    # Permute the buffers to have order (num_envs, num_transitions_per_env, ...), for correct reshaping
-    flat_dones = dones.transpose(1, 0).reshape(-1, 1)
-
-    # Get length of trajectory by counting the number of successive not done elements
-    done_indices = torch.cat((flat_dones.new_tensor([-1], dtype=torch.int64), flat_dones.nonzero()[:, 0]))
-    trajectory_lengths = done_indices[1:] - done_indices[:-1]
-    trajectory_lengths_list = trajectory_lengths.tolist()
-    # Extract the individual trajectories
-    trajectories = torch.split(tensor.transpose(1, 0).flatten(0, 1), trajectory_lengths_list)
-    # add at least one full length trajectory
-    trajectories = trajectories + (torch.zeros(tensor.shape[0], *tensor.shape[2:], device=tensor.device),)
-    # pad the trajectories to the length of the longest trajectory
-    padded_trajectories = torch.nn.utils.rnn.pad_sequence(trajectories)
-    # remove the added tensor
-    padded_trajectories = padded_trajectories[:, :-1]
-
-    trajectory_masks = trajectory_lengths > torch.arange(0, tensor.shape[0], device=tensor.device).unsqueeze(1)
-    return padded_trajectories, trajectory_masks
-
-
-def unpad_trajectories(trajectories, masks):
-    """Does the inverse operation of  split_and_pad_trajectories()"""
-    # Need to transpose before and after the masking to have proper reshaping
-    return (
-        trajectories.transpose(1, 0)[masks.transpose(1, 0)]
-        .view(-1, trajectories.shape[0], trajectories.shape[-1])
-        .transpose(1, 0)
-    )
-
-
-def store_code_state(logdir, repositories) -> list:
-    git_log_dir = os.path.join(logdir, "git")
-    os.makedirs(git_log_dir, exist_ok=True)
-    file_paths = []
-    for repository_file_path in repositories:
-        try:
-            repo = git.Repo(repository_file_path, search_parent_directories=True)
-            t = repo.head.commit.tree
-        except Exception:
-            print(f"Could not find git repository in {repository_file_path}. Skipping.")
-            # skip if not a git repository
-            continue
-        # get the name of the repository
-        repo_name = pathlib.Path(repo.working_dir).name
-        diff_file_name = os.path.join(git_log_dir, f"{repo_name}.diff")
-        # check if the diff file already exists
-        if os.path.isfile(diff_file_name):
-            continue
-        # write the diff file
-        print(f"Storing git diff for '{repo_name}' in: {diff_file_name}")
-        with open(diff_file_name, "x", encoding="utf-8") as f:
-            content = f"--- git status ---\n{repo.git.status()} \n\n\n--- git diff ---\n{repo.git.diff(t)}"
-            f.write(content)
-        # add the file path to the list of files to be uploaded
-        file_paths.append(diff_file_name)
-    return file_paths
-
-
-def string_to_callable(name: str) -> Callable:
-    """Resolves the module and function names to return the function.
-
-    Args:
-        name (str): The function name. The format should be 'module:attribute_name'.
-
-    Raises:
-        ValueError: When the resolved attribute is not a function.
-        ValueError: When unable to resolve the attribute.
-
-    Returns:
-        Callable: The function loaded from the module.
-    """
-    try:
-        mod_name, attr_name = name.split(":")
-        mod = importlib.import_module(mod_name)
-        callable_object = getattr(mod, attr_name)
-        # check if attribute is callable
-        if callable(callable_object):
-            return callable_object
-        else:
-            raise ValueError(f"The imported object is not callable: '{name}'")
-    except AttributeError as e:
-        msg = (
-            "We could not interpret the entry as a callable object. The format of input should be"
-            f" 'module:attribute_name'\nWhile processing input '{name}', received the error:\n {e}."
-        )
-        raise ValueError(msg)
\ No newline at end of file
diff --git a/my_deeploco/rsl_rl/utils/wandb_utils.py b/my_deeploco/rsl_rl/utils/wandb_utils.py
deleted file mode 100644
index cf56fd2..0000000
--- a/my_deeploco/rsl_rl/utils/wandb_utils.py
+++ /dev/null
@@ -1,89 +0,0 @@
-# Copyright (c) 2021-2025, ETH Zurich and NVIDIA CORPORATION
-# All rights reserved.
-#
-# SPDX-License-Identifier: BSD-3-Clause
-
-from __future__ import annotations
-
-import os
-from dataclasses import asdict
-from torch.utils.tensorboard import SummaryWriter
-
-try:
-    import wandb
-except ModuleNotFoundError:
-    raise ModuleNotFoundError("Wandb is required to log to Weights and Biases.")
-
-
-class WandbSummaryWriter(SummaryWriter):
-    """Summary writer for Weights and Biases."""
-
-    def __init__(self, log_dir: str, flush_secs: int, cfg):
-        super().__init__(log_dir, flush_secs)
-
-        # Get the run name
-        run_name = os.path.split(log_dir)[-1]
-
-        try:
-            project = cfg["wandb_project"]
-        except KeyError:
-            raise KeyError("Please specify wandb_project in the runner config, e.g. legged_gym.")
-
-        try:
-            entity = os.environ["WANDB_USERNAME"]
-        except KeyError:
-            entity = None
-
-        # Initialize wandb
-        wandb.init(project=project, entity=entity, name=run_name)
-
-        # Add log directory to wandb
-        wandb.config.update({"log_dir": log_dir})
-
-        self.name_map = {
-            "Train/mean_reward/time": "Train/mean_reward_time",
-            "Train/mean_episode_length/time": "Train/mean_episode_length_time",
-        }
-
-    def store_config(self, env_cfg, runner_cfg, alg_cfg, policy_cfg):
-        wandb.config.update({"runner_cfg": runner_cfg})
-        wandb.config.update({"policy_cfg": policy_cfg})
-        wandb.config.update({"alg_cfg": alg_cfg})
-        if hasattr(env_cfg, "to_dict"):
-            wandb.config.update({"env_cfg": env_cfg.to_dict()})
-        elif hasattr(env_cfg, "__dataclass_fields__"):
-            wandb.config.update({"env_cfg": asdict(env_cfg)})
-        else:
-            wandb.config.update({"env_cfg": dict(env_cfg)})
-
-    def add_scalar(self, tag, scalar_value, global_step=None, walltime=None, new_style=False):
-        super().add_scalar(
-            tag,
-            scalar_value,
-            global_step=global_step,
-            walltime=walltime,
-            new_style=new_style,
-        )
-        wandb.log({self._map_path(tag): scalar_value}, step=global_step)
-
-    def stop(self):
-        wandb.finish()
-
-    def log_config(self, env_cfg, runner_cfg, alg_cfg, policy_cfg):
-        self.store_config(env_cfg, runner_cfg, alg_cfg, policy_cfg)
-
-    def save_model(self, model_path, iter):
-        wandb.save(model_path, base_path=os.path.dirname(model_path))
-
-    def save_file(self, path, iter=None):
-        wandb.save(path, base_path=os.path.dirname(path))
-
-    """
-    Private methods.
-    """
-
-    def _map_path(self, path):
-        if path in self.name_map:
-            return self.name_map[path]
-        else:
-            return path
\ No newline at end of file
diff --git a/pyproject.toml b/pyproject.toml
deleted file mode 100644
index 1d284e0..0000000
--- a/pyproject.toml
+++ /dev/null
@@ -1,3 +0,0 @@
-[build-system]
-requires = ["setuptools>=64.0.0", "wheel"]
-build-backend = "setuptools.build_meta"
\ No newline at end of file
diff --git a/rsl_rl b/rsl_rl
deleted file mode 160000
index e662185..0000000
--- a/rsl_rl
+++ /dev/null
@@ -1 +0,0 @@
-Subproject commit e66218528d2f7d18a95f1dd91f670ae7ec91478f
diff --git a/setup.py b/setup.py
deleted file mode 100644
index d337988..0000000
--- a/setup.py
+++ /dev/null
@@ -1,7 +0,0 @@
-from setuptools import setup, find_packages
-
-setup(
-    name="my_deeploco",
-    version="0.1",
-    packages=["my_deeploco"],
-)
\ No newline at end of file
diff --git a/test.py b/test.py
deleted file mode 100644
index 8cc9b65..0000000
--- a/test.py
+++ /dev/null
@@ -1,20 +0,0 @@
-import torch
-
-# Path to your checkpoint file
-checkpoint_path = "/home/dodolab/tkworkspace/My_deeploco/my_deeploco/log/g1-deeploco-walk/model_1300.pt"  # Change to your actual file
-
-# Load the checkpoint
-checkpoint = torch.load(checkpoint_path, map_location="cpu")  # or "cuda" if you want
-
-# Print all keys in the checkpoint
-print("Checkpoint keys:", checkpoint.keys())
-
-# Example: print the current iteration if present
-if "iter" in checkpoint:
-    print("Current iteration:", checkpoint["iter"])
-
-# Example: print the model state dict keys
-if "model_state_dict" in checkpoint:
-    print("Model state dict keys:", list(checkpoint["model_state_dict"].keys()))
-
-# You can also inspect other parts, e.g. optimizer state, infos, etc.